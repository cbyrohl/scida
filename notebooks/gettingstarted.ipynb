{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b51657",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "This package is designed to aid in the efficient analysis of large simulations, such as cosmological (hydrodynamical) simulations of large-scale structure.\n",
    "\n",
    "It uses the [dask](https://dask.org/) library to perform computations, which has several key advantages:\n",
    "* (i) very large datasets which cannot normally fit into memory can be analyzed,\n",
    "* (ii) calculations can be automatically distributed onto parallel 'workers', across one or more nodes, to speed them up.\n",
    "* (iii) we can create abstract graphs (\"recipes\", such as for derived quantities) and only evaluate on actual demand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33819c43",
   "metadata": {},
   "source": [
    "## Loading an individual dataset\n",
    "\n",
    "The first step is to choose an existing snapshot of a simulation. To start, we will intentionally select the $z=0$ output of TNG50-4, which is the lowest resolution version of [TNG50](https://www.tng-project.org/), a suite for galaxy formation simulations in cosmological volumes. Choosing TNG50-4 means that the data size in the snapshot is small and easy to work with. We demonstrate how to work with larger data sets at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ecbd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scida import load\n",
    "ds = load(\"TNG50-4_snapshot\", units=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd1cefa",
   "metadata": {},
   "source": [
    "We can get some general information about this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29066266",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "Loading this data gives us access to the simulation snapshot's contents. For example, in the case of [AREPO](https://arepo-code.org) we find most of the metadata in the attributes \"config\", \"header\" and \"parameters\". The raw metadata these dictionaries are derived from is given under\n",
    "```\n",
    "ds.metadata\n",
    "```\n",
    "\n",
    "irrespective of the dataset type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bbae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"some ds.config entry:\", next(iter(ds.config.items())))\n",
    "print(\"some ds.header entry:\", next(iter(ds.header.items())))\n",
    "print(\"some ds.parameters entry:\",next(iter(ds.parameters.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3185b6",
   "metadata": {},
   "source": [
    "If you are familiar with AREPO snapshots, you will know that oftentimes the output is split into multiple files. Most of the metadata will be the same for all files, but some (such as the number of particles in given file `NumPart_ThisFile`) will not. In these cases, the differing entries are stacked along the first axis, so that we also have access to this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96efa969",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gas cells for each file:\", ds.header['NumPart_ThisFile'][:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98ccbd",
   "metadata": {},
   "source": [
    "## Particle/cell data\n",
    "\n",
    "Within our `ds` object, `ds.data` contains references to all the particle/cell data in this snapshot. Data is organized in a nested dictionary depending on the type of data.\n",
    "\n",
    "If the snapshot is split across multiple file chunks on disk (as is the case for most large cosmological simulations), then these are virtually \"combined\" as for the metadata, see above.\n",
    "\n",
    "As a result, there is a single array per data entry at the leaves of the nested dictionary. Note that these arrays are **not** normal numpy arrays, but are instead **dask arrays**, which we will return to later.\n",
    "\n",
    "For the TNG50-4 datasets, the first level of `ds.data` maps the different particle types (such as gas and dark matter), and the second level holds the different physical field arrays (such as density and ionization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ef767",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,val in ds.data.items():\n",
    "    print(\"Particle species:\", key)\n",
    "    print(\"Three of its fields:\", list(val.keys())[:3], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799fafe6",
   "metadata": {},
   "source": [
    "## Analyzing snapshot data\n",
    "\n",
    "In order to perform a given analysis on some available snapshot data, we would normally first explicitly load the required data from disk, and then run some calculations on this data (in memory).\n",
    "\n",
    "Instead, with dask, our fields are loaded automatically as well as \"lazily\" -- only when actually required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5d50df",
   "metadata": {},
   "source": [
    "### Computing a simple statistic on (all) particles\n",
    "\n",
    "The fields in our snapshot object behave similar to actual numpy arrays. \n",
    "\n",
    "As a first simple example, let's calculate the total mass of gas cells in the entire simulation. Just as in numpy we can write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b00cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "masses = ds.data[\"PartType0\"][\"Masses\"]\n",
    "task = masses.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c537797d",
   "metadata": {},
   "source": [
    "Note that all objects remain 'virtual': they are not calculated or loaded from disk, but are merely the required instructions, encoded into tasks. In a notebook we can inspect these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eba147",
   "metadata": {},
   "outputs": [],
   "source": [
    "masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac37b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d823f56",
   "metadata": {},
   "source": [
    "We can request a calculation of the actual operation(s) by applying the `.compute()` method to the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49842dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "task.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1795cbcb",
   "metadata": {},
   "source": [
    "### Creating a visualization: projecting onto a 2D image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3493319",
   "metadata": {},
   "source": [
    "As an example of calculating something more complicated than just `sum()`, let's do the usual \"poor man's projection\" via a 2D histogram.\n",
    "\n",
    "To do so, we use [da.histogram2d()](https://docs.dask.org/en/latest/array.html) of dask, which is analogous to [numpy.histogram2d()](https://numpy.org/doc/stable/reference/generated/numpy.histogram2d.html), except that it operates on a dask array. Later on, we will discuss more advanced, interactive visualization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e992a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import numpy as np\n",
    "\n",
    "coords = ds.data[\"PartType0\"][\"Coordinates\"]\n",
    "x = coords[:,0]\n",
    "y = coords[:,1]\n",
    "\n",
    "nbins = 512\n",
    "bins1d = np.linspace(0, ds.header[\"BoxSize\"], nbins+1)\n",
    "\n",
    "result = da.histogram2d(x,y,bins=[bins1d,bins1d])\n",
    "im2d = result[0].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b05bf",
   "metadata": {},
   "source": [
    "The resulting `im2d` is just a two-dimensional array which we can display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4c47b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10614642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import LogNorm\n",
    "fig = plt.figure(figsize=(6, 6), dpi=300)\n",
    "cmap = mpl.cm.viridis\n",
    "cranges = np.logspace(*[np.percentile(np.log10(im2d), i) for i in [1, 99.9]], 10)\n",
    "norm = mpl.colors.BoundaryNorm(cranges, cmap.N, extend='both')\n",
    "plt.imshow(im2d.T, norm=norm, extent=[0, ds.header[\"BoxSize\"], 0, ds.header[\"BoxSize\"]], interpolation=\"bilinear\", rasterized=True)\n",
    "plt.xlabel(\"x (ckpc/h)\")\n",
    "plt.ylabel(\"y (ckpc/h)\")\n",
    "ram = BytesIO()\n",
    "plt.savefig(ram, bbox_inches=\"tight\", dpi=150)\n",
    "ram.seek(0)\n",
    "im = Image.open(ram)\n",
    "im2 = im.convert('RGB').convert('P', palette=Image.ADAPTIVE)\n",
    "im2.save(\"hist.png\" , format='PNG')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scida",
   "language": "python",
   "name": "scida"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
