{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb7a191f",
   "metadata": {},
   "source": [
    "# Handling Large Data Sets\n",
    "Until now, we have applied our framework to a very small simulation. However, what if we are working with a very large data set (like TNG50-1, which has $2160^3$ particles, $512$ times more than TNG50-4)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bdd344",
   "metadata": {},
   "source": [
    "# Starting simple: computing in chunks\n",
    "\n",
    "First, we can still run the same calculation as above, and it will \"just work\" (hopefully).\n",
    "\n",
    "This is because Dask has many versions of common algorithms and functions which work on \"blocks\" or \"chunks\" of the data, which split up the large array into smaller arrays. Work is needed on each chunk, after which the final answer is assembled.\n",
    "\n",
    "Importantly, in our case above, even if the `mass` array above does not fit into memory, the `mass.sum().compute()` will chunk the operation up in a way that the task can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d5cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scida import load\n",
    "ds = load(\"/data/public/testdata-scida/TNG50-3_snapshot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8819385",
   "metadata": {},
   "source": [
    "Before we start, let's enable a progress indicator from dask (note that this will only work for local schedulers, see next section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39bbad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22560a13",
   "metadata": {},
   "source": [
    "And then we can request the actual computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ds.data[\"PartType0\"][\"Masses\"].sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb80b66",
   "metadata": {},
   "source": [
    "While the result is eventually computed, it is a bit slow, primarily because the actual reading of the data off disk is the limiting factor, and we can only use resources available on our local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b46bc04",
   "metadata": {},
   "source": [
    "## More advanced: computing in parallel\n",
    "\n",
    "Rather than sequentially calculating large tasks, we can also run the computation in parallel. \n",
    "\n",
    "To do so different advanced dask schedulers are available. Here, we use the most straight forward [distributed scheduler](https://docs.dask.org/en/latest/how-to/deploy-dask/single-distributed.html).\n",
    "\n",
    "Usually, we would start a scheduler and then connect new workers (e.g. running on multiple compute/backend nodes of a HPC cluster). After, tasks (either interactively or scripted) can leverage the power of these connected resources.\n",
    "\n",
    "For this example, we will use the same \"distributed\" scheduler/API, but keep things simple by using just the one (local) node we are currently running on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(n_workers=8, threads_per_worker=1, \n",
    "                       dashboard_address=\":8787\")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf504f7",
   "metadata": {},
   "source": [
    "Here is our client. We can access the scheduler on specified dashboard port to investigate its state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60808601",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc76406",
   "metadata": {},
   "source": [
    "We can now perform the same operations, but it is performed in a distributed manner, in parallel.\n",
    "\n",
    "One significant advantage is that (even when using only a single node) individual workers will load just the subsets of data they need to work on, meaing that I/O operations become parallel.\n",
    "\n",
    "Note: after creating a `Client()`, all calls to `.compute()` will automatically use this scheduler and its set of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d7f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ds.data[\"PartType0\"][\"Masses\"].sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbdd1fb",
   "metadata": {},
   "source": [
    "The progress bar, we could use for the default scheduler (before initializing `LocalCluster`), is unavailable for the distributed scheduler. However, we can still view the progress of this task as it executes using its status dashboard (as a webpage in a new browser tab or within [jupyter lab](https://github.com/dask/dask-labextension)). You can find it by clicking on the \"Dashboard\" link above. If running this notebook server remotely, e.g. on a login node of a HPC cluster, you may have to change the '127.0.0.1' part of the address to be the same machine name/IP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scida",
   "language": "python",
   "name": "scida"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
