{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#scida","title":"scida","text":"<p>Scalable analysis for large astrophysical datasets</p> <p>Process cosmological simulations and observational data with dask-powered parallel computing and automatic physical units.</p> Get Started View on GitHub"},{"location":"#one-line-loading","title":"One-Line Loading","text":"<p>Load any supported dataset with a single function call. Automatic type detection selects the right handler.</p> <pre><code>import scida\nds = scida.load(\"snapshot_099.hdf5\")\n</code></pre>"},{"location":"#dask-powered","title":"Dask-Powered","text":"<p>Scale from your laptop to HPC clusters. All data is loaded as lazy dask arrays, computed only when needed.</p> <pre><code>masses = ds.data[\"PartType0\"][\"Masses\"]\ntotal = masses.sum().compute()  # runs in parallel\n</code></pre>"},{"location":"#physical-units","title":"Physical Units","text":"<p>Automatic unit support via pint. Easily compare results across different simulation codes and observational surveys with consistent physical units.</p> <pre><code>ds = scida.load(\"snapshot_099.hdf5\", units=True)\ncoords = ds.data[\"PartType0\"][\"Coordinates\"]\ncoords_kpc = coords.to(\"kpc\")\n</code></pre>"},{"location":"#multiple-formats","title":"Multiple Formats","text":"<p>Native support for HDF5, zarr, and FITS. Works with AREPO, SWIFT, GIZMO, Gadget, and more out of the box.</p> <p>See supported datasets </p>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#configuration","title":"Configuration","text":""},{"location":"configuration/#main-configuration-file","title":"Main configuration file","text":"<p>The main configuration file is located at <code>~/.config/scida/config.yaml</code>. If this file does not exist, it is created with the first use of scida. The file is using the YAML format. The following options are available:</p> <code>copied_default</code> <p>If this option is set True, a warning is printed because the copied default config has not been adjusted by the user    yet. Once you have done so, remove this line.</p> <code>cache_path</code> <p>Sets the folder to use as a cache for scida. Recommended to be located on a fast disk.</p> <code>datafolders</code> <p>A list of folders to scan for data specifiers when using <code>scida.load(\"specifier\")</code>.</p> <code>nthreads</code> <p>scida itself might use multiple threads for some operations. This option sets the number of threads to use.   This is independent of any dask threading. Default: 8</p> <code>missing_units</code> <p>How to handle missing units. Can be \"warn\", \"raise\", or \"ignore\". \"warn\" will print a warning, \"raise\" will raise an   exception, and \"ignore\" will silently continue without the right units. Default: \"warn\"</p> <code>testdata_path</code> The base path to the test data sets defined in \"tests/testdata.yaml\"."},{"location":"configuration/#simulation-configuration","title":"Simulation configuration","text":"<p>By default, scida will load supported simulation configurations from the package. User configurations for simulations are loaded from <code>~/.config/scida/simulations.yaml</code>. This file is also in YAML format.</p> <p>The configuration has to have the following structure: </p><pre><code>data:\n  SIMNAME1:\n\n  SIMNAME2:\n</code></pre><p></p> <p>Each simulation could look something like this:</p> <pre><code>data:\n  SIMNAME1:\n    aliases:\n      - SIMNAME\n      - SMN1\n    identifiers:\n      Parameters:\n        SimName: SIMNAME1\n      Config:\n        SavePath:\n          content: /path/to/simname\n          match: substr\n    unitfile: units/simnameunits.yaml\n    dataset_type:\n      series: ArepoSimulation\n      dataset: ArepoSnapshot\n</code></pre> <code>aliases</code> <p>A list of aliases for the simulation. These can be used to load the simulation with <code>scida.load(\"alias\")</code>.</p> <code>identifiers</code> <p>A dictionary of identifiers from the metadata of a given dataset to identify it as such.   In above example \"/Parameters\" is the path to an attribute \"SimName\" in the HDF5/zarr metadata   with the exact content as given. Multiple identifiers can be given, in which case all have to match.   Partial matches of a given key-value key are possible by passing a dictionary {\"content\": \"valuesubstr\", match: substring}   rather than a string.</p> <code>unitfile</code> <p>The path to the unitfile relative to the user/repository simulation configuration. user configurations   take precedence over the package configuration.</p> <code>dataset_type</code> <p>Can explicitly fix the dataset/series type for a simulation.</p>"},{"location":"configuration/#unit-files","title":"Unit files","text":"<p>Unit files are used to determine the units of datasets, particularly for datasets that do not have metadata that can be used to infer units. Unit files are specified either explicitly via the <code>unitfile</code> option in <code>scida.load</code> or implicitly via the simulation configuration, see above. The precedence in descending order is: absolute paths, relative paths in the current working directory, relative paths in the user config folder (<code>~/.config/scida/</code>), and the package config folder.</p> <p>A unit file could look like this:</p> <pre><code>metadata_unitsystem: cgs\nunits:\n  unit_length: 100.0 * km\n  unit_mass: g\nfields:\n  _all:\n    CounterID: none\n    Coordinates: unit_length\n  InternalArrays: none\n  PartType0:\n    SubPartType0:\n      FurthestSubgroupDistance: unit_length\n    NearestNeighborDistance: unit_length\n    Energy: 10.0 * erg\n</code></pre> <code>metadata_unitsystem</code> <p>The unitsystem assumed when deducing units from metadata dimensions where available.   Only cgs supported right now.</p> <code>units</code> <p>unit definitions that are used in the following <code>fields</code> section. The units are defined as   pint expressions.</p> <code>fields</code> <p>A dictionary of fields and their units. The fields are specified as a path to the field in the dataset.   The special field <code>_all</code> can be used to set the default unit for all fields with a given name irrespective   of the path of the field. Other than that, entries represent the fields or containers of fields. The special   field <code>none</code> can be used to set the unit to None, i.e. no unit. This is differently handled than \" \"/\"dimensionless\" as   the field will be treated as array rather than dimensionless pint array.</p>"},{"location":"derived_fields/","title":"Derived fields","text":""},{"location":"derived_fields/#derived-fields","title":"Derived fields","text":"<p>Info</p> <p>If you want to run the code below, consider downloading the demo data or use the TNGLab online.</p> <p>Commonly during analysis, newly derived quantities/fields are to be synthesized from one or more snapshot fields into a new field. For example, while the temperature, pressure, or entropy of gas is not stored directly in the snapshots, they can be computed from fields which are present on disk.</p> <p>There are two ways to create new derived fields. For quick analysis, we can simply leverage dask arrays themselves.</p>"},{"location":"derived_fields/#defining-new-quantities-with-dask-arrays","title":"Defining new quantities with dask arrays","text":"<pre><code>from scida import load\nds = load(\"./snapdir_030\") # (1)!\ngas = ds.data['gas']\nkineticenergy = 0.5*gas['Masses']*(gas['Velocities']**2).sum(axis=1)\n</code></pre> <ol> <li>In this example, we assume a dataset, such as the demo data set, that has its fields (Masses, Velocities) nested by particle type (gas)</li> </ol> <p>In the example above, we define a new dask array called \"kineticenergy\". Note that just like all other dask arrays and dataset fields, these fields are \"virtual\", i.e. only the graph of their construction is held in memory, which can be instantiated by applying the .compute() method.</p> <p>We can also add this field from above example to the existing ones in the dataset.</p> <pre><code>gas['kineticenergy'] = kineticenergy\n</code></pre>"},{"location":"derived_fields/#defining-new-quantities-with-field-recipes","title":"Defining new quantities with field recipes","text":"<p>Working with complex datasets over a longer period, it is often useful to have a large range of fields available. The above approach with dask arrays suffers from some shortcomings. For example, in some cases the memory footprint and instantiation time for each field can add up to substantial loading times. Also, when defining fields with dask arrays, these fields need to be defined in order of their respective dependencies.</p> <p>For this purpose, field recipes are available. An example of such recipe is given below.</p> <pre><code>import numpy as np\n\nfrom scida import load\nds = load(\"./snapdir_030\")\n\n@ds.register_field(\"stars\")  # (1)!\ndef VelMag(arrs, **kwargs):\n    import dask.array as da\n    vel = arrs['Velocities']\n    v, u = vel.magnitude, vel.units\n    return da.sqrt(v[:,0]**2 + v[:,1]**2 + v[:,2]**2) * u\n</code></pre> <ol> <li>Here, stars is the name of the field container the field should be added to. The field will now be available as ds['stars']['VelMag']</li> </ol> <p>The field recipe is translated into a regular field, i.e. dask array, the first time it is queried for. Above example can be queried as:</p> <pre><code>ds['stars']['VelMag']\n</code></pre> <p>Practically working with these fields, there is no difference between derived and on-disk fields.</p>"},{"location":"derived_fields/#adding-multiple-fields","title":"Adding multiple fields","text":"<p>It can be useful to write (a) dedicated field definition file(s). First, initialize a FieldContainer</p> <pre><code>from scida.fields import FieldContainer\ngroupnames = [\"PartType0\", \"Subhalo\"]  # (1)!\nfielddefs = FieldContainer(containers=groupnames)\n\n@fielddefs.register_field(\"PartType0\") # (2)!\ndef Volume(arrs, **kwargs):\n    return arrs[\"Masses\"]/arrs[\"Density\"]\n\n@fielddefs.register_field(\"all\") # (3)!\ndef GroupDistance3D(arrs, snap=None):\n    \"\"\"Returns distance to hosting group center. Returns rubbish if not actually associated with a group.\"\"\"\n    import dask.array as da\n    boxsize = snap.header[\"BoxSize\"]\n    pos_part = arrs[\"Coordinates\"]\n    groupid = arrs[\"GroupID\"]\n    if hasattr(groupid, \"magnitude\"):\n        groupid = groupid.magnitude\n        boxsize *= snap.ureg(\"code_length\")\n    pos_cat = snap.data[\"Group\"][\"GroupPos\"][groupid]\n    dist3 = (pos_part-pos_cat)\n    dist3, u = dist3.magnitude, dist3.units\n    dist3 = da.where(dist3&gt;boxsize/2.0, boxsize-dist3, dist3)\n    dist3 = da.where(dist3&lt;=-boxsize/2.0, boxsize+dist3, dist3) # PBC\n    return dist3 * u\n\n@fielddefs.register_field(\"all\")\ndef GroupDistance(arrs, snap=None):\n    import dask.array as da\n    dist3 = arrs[\"GroupDistance3D\"]\n    dist3, u = dist3.magnitude, dist3.units\n    dist = da.sqrt((dist3**2).sum(axis=1))\n    dist = da.where(arrs[\"GroupID\"]==-1, np.nan, dist) # set unbound gas to nan\n    return dist * u\n</code></pre> <ol> <li>We define a list of field containers that we want to add particles to.</li> <li>Specify the container we want to have the field added to.</li> <li>Using the \"all\" identifier, we can also attempt to add this field to all containers we have specified.</li> </ol> <p>Finally, we just need to import the fielddefs object (if we have defined it in another file) and merge them with a dataset that we loaded:</p> <pre><code>ds = load(\"./snapdir_030\")\nds.data.merge(fielddefs)\n</code></pre> <p>In above example, we now have the following fields available:</p> <pre><code>gas = ds.data[\"PartType0\"]\nprint(gas[\"Volume\"])\nprint(gas[\"GroupDistance\"])\n</code></pre>"},{"location":"developer/","title":"Development","text":""},{"location":"developer/#developer-guide","title":"Developer Guide","text":"<p>We welcome contributions to scida, such as bug reports, feature requests, and design proposals. This page contains information on how to contribute to scida.</p>"},{"location":"developer/#development-environment","title":"Development environment","text":""},{"location":"developer/#clone-the-repository","title":"Clone the repository","text":"<p>Make a fork of the repository, then clone the repository to your local machine:</p> <pre><code>git clone https://github.com/YOURUSERNAME/scida\ncd scida\n</code></pre>"},{"location":"developer/#install","title":"Install","text":"<p>We use uv to manage dependencies and the development environment. After installing uv, you can install scida and its dependencies with</p> <pre><code>uv sync --all-groups\n</code></pre> <p>This will create a virtual environment and install scida and its dependencies, including development dependencies. All commands, such as <code>python</code> and <code>pytest</code>, should be run via <code>uv run ...</code>.</p> <p>Alternatively, you can install scida with pip in a virtual environment of your choice:</p> <pre><code>python -m venv scida_venv\nsource scida_venv/bin/activate\npip install -e .\n</code></pre> <p>Note that in this case, you will have to manage the dependencies yourself, including development dependencies. If choosing this path, remove any <code>uv run</code> prefixes from the commands below accordingly.</p>"},{"location":"developer/#run-tests","title":"Run tests","text":"<p>To run the tests, use</p> <pre><code>uv run pytest\n</code></pre> <p>Many tests require test data sets. These might not be available to you and lead to many tests being skipped.</p>"},{"location":"developer/#contributing-code","title":"Contributing code","text":""},{"location":"developer/#code-formatting","title":"Code Formatting","text":"<p>We use the black code formatter to ensure a consistent code style. This style is ensured by the pre-commit hook config. Make sure to have pre-commit installed and run</p> <pre><code>pre-commit install\n</code></pre> <p>in the repository to install the hook.</p>"},{"location":"developer/#docstring-style","title":"Docstring style","text":"<p>We use numpydoc to format docstrings. Please annotate all functions and classes with docstrings accordingly.</p>"},{"location":"developer/#testing","title":"Testing","text":"<p>We use pytest for testing. Add new tests for added functionality in a test file in the <code>tests</code> directory. Make sure to run the tests before submitting a pull request.</p> <p>Many tests require test data sets. These data sets are not included in the repository. Scida will check for the existence of these data sets and skip the tests if they are not available. The metadata for the test data sets is defined in \"tests/testdata.yaml\". The paths are evaluated relative to the \"testdata_path\" entry in the configuration, see configuration. A subset of smaller test data sets can be downloaded for the data sets described in \".github/actions/get-testdata-all/action.yaml\" in the scida repository. This file also contains the symbolic links to larger data sets, which are only available through access to the /virgotng file system on MPCDF resources.</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"faq/#common-warningsexceptions","title":"Common warnings/exceptions","text":"<p>I get the warning \"UserWarning: Passing an object to dask.array.from_array which is already a Dask collection. This can lead to unexpected behavior.\". What does this mean?</p> <p>In scida, this warning often occurs when passing a field with units to a dask.array function. Consider explicitly removing the units (e.g. using \"arr.magnitude\" for the field/array \"arr\") before the dask.array function call. Then attach the correct units afterwards again. This will be automatized in the future.</p> <p>\"ValueError: Cannot operate with Quantity and Quantity of different registries.\"</p> <p>Likely, you are trying to combine fields using different unit registries, e.g. when comparing fields from different datasets. This is not allowed with pint.</p> <p>If you can assure that the definitions of the units for a given field are the same between registries, you could use the following workaround:</p> <pre><code>from scida import load\nds = load(\"./snapdir_030\")\nds2 = load(\"./snapdir_030\")\nunitregistry = ds.ureg\nfield = ds2.data['PartType0'][\"Masses\"]\nunits = unitregistry(str(field.units))\nfield = field.magnitude * units\nds.data['PartType0'][\"Masses2\"] = field\n</code></pre> <p>\"AttributeError: Series do not have 'data' attribute. Load a dataset from series.get_dataset().\"</p> <p>You most likely invoked the load function prior to accessing the data attribute. Depending on the path passed to the load function, it returns an instance of a DataSeries or Dataset. DataSeries are collections of Datasets, and do not have the \".data\" attribute. You can either access a dataset of the series (e.g. \"sim.get_dataset(0)\" where 0 is the index of the dataset to load), or directly load the dataset by specifying the correct subdirectory. You can subsequently call the data attribute on the dataset.</p>"},{"location":"faq/#extending-existing-datasets","title":"Extending existing datasets","text":"<p>How do I add custom fields (that are not derived fields) to my existing dataset?</p> <p>While derived fields are the preferred way to add fields, you can also add custom dask arrays to a dataset.</p> <p>After loading some dataset, you can add custom fields to a given container, here called PartType0 by simply assigning a dask array under the desired field name:</p> <pre><code>import dask.array as da\nds = load(\"./snapdir_030\")\narray = da.zeros_like(ds.data[\"PartType0\"][\"Density\"])\nunitregistry = ds.ureg\nds.data['PartType0'][\"zerofield\"] = array * unitregistry(\"m\")\n</code></pre> <p>Info</p> <p>Please note that all fields within a container are expected to have the same shape in their first axis. When attaching units, it is important to use the dataset's unit registry <code>ds.ureg</code>.</p> <p>As scida expects dask arrays, make sure to cast your array accordingly. For example, if your field is just a numpy array or a hdf5 memmap, you can use <code>da.from_array</code> to cast it to a dask array. Alternatively, if you have another dataset loaded, you can assign fields from one to another:</p> <pre><code>ds2 = load(\"./snapdir_030\")\nds.data['PartType0'][\"NewDensity\"] = ds2.data['PartType0'][\"Density\"]\n</code></pre>"},{"location":"faq/#what-libraries-is-scida-built-on","title":"What libraries is scida built on?","text":"<p>Scida strongly relies on the following libraries: - dask: For parallel computing - pint: For handling units</p> <p>Data support is provided, among others, by:</p> <ul> <li>h5py: For reading HDF5 files</li> <li>zarr: For reading Zarr files</li> </ul> <p>Visualization in the examples is commonly done with:</p> <ul> <li>matplotlib: For plotting</li> <li>bokeh: For interactive plotting</li> <li>holoviz: high-level interactive visualization</li> </ul> <p>The original repository structure was inspired by the following templates:</p> <ul> <li>wolt template</li> <li>hypermodern python template</li> </ul> <p>These lists are not exhaustive. Also see the pyproject.toml file for a list of dependencies.</p>"},{"location":"faq/#misc","title":"Misc","text":"<p>How does load() determine the right type of dataset/series to load?</p> <p>load() will step through all subclasses of Series() and Dataset() and call their validate_path() class method. A list of candidate classes that return True upon this call is assembled. If more than one candidate exists, the most specific candidate, i.e. the one furthest down the inheritance tree, is chosen.</p> <p>The candidate can be overwritten when a YAML configuration specifies \"dataset_type/series\" and/or \"dataset_type/dataset\" keys to the respective class name.</p> <p>In addition to this, different features, such as for datasets using Cartesian coordinates, are added as so-called Mixins to the dataset class.</p>"},{"location":"halocatalogs/","title":"Halo/Galaxy Catalogs","text":""},{"location":"halocatalogs/#halo-and-galaxy-catalogs","title":"Halo and galaxy catalogs","text":"<p>Cosmological simulations are often post-processed with a substructure identification algorithm in order to identify halos and galaxies. The resulting catalogs can be loaded and connect with the particle-level snapshot data.</p> <p>Info</p> <p>If you want to run the code below, consider downloading the demo data or use the TNGLab online.</p>"},{"location":"halocatalogs/#adding-and-using-halogalaxy-catalog-information","title":"Adding and using halo/galaxy catalog information","text":"<p>Currently, we support the usual FOF/Subfind combination and format. Their presence will be automatically detected and the catalogs will be loaded into ds.data as shown below.</p> <pre><code>from scida import load\nds = load(\"./snapdir_030\")  # (1)!\n</code></pre> <ol> <li>In this example, we assume a dataset, such as the demo data set, that has its fields (Masses, Velocities) nested by particle type (gas)</li> </ol> <p>The dataset itself passed to load does not possess information about the FoF/Subfind outputs as they are commonly saved in a separate folder or hdf5 file. For typical folder structures of GADGET/AREPO style simulations, an attempt is made to automatically discover and add such information. The path to the catalog can otherwise explicitly be passed to load() via the catalog=... keyword.</p>"},{"location":"halocatalogs/#accessing-halogalaxy-catalog-information","title":"Accessing halo/galaxy catalog information","text":"<p>Groups and subhalo information is added into the dataset with the data containers Group and Subhalo. For example, we can obtain the masses of each group as:</p> <pre><code>group_mass = ds.data[\"Group\"][\"GroupMass\"]\n</code></pre>"},{"location":"halocatalogs/#accessing-particle-level-halogalaxy-information","title":"Accessing particle-level halo/galaxy information","text":"<p>In addition to these two data containers, new information is added to all other containers about their belonging to a given group and subhalo.</p> <pre><code>groupid = ds.data[\"PartType0\"][\"GroupID\"] #(1)!\nsubhaloid = ds.data[\"PartType0\"][\"SubhaloID\"]\nlocalsubhaloid = ds.data[\"PartType0\"][\"LocalSubhaloID\"]\n</code></pre> <ol> <li>This information is also available for the other particle types.</li> </ol> <p>In above example, we fetch the virtual dask arrays holding information about the halo and subhalo association for each particle.</p> <code>GroupID</code> <p>The group ID of the group the particle belongs to. This is the index into the group catalog.</p> <code>SubhaloID</code> <p>The subhalo ID of the subhalo the particle belongs to. This is the index into the subhalo catalog.</p> <code>LocalSubhaloID</code> <p>This is the Subhalo ID relative to the central subhalo of a given group. For the central subhalo, this is 0.    Satellites accordingly start at index 1.</p> <p>Particles that are not associated with a group or subhalo that are queried for such ID will return <code>ds.misc['unboundID']'</code>. This is currently set to 9223372036854775807, but might change to -1 in the future.</p> <p>This operation allows us to efficiently query the belonging of given particles. So, for example, we can compute the group IDs of the gas particles 1000-1099 by running</p> <pre><code>groupid[1000:1100].compute()\n</code></pre>"},{"location":"halocatalogs/#working-with-halo-data","title":"Working with halo data","text":""},{"location":"halocatalogs/#query-all-particles-belonging-to-some-group","title":"Query all particles belonging to some group","text":"<p>Often we only want to operate with the particles of a given halo. We can efficiently return a virtual view of all fields in ds.data for a given halo ID as for example in:</p> <pre><code>data = ds.return_data(haloID=42)\n</code></pre> <p>data will have the same structure as ds.data but restricted to particles of a given group.</p> <p>We can similarly restrict the data to a given subhalo by passing the subhaloID keyword instead of haloID:</p> <pre><code>data = ds.return_data(subhaloID=42)\n</code></pre> <p>Note that this is the global subhalo ID (i.e. the position within the Subhalo catalog) and not the local subhalo ID (i.e. the subhalo position within the group). If you want the latter, use the localSubhaloID field:</p> <pre><code>data = ds.return_data(haloID=1, localSubhaloID=3)\n</code></pre>"},{"location":"halocatalogs/#applying-to-all-groups-in-parallel","title":"Applying to all groups in parallel","text":"<p>In many cases, we do not want the particle data of an individual group, but we want to calculate some reduced statistic from the bound particles of each group. For this, we provide the grouped functionality. In the following we give a range of examples of its use.</p> Warning <p>Executing the following commands can be demanding on compute resources and memory. Usually, one wants to restrict the groups to run on. You can either specify \"nmax\" to limit the maximum halo id to evaluate up to. This is usually desired in any case as halos are ordered (in descending order) by their mass. For more fine-grained control, you can also pass a list of halo IDs to evaluate via the \"idxlist\" keyword. These keywords should be passed to the \"evaluate\" call.</p> Info <p>By default, operations are done on for halos. By passing <code>objtype=\"subhalo\"</code> to the <code>grouped</code> call, the operation is done on subhalos instead.</p>"},{"location":"halocatalogs/#baryon-mass","title":"Baryon mass","text":"<p>Let's say we want to calculate the baryon mass for each halo from the particles.</p> <pre><code>mass = ds.grouped(\"Masses\", parttype=\"PartType0\").sum().evaluate(compute=True)\nmass\n</code></pre> <p>Unless compute=True a dask operation is returned.</p>"},{"location":"halocatalogs/#electron-mass","title":"Electron mass","text":"<p>Instead of an existing field, we can also pass another dask array of matching field for the given particle species (here: PartType0). The following example calculates the total mass of electrons in each halo.</p> <pre><code>import dask.array as da\ngas = ds.data[\"PartType0\"]\n# total electron mass\nme = 9.1e-28 # cgs\nmp = 1.7e-24 # cgs\n# me and mp units cancel each other\nne = gas[\"ElectronAbundance\"] * 0.76 * gas[\"Density\"]/mp\nvol = gas[\"Masses\"] / gas[\"Density\"]\nemass_field = vol * me * ne\nemass = ds.grouped(emass_field).sum().evaluate(compute=True)\nemass\n</code></pre>"},{"location":"halocatalogs/#heaviest-black-hole","title":"Heaviest black hole","text":"<pre><code>bhmassmax = ds.grouped(\"Masses\", parttype=\"PartType5\").max().evaluate()\nbhmassmax\n</code></pre>"},{"location":"halocatalogs/#radial-profile-for-each-halo","title":"Radial profile for each halo","text":"<pre><code>import numpy as np\nfrom scipy.stats import binned_statistic\n\ngrp = ds.data[\"Group\"]\npos3 = gas[\"Coordinates\"] - grp[\"GroupPos\"][gas[\"GroupID\"]]\np, u = pos3.magnitude, pos3.units\ndist = da.sqrt(da.sum(p**2, axis=1)) * u # (1)!\n\ndef customfunc(dist, density, volume):\n    a = binned_statistic(dist, density, statistic=\"sum\", bins=np.linspace(0, 200, 10))[0]\n    b = binned_statistic(dist, volume, statistic=\"sum\", bins=np.linspace(0, 200, 10))[0]\n    return a/b\n\ng = ds.grouped(dict(dist=dist, Density=gas[\"Density\"],\n                    Volume=vol))\ns = g.apply(customfunc).evaluate()\n</code></pre> <ol> <li>We do not incorporate periodic boundary conditions in this example for brevity.</li> </ol> <p>Note that here we defined a custom function customfunc that will be applied to each halo respectively. The custom function accepts any inputs we feed to ds.grouped(). The customfunc receives numpy representation (rather than dask arrays) as inputs. As unit support with dask is still in development, unit-related warnings might occur. Units might not be passed, it can therefore be necessary to explicitly attach the expected units to the outputs again.</p>"},{"location":"impressions/","title":"Visual Impressions","text":""},{"location":"impressions/#visual-impressions-using-scida","title":"Visual impressions using scida","text":"<p>The following images were created using scida with matplotlib.</p>"},{"location":"impressions/#simulations","title":"Simulations","text":"<p>Plots from the SIMBA, FLAMINGO, TNG100 and THESAN simulations.</p> <p> </p>"},{"location":"impressions/#observational-data","title":"Observational data","text":"<p>Plots from the SDSS DR16, eROSITA-DE DR1 and Gaia DR3.</p> <p> </p>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#getting-started","title":"Getting started","text":""},{"location":"install/#installation","title":"Installation","text":"<p>scida can be installed via PyPI. scida requires Python 3.9 to 3.12.</p> Encapsulating packages <p>We recommend encapsulating your python environments. For example using anaconda or virtualenv.</p> <p>If you use anaconda, we recommend running</p> <pre><code>conda create -n scida python=3.12\n</code></pre> <p>Activate the environment as needed (as for the following installation) as</p> <pre><code>conda activate scida\n</code></pre> <p>If you are using jupyter/ipython, install and register the scida kernel via</p> <pre><code>conda install ipykernel\npython -m ipykernel install --user --name scida --display-name \"scida\"\n</code></pre> <p>Now you can install scida as described below and use it in jupyter notebooks with the given kernel.</p> <pre><code>pip install scida\n</code></pre>"},{"location":"install/#next-steps","title":"Next steps","text":"<p>Next, get started with the tutorial for either simulations or observations:</p> <p>           Simulations         </p> <p>Tutorial on a simulation dataset.</p> <p>           Observations         </p> <p>Tutorial on an observational dataset.</p>"},{"location":"largedatasets/","title":"Large datasets","text":""},{"location":"largedatasets/#handling-large-data-sets","title":"Handling Large Data Sets","text":"<p>Info</p> <p>If you want to run the code below, you need access to or download the full TNG simulation dataset. The easiest way to access all TNG data sets is to use the TNGLab, which supports scida.</p> <p>Until now, we have applied our framework to a very small simulation. However, what if we are working with a very large data set (like the TNG50-1 cosmological simulation, which has \\(2160^3\\) particles, \\(512\\) times more than TNG50-4)?</p>"},{"location":"largedatasets/#starting-simple-computing-in-chunks","title":"Starting simple: computing in chunks","text":"<p>First, we can still run the same calculation as above, and it will \"just work\" (hopefully).</p> <p>This is because Dask has many versions of common algorithms and functions which work on \"blocks\" or \"chunks\" of the data, which split up the large array into smaller arrays. Work is needed on each chunk, after which the final answer is assembled.</p> <p>Importantly, in our case above, even if the <code>mass</code> array above does not fit into memory, the <code>mass.sum().compute()</code> will chunk the operation up in a way that the task can be calculated.</p> <pre><code>&gt;&gt;&gt; from scida import load\n&gt;&gt;&gt; sim = load(\"TNG50-1\")\n&gt;&gt;&gt; ds = sim.get_dataset(99)\n</code></pre> <p>Before we start, let's enable a progress indicator from dask (note that this will only work for local schedulers, see next section):</p> <pre><code>&gt;&gt;&gt; from dask.diagnostics import ProgressBar\n&gt;&gt;&gt; ProgressBar().register()\n</code></pre> <p>Let's benchmark this operation on our local machine.</p> <pre><code>&gt;&gt;&gt; %time ds.data[\"PartType0\"][\"Masses\"].sum().compute()\n[########################################] | 100% Completed | 194.28 s\nCPU times: user 12 s, sys: 16.2 s, total: 28.2 s\nWall time: 3min 16s\n52722.6796875 code_mass\n</code></pre>"},{"location":"largedatasets/#more-advanced-computing-in-parallel","title":"More advanced: computing in parallel","text":"<p>Rather than sequentially calculating large tasks, we can also run the computation in parallel.</p> <p>To do so different advanced dask schedulers are available. Here, we use the most straight forward distributed scheduler.</p>"},{"location":"largedatasets/#running-a-localcluster","title":"Running a LocalCluster","text":"<p>Usually, we would start a scheduler and then connect new workers (e.g. running on multiple compute/backend nodes of a HPC cluster). After, tasks (either interactively or scripted) can leverage the power of these connected resources.</p> <p>For this example, we will use the same \"distributed\" scheduler/API, but keep things simple by using just the one (local) node we are currently running on.</p> <p>While the result is eventually computed, it is a bit slow, primarily because the actual reading of the data off disk is the limiting factor, and we can only use resources available on our local machine.</p> <pre><code>&gt;&gt;&gt; from dask.distributed import Client, LocalCluster\n&gt;&gt;&gt; cluster = LocalCluster(n_workers=16, threads_per_worker=1,\n                           dashboard_address=\":8787\")\n&gt;&gt;&gt; client = Client(cluster)\n&gt;&gt;&gt; client\n</code></pre> <p>This is our client. We can access the scheduler on specified dashboard port to investigate its state.</p> <p>We can now perform the same operations, but it is performed in a distributed manner, in parallel.</p> <p>One significant advantage is that (even when using only a single node) individual workers will load just the subsets of data they need to work on, meaing that I/O operations become parallel.</p> <p>Note: after creating a <code>Client()</code>, all calls to <code>.compute()</code> will automatically use this scheduler and its set of workers.</p> <pre><code>&gt;&gt;&gt; %time ds.data[\"PartType0\"][\"Masses\"].sum().compute()\nCPU times: user 5.11 s, sys: 1.42 s, total: 6.53 s\nWall time: 24.7 s\n\n52722.6796875 code_mass\n</code></pre> <p>The progress bar, we could use for the default scheduler (before initializing <code>LocalCluster</code>), is unavailable for the distributed scheduler. However, we can still view the progress of this task as it executes using its status dashboard (as a webpage in a new browser tab or within jupyter lab). You can find it by clicking on the \"Dashboard\" link above. If running this notebook server remotely, e.g. on a login node of a HPC cluster, you may have to change the '127.0.0.1' part of the address to be the same machine name/IP.</p>"},{"location":"largedatasets/#running-a-slurmcluster","title":"Running a SLURMCluster","text":"<p>If you are working with HPC resources, such as compute clusters with common schedulers (e.g. SLURM), check out Dask-Jobqueue to automatically batch jobs spawning dask workers.</p> <p>Below is an example using the SLURMCluster. We configure the job and node resources before submitting the job via the <code>scale()</code> method.</p> <pre><code>&gt;&gt;&gt; from dask.distributed import Client\n&gt;&gt;&gt; from dask_jobqueue import SLURMCluster\n&gt;&gt;&gt; cluster = SLURMCluster(queue='p.large', cores=72, memory=\"500 GB\",\n&gt;&gt;&gt;                        processes=36,\n&gt;&gt;&gt;                        scheduler_options={\"dashboard_address\": \":8811\"})\n&gt;&gt;&gt; cluster.scale(jobs=1)  # submit 1 job for 1 node\n&gt;&gt;&gt; client = Client(cluster)\n\n&gt;&gt;&gt; from scida import load\n&gt;&gt;&gt; sim = load(\"TNG50-1\")\n&gt;&gt;&gt; ds = sim.get_dataset(99)\n&gt;&gt;&gt; %time ds.data[\"PartType0\"][\"Masses\"].sum().compute()\nCPU times: user 1.27 s, sys: 152 ms, total: 1.43 s\nWall time: 21.4 s\n&gt;&gt;&gt; client.shutdown()\n</code></pre> <p>The SLURM job will be killed by invoking <code>client.shutdown()</code> or if the spawning python process or ipython kernel dies. Make sure to properly handle exceptions, particularly in active jupyter notebooks, as allocated nodes might otherwise idle and not be cleaned up.</p>"},{"location":"series/","title":"Data series","text":""},{"location":"series/#series","title":"Series","text":"<p>Info</p> <p>If you want to run the code below, you need a folder containing multiple scida datasets as subfolders. Specify the path in load() to the base directory of the series. The example below uses an AREPO simulation, the TNG50-4 simulation, as a series of snapshots. This simulation can be downloaded from the TNG website or directly accessed online in the TNGLab.</p> <p>In the tutorial section, we have only considered individual data sets. Often data sets are given in a series (e.g. multiple snapshots of a simulation, multiple exposures in a survey). Loading this as a series provides convenient access to all contained objects.</p> <pre><code>&gt;&gt;&gt; from scida import load\n&gt;&gt;&gt; series = load(\"TNG50-4\") #(1)!\n</code></pre> <ol> <li>Pass the base path of the simulation.</li> </ol> <p>We can now access the individual data sets from the series object:</p> <pre><code>&gt;&gt;&gt; series[0] #(1)!\n</code></pre> <ol> <li>Alias for 'series.datasets[0]'</li> </ol> <p>Depending on the available metadata, we can select data sets by these.</p> <p>For example, cosmological simulations usually have information about their redshift:</p> <pre><code>&gt;&gt;&gt; snp = series.get_dataset(redshift=2.0)\n&gt;&gt;&gt; snp.header[\"Redshift\"]\n2.0020281392528516\n</code></pre>"},{"location":"supported_data/","title":"Supported datasets","text":""},{"location":"supported_data/#supported-datasets","title":"Supported datasets","text":"<p>The following table shows a selection of supported datasets. The table is not exhaustive, but should give an idea of the range of supported datasets. If you want to use a dataset that is not listed here, read on here and consider opening an issue or contact us directly.</p> Name Support Description AURIGA Cosmological zoom-in galaxy formation simulations EAGLE Cosmological galaxy formation simulations FIRE2 Cosmological zoom-in galaxy formation simulations FLAMINGO Cosmological galaxy formation simulations Gaia <sup>[download]</sup> Observations of a billion nearby stars Illustris Cosmological galaxy formation simulations LGalaxies <sup>[1]</sup> Semi-analytical model for Millenium simulations SDSS DR16 Observations for millions of galaxies SIMBA Cosmological galaxy formation simulations TNG<sup>[2]</sup> Cosmological galaxy formation simulations TNG-Cluster Cosmological zoom-in galaxy formation simulations <p>A  checkmark indicates support out-of-the-box, a  checkmark indicates work-in-progress support or the need to create a suitable configuration file. A  checkmark indicates support for converted HDF5 versions of the original data.</p>"},{"location":"supported_data/#dataset-details","title":"Dataset Details","text":""},{"location":"supported_data/#lgalaxies","title":"LGalaxies","text":"<p>Access via individual datasets are supported, e.g.:</p> <pre><code>&gt;&gt;&gt; from scida import load\n&gt;&gt;&gt; load(\"LGal_Ayromlou2021_snap58.hdf5\")\n</code></pre> <p>while access to the series at once (i.e. loading all data for all snapshots in a folder) is not supported.</p>"},{"location":"supported_data/#the-tng-simulation-suite","title":"The TNG Simulation Suite","text":""},{"location":"supported_data/#overview","title":"Overview","text":"<p>The IllustrisTNG project is a series of large-scale cosmological magnetohydrodynamical simulations of galaxy formation. The data is available at www.tng-project.org.</p>"},{"location":"supported_data/#demo-data","title":"Demo data","text":"<p>Many of the examples in this documentation use the TNG50-4 simulation. In particular, we make a snapshot and group catalog available to run these examples. You can download and extract the snapshot and its group catalog from the TNG50-4 test data using the following commands:</p> <pre><code>wget \"https://heibox.uni-heidelberg.de/f/dc65a8c75220477eb62d/?dl=1\" -O snapshot.tar.gz\ntar -xvf snapshot.tar.gz\nwget \"https://heibox.uni-heidelberg.de/f/ff27fb6975fb4dc391ef/?dl=1\" -O catalog.tar.gz\ntar -xvf catalog.tar.gz\n</code></pre> <p>These files are exactly the same files that can be downloaded from the official IllustrisTNG data release.</p> <p>The snapshot and group catalog should be placed in the same folder. Then you can load the snapshot with <code>ds = load(\"./snapdir_030\")</code>. If you are executing the code from a different folder, you need to adjust the path accordingly. The group catalog should automatically be detected when available in the same parent folder as the snapshot, otherwise you can also pass the path to the catalog via the <code>catalog</code> keyword to <code>load()</code>.</p>"},{"location":"supported_data/#tnglab","title":"TNGLab","text":"<p>The TNGLab is a web-based analysis platform running a JupyterLab instance with access to dedicated computational resources and all TNG data sets to provide a convenient way to run analysis code on the TNG data sets. As TNGLab supports scida, it is a great way to get started and for running the examples.</p> <p>In order to run the examples which use the demo data, replace</p> <pre><code>ds = load(\"./snapdir_030\")\n</code></pre> <p>with</p> <pre><code>ds = load(\"/home/tnguser/sims.TNG/TNG50-4/output/snapdir_030\")\n</code></pre> <p>for these examples.</p> <p>Alternatively, you can use</p> <pre><code>sim = load(\"TNG50-4\")\nds = sim.get_dataset(30)\n</code></pre> <p>where \"TNG50-4\" is a pre-defined shortcut to the TNG50-4 simulation path on TNGLab. After having loaded the simulation, we request the snapshot \"30\" as used in the demo data. Custom shortcuts can be defined in the simulation configuration.</p>"},{"location":"supported_data/#supported-file-formats-and-their-structure","title":"Supported file formats and their structure","text":"<p>Here, we discuss the requirements for easy extension/support of new datasets. Currently, input files need to have one of the following formats:</p> <ul> <li>hdf5</li> <li>multi-file hdf5: We assume a directory containing hdf5 files of the pattern prefix.XXX.hdf5, where prefix will be determined automatically and XXX is a contiguous list of integers indicating the order of hdf5 files to be merged. Hdf5 files are expected to have the same structure and all fields, i.e. hdf5 datasets, will be concatenated along their first axis.</li> <li>zarr</li> </ul> <p>Support for FITS is work-in-progress, also see here for a proof-of-concept.</p> <p>Scida and above file formats use a hierarchical structure to store data with three fundamental objects:</p> <ul> <li>Groups are containers for other groups or datasets.</li> <li>Datasets are multidimensional arrays of a homogeneous type, usually bundled into some Group.</li> <li>Attributes provide various metadata.</li> </ul> <p>At this point, we only support unstructured datasets, i.e. datasets that do not depend on the memory layout for their interpretation. For example, this implies that simulation codes utilizing uniform or adaptive grids are not supported.</p> <p>We explicitly support simulations run with the following codes:</p> <ul> <li>Gadget</li> <li>Gizmo</li> <li>Arepo</li> <li>Swift</li> </ul>"},{"location":"units/","title":"Units","text":""},{"location":"units/#units","title":"Units","text":"<p>Info</p> <p>If you want to run the code below, consider downloading the demo data or use the TNGLab online.</p>"},{"location":"units/#loading-data-with-units","title":"Loading data with units","text":"<p>Loading data sets with</p> <pre><code>from scida import load\nds = load(\"./snapdir_030\")\n</code></pre> <p>will automatically attach units to the data. This can be deactivated by passing \"units=False\" to the load function. By default, code units are used, alternatively, cgs conversions can be applied by passing \"units='cgs'\" (experimental).</p> <p>Units are introduced via the pint package, see there for more details.</p> <p>Sometimes, the units cannot be inferred or parsed. This will be indicated in the output following the call to <code>load()</code>, e.g. as:</p> <pre><code>Missing units for 1 fields.\nFields with missing units:\n  - /PartType0/FieldWithoutUnits (missing)\n</code></pre> <p>You can obtain more information on the cause by setting</p> <pre><code>&gt;&gt;&gt; import logging\n&gt;&gt;&gt; logging.getLogger().setLevel(logging.DEBUG)\n</code></pre> <p>before calling <code>load()</code>.</p> <p>Units for custom datasets can also be manually be specified using unit files, see here.</p>"},{"location":"units/#using-data-with-units","title":"Using data with units","text":"<pre><code>&gt;&gt;&gt; gas = ds.data[\"PartType0\"]\n&gt;&gt;&gt; gas[\"Coordinates\"]\ndask.array&lt;mul, shape=(18540104, 3), dtype=float64, chunksize=(5592405, 3), chunktype=numpy.ndarray&gt; &lt;Unit('code_length')&gt;\n</code></pre> <p>We can access the underlying dask array and the units separately:</p> <pre><code>&gt;&gt;&gt; gas[\"Coordinates\"].magnitude, gas[\"Coordinates\"].units\n(dask.array&lt;mul, shape=(18540104, 3), dtype=float64, chunksize=(5592405, 3), chunktype=numpy.ndarray&gt;,\n &lt;Unit('code_length')&gt;)\n</code></pre>"},{"location":"units/#unit-conversions","title":"Unit conversions","text":"<p>We can change units for evaluation as desired:</p> <pre><code>&gt;&gt;&gt; coords = gas[\"Coordinates\"]\n&gt;&gt;&gt; coords.to(\"cm\")\n&gt;&gt;&gt; # here the default system is cgs, thus we get the same result from\n&gt;&gt;&gt; coords.to_base_units()\ndask.array&lt;mul, shape=(18540104, 3), dtype=float64, chunksize=(5592405, 3), chunktype=numpy.ndarray&gt; &lt;Unit('centimeter')&gt;\n</code></pre>"},{"location":"units/#the-unit-registry","title":"The unit registry","text":"<p>The unit registry keeps all units. There is no global registry, but each dataset has its own registry as attribute ureg. The use of a global registry (or lack thereof here) can lead to some confusion, please consult the pint documentation when in doubt.</p> <pre><code>&gt;&gt;&gt; ureg = ds.ureg\n&gt;&gt;&gt; # get the unit meter\n&gt;&gt;&gt; ureg(\"m\")\n1 &lt;Unit('meter')&gt;\n</code></pre> <pre><code>&gt;&gt;&gt; # define an array with units meter (dask arrays analogously)\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.arange(10) * ureg(\"m\")\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) &lt;Unit('meter')&gt;\n</code></pre>"},{"location":"units/#synthesize-new-dask-arrays-with-units","title":"Synthesize new dask arrays with units","text":"<pre><code>&gt;&gt;&gt; energy_restframe = (gas[\"Masses\"]*ureg(\"c\")**2).to(\"erg\")  # E=mc^2\n&gt;&gt;&gt; energy_restframe\ndask.array&lt;mul, shape=(18540104,), dtype=float64, chunksize=(18540104,), chunktype=numpy.ndarray&gt; &lt;Unit('erg')&gt;\n</code></pre>"},{"location":"units/#custom-units","title":"Custom units","text":"<pre><code>&gt;&gt;&gt; ureg.define(\"halfmeter = 0.5 * m\")\n&gt;&gt;&gt; # first particle coordinates in halfmeters\n&gt;&gt;&gt; coords.to(\"halfmeter\")[0].compute()\narray([6.64064027e+23, 2.23858253e+24, 1.94176712e+24]) &lt;Unit('halfmeter')&gt;\n</code></pre>"},{"location":"visualization/","title":"Visualization","text":""},{"location":"visualization/#visualization","title":"Visualization","text":"<p>Info</p> <p>If you want to run the code below, consider downloading the demo data or use the TNGLab online.</p>"},{"location":"visualization/#creating-plots","title":"Creating plots","text":"<p>As we often use large datasets, we need to be careful with the amount of data we plot. Generally, we reduce the data by either selecting a subset or reducing it prior to plotting. For example, we can select a subset of particles by applying a cut on a given field.</p> Selecting a subset of particles<pre><code>from scida import load\nimport matplotlib.pyplot as plt\n\nds = load(\"./snapdir_030\")\ndens = ds.data[\"PartType0\"][\"Density\"][:10000].to(\"Msun/kpc^3\").magnitude.compute()  # (1)!\ntemp = ds.data[\"PartType0\"][\"Temperature\"][:10000].to(\"K\").magnitude.compute()\n\nfig, ax = plt.subplots(figsize=(6, 3))\nax.plot(dens, temp, \"o\", markersize=0.5)\nax.set_xscale(\"log\")\nax.set_yscale(\"log\")\nax.set_xlabel(r\"$\\rho$ (M$_\\odot$/kpc$^3$))\")\nax.set_ylabel(r\"T (K)\")\nax.xaxis.tick_top()\nax.xaxis.set_label_position('top')\nplt.show()\n</code></pre> <ol> <li>Note the subselection of the first 10000 particles and conversion to a numpy array. Replace this operation with a meaninguful selection operation (e.g. a certain spatial region selection).</li> </ol> <p> </p> Simple scatter plot. <p>Instead of subselection, we sometimes want to visualize all of the data. We can do so by first applying reduction operations using dask. A common example would be a 2D histogram.</p> 2D histograms<pre><code>import dask.array as da\nimport numpy as np\nfrom scida import load\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LogNorm\n\nds = load(\"./snapdir_030\")\ndens10 = da.log10(ds.data[\"PartType0\"][\"Density\"].to(\"Msun/kpc^3\").magnitude)\ntemp10 = da.log10(ds.data[\"PartType0\"][\"Temperature\"].to(\"K\").magnitude)\n\nbins = [np.linspace(1, 9, 256 + 1), np.linspace(3.5, 8, 128 + 1)]\nhist, xedges, yedges = da.histogram2d(dens10, temp10, bins=bins)\nhist, xedges, yedges = hist.compute(), xedges.compute(), yedges.compute()\nextent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n\nfig, ax = plt.subplots(figsize=(6,3))\nax.imshow(hist.T, origin=\"lower\", extent=extent, aspect=\"auto\", cmap=\"viridis\",\n          norm=LogNorm(), interpolation=\"none\")\nax.set_xlabel(r\"$\\log_{10}$($\\rho$/(M$_\\odot$/kpc$^3$))\")\nax.set_ylabel(r\"$\\log_{10}$(T/K)\")\nax.xaxis.tick_top()\nax.xaxis.set_label_position('top')\nplt.show()\n</code></pre> <p> </p> Density-temperature phase-space diagram for TNG50-4"},{"location":"visualization/#interactive-visualization","title":"Interactive visualization","text":"<p>Info</p> <p>This example requires the <code>holoviews</code>, <code>datashader</code> and <code>bokeh</code> packages installed. Make sure that these holoviews examples work before continuing. The conversion from hdf5-backed dask arrays to dataframes seems to be broken with recent dask versions, this example might currently not work.</p> <p>We can do interactive visualization with holoviews. For example, we can create a scatter plot of the particle positions.</p> <pre><code>import holoviews as hv\nimport holoviews.operation.datashader as hd\nimport datashader as dshdr\nfrom scida import load\n\nds = load(\"./snapdir_030\")\nddf = ds.data[\"PartType0\"].get_dataframe([\"Coordinates0\", \"Coordinates1\", \"Masses\"])  # (1)!\n\nhv.extension(\"bokeh\")\nshaded = hd.datashade(hv.Points(ddf, [\"Coordinates0\", \"Coordinates1\"]), cmap=\"viridis\", interpolation=\"linear\",\n                      aggregator=dshdr.sum(\"Masses\"), x_sampling=5, y_sampling=5)\nhd.dynspread(shaded, threshold=0.9, max_px=50).opts(bgcolor=\"black\", xaxis=None, yaxis=None, width=500, height=500)\n</code></pre> <ol> <li>Visualization operations in holowview primarily run with dataframes, which we thus need to create using this wrapper for given fields.</li> </ol> <p></p>"},{"location":"api/base_api/","title":"Basic","text":""},{"location":"api/base_api/#base-api","title":"Base API","text":""},{"location":"api/base_api/#convenience-functions","title":"Convenience functions","text":"<p>Commonly used functions and classes. There are two main object types in scida:</p> <ul> <li>Dataset</li> <li>DatasetSeries</li> </ul> <p>All specialized classes derive from these two classes. Generally, we do not have to instantiate the correct class ourselves, but can use the load function to load a dataset. This function will furthermore adjust the underlying class with additional functionality, such as units, depending on the data set.</p>"},{"location":"api/base_api/#scida.convenience.load","title":"<code>load(path, units=True, unitfile='', overwrite=False, force_class=None, **kwargs)</code>","text":"<p>Load a dataset or dataset series from a given path. This function will automatically determine the best-matching class to use and return the initialized instance.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to dataset or dataset series. Usually the base folder containing all files of a given dataset/series.</p> required <code>units</code> <code>bool | str</code> <p>Whether to load units.</p> <code>True</code> <code>unitfile</code> <code>str</code> <p>Can explicitly pass path to a unitfile to use.</p> <code>''</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite an existing cache.</p> <code>False</code> <code>force_class</code> <code>type | None</code> <p>Force a specific class to be used.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Dataset, DatasetSeries]:</code> <p>Initialized dataset or dataset series.</p>"},{"location":"api/base_api/#datasets-and-series","title":"Datasets and Series","text":"<p>The Dataset class is the base class for all data sets. Collections of datasets are represented by the DatasetSeries class. These objects are not instantiated directly, but are created by the load function.</p> <p>For example, AREPO snapshots are defined by the ArepoSnapshot class. The load function will select a class based on the following criteria (descending priority):</p> <ol> <li>The class is passed to load as the force_class argument.</li> <li>The class is specified in the simulation configuration, see here.</li> <li>The class implements validate_path() returning True for the given path if the class is applicable.</li> </ol>"},{"location":"api/moduleindex/","title":"Index","text":""},{"location":"api/moduleindex/#advanced-api","title":"Advanced API","text":"<p>For now, we list the remaining package documentation here.</p> <p>Scida is a python package for reading and analyzing scientific big data.</p>"},{"location":"api/moduleindex/#scida.config","title":"<code>config</code>","text":"<p>Configuration handling.</p>"},{"location":"api/moduleindex/#scida.config.combine_configs","title":"<code>combine_configs(configs, mode='overwrite_keys')</code>","text":"<p>Combine multiple configurations recursively. Replacing entries in the first config with entries from the latter</p> <p>Parameters:</p> Name Type Description Default <code>configs</code> <code>list[dict[str, Any]]</code> <p>The list of configurations to combine.</p> required <code>mode</code> <code>str</code> <p>The mode for combining the configurations. \"overwrite_keys\": overwrite keys in the first config with keys from the latter (default). \"overwrite_values\": overwrite values in the first config with values from the latter.</p> <code>'overwrite_keys'</code> <p>Returns:</p> Type Description <code>dict</code> <p>The combined configuration.</p> Source code in <code>src/scida/config.py</code> <pre><code>def combine_configs(\n    configs: list[dict[str, Any]], mode: str = \"overwrite_keys\"\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Combine multiple configurations recursively.\n    Replacing entries in the first config with entries from the latter\n\n    Parameters\n    ----------\n    configs: list\n        The list of configurations to combine.\n    mode: str\n        The mode for combining the configurations.\n        \"overwrite_keys\": overwrite keys in the first config with keys from the latter (default).\n        \"overwrite_values\": overwrite values in the first config with values from the latter.\n\n    Returns\n    -------\n    dict\n        The combined configuration.\n    \"\"\"\n    if mode == \"overwrite_values\":\n\n        def mergefunc_values(a, b):\n            \"\"\"merge values\"\"\"\n            if b is None:\n                return a\n            return b  # just overwrite by latter entry\n\n        mergefunc_keys = None\n    elif mode == \"overwrite_keys\":\n\n        def mergefunc_keys(a, b):\n            \"\"\"merge keys\"\"\"\n            if b is None:\n                return a\n            return b\n\n        mergefunc_values = None\n    else:\n        raise ValueError(\"Unknown mode '%s'\" % mode)\n    conf = configs[0]\n    for c in configs[1:]:\n        merge_dicts_recursively(\n            conf, c, mergefunc_keys=mergefunc_keys, mergefunc_values=mergefunc_values\n        )\n    return conf\n</code></pre>"},{"location":"api/moduleindex/#scida.config.copy_defaultconfig","title":"<code>copy_defaultconfig(overwrite=False)</code>","text":"<p>Copy the configuration example to the user's home directory.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <p>Overwrite existing configuration file.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/config.py</code> <pre><code>def copy_defaultconfig(overwrite=False) -&gt; None:\n    \"\"\"\n    Copy the configuration example to the user's home directory.\n    Parameters\n    ----------\n    overwrite: bool\n        Overwrite existing configuration file.\n\n    Returns\n    -------\n    None\n    \"\"\"\n\n    path_user = os.path.expanduser(\"~\")\n    path_confdir = os.path.join(path_user, \".config/scida\")\n    if not os.path.exists(path_confdir):\n        os.makedirs(path_confdir, exist_ok=True)\n    path_conf = os.path.join(path_confdir, \"config.yaml\")\n    if os.path.exists(path_conf) and not overwrite:\n        raise ValueError(\"Configuration file already exists at '%s'\" % path_conf)\n    resource = importlib.resources.files(\"scida.configfiles\").joinpath(\"config.yaml\")\n    with resource.open(\"r\") as file:\n        content = file.read()\n        with open(path_conf, \"w\") as newfile:\n            newfile.write(content)\n</code></pre>"},{"location":"api/moduleindex/#scida.config.get_config","title":"<code>get_config(reload=False, update_global=True)</code>","text":"<p>Load the configuration from the default path.</p> <p>Parameters:</p> Name Type Description Default <code>reload</code> <code>bool</code> <p>Reload the configuration, even if it has already been loaded.</p> <code>False</code> <code>update_global</code> <p>Update the global configuration dictionary.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>The configuration dictionary.</p> Source code in <code>src/scida/config.py</code> <pre><code>def get_config(reload: bool = False, update_global=True) -&gt; dict:\n    \"\"\"\n    Load the configuration from the default path.\n\n    Parameters\n    ----------\n    reload: bool\n        Reload the configuration, even if it has already been loaded.\n    update_global: bool\n        Update the global configuration dictionary.\n\n    Returns\n    -------\n    dict\n        The configuration dictionary.\n    \"\"\"\n    global _conf\n    prefix = \"SCIDA_\"\n    envconf = {\n        k.replace(prefix, \"\").lower(): v\n        for k, v in os.environ.items()\n        if k.startswith(prefix)\n    }\n\n    # in any case, we make sure that there is some config in the default path.\n    path_confdir = _access_confdir()\n    path_conf = os.path.join(path_confdir, \"config.yaml\")\n\n    # next, we load the config from the default path, unless explicitly overridden.\n    path = envconf.pop(\"config_path\", None)\n    if path is None:\n        path = path_conf\n    if not reload and len(_conf) &gt; 0:\n        return _conf\n    config = get_config_fromfile(path)\n    if config.get(\"copied_default\", False):\n        print(\n            \"Warning! Using default configuration. Please adjust/replace in '%s'.\"\n            % path\n        )\n\n    config.update(**envconf)\n    if update_global:\n        _conf = config\n    return config\n</code></pre>"},{"location":"api/moduleindex/#scida.config.get_config_fromfile","title":"<code>get_config_fromfile(resource)</code>","text":"<p>Load config from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>str</code> <p>The name of the resource or file path.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/config.py</code> <pre><code>def get_config_fromfile(resource: str) -&gt; dict[str, Any]:\n    \"\"\"\n    Load config from a YAML file.\n    Parameters\n    ----------\n    resource\n        The name of the resource or file path.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if resource == \"\":\n        raise ValueError(\"Config name cannot be empty.\")\n    # order (in descending order of priority):\n    # 1. absolute path?\n    path = os.path.expanduser(resource)\n    if os.path.isabs(path):\n        with open(path, \"r\") as file:\n            return cast(dict[str, Any], yaml.safe_load(file))\n    # 2. non-absolute path?\n    # 2.1. check local (project-specific) path\n    if os.path.isfile(resource):\n        with open(resource, \"r\") as file:\n            return cast(dict[str, Any], yaml.safe_load(file))\n    # 2.2. check ~/.config/scida/\n    bpath = os.path.expanduser(\"~/.config/scida\")\n    path = os.path.join(bpath, resource)\n    if os.path.isfile(path):\n        with open(path, \"r\") as file:\n            return cast(dict[str, Any], yaml.safe_load(file))\n    # 2.3. check scida package resources\n    resource_path = \"scida.configfiles\"\n    resource_elements = resource.split(\"/\")\n    rname = resource_elements[-1]\n    if len(resource_elements) &gt; 1:\n        resource_path += \".\" + \".\".join(resource_elements[:-1])\n    resource = importlib.resources.files(resource_path).joinpath(rname)\n    with resource.open(\"r\") as file:\n        return cast(dict[str, Any], yaml.safe_load(file))\n</code></pre>"},{"location":"api/moduleindex/#scida.config.get_config_fromfiles","title":"<code>get_config_fromfiles(paths, subconf_keys=None)</code>","text":"<p>Load and merge multiple YAML config files</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[str]</code> <p>Paths to the config files.</p> required <code>subconf_keys</code> <code>list[str] | None</code> <p>The keys to the correct sub configuration within each config.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/config.py</code> <pre><code>def get_config_fromfiles(\n    paths: list[str], subconf_keys: list[str] | None = None\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Load and merge multiple YAML config files\n    Parameters\n    ----------\n    paths\n        Paths to the config files.\n    subconf_keys\n        The keys to the correct sub configuration within each config.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    confs = []\n    for path in paths:\n        confs.append(get_config_fromfile(path))\n    conf: dict[str, Any] = {}\n    for confdict in confs:\n        conf = merge_dicts_recursively(conf, confdict)\n    return conf\n</code></pre>"},{"location":"api/moduleindex/#scida.config.get_simulationconfig","title":"<code>get_simulationconfig()</code>","text":"<p>Get the simulation configuration. Search regular user config file, scida simulation config file, and user's simulation config file.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The simulation configuration.</p> Source code in <code>src/scida/config.py</code> <pre><code>def get_simulationconfig():\n    \"\"\"\n    Get the simulation configuration.\n    Search regular user config file, scida simulation config file, and user's simulation config file.\n\n    Returns\n    -------\n    dict\n        The simulation configuration.\n    \"\"\"\n    conf_user = get_config()\n    conf_sims = get_config_fromfile(\"simulations.yaml\")\n    conf_sims_user = _get_simulationconfig_user()\n\n    confs_base = [conf_sims, conf_user]\n    if conf_sims_user is not None:\n        confs_base.append(conf_sims_user)\n    # we only want to overwrite keys within \"data\", otherwise no merging of simkeys would take place\n    confs = []\n    for c in confs_base:\n        if \"data\" in c:\n            confs.append(c[\"data\"])\n\n    conf_sims = combine_configs(confs, mode=\"overwrite_keys\")\n    conf_sims = {\"data\": conf_sims}\n\n    return conf_sims\n</code></pre>"},{"location":"api/moduleindex/#scida.config.merge_dicts_recursively","title":"<code>merge_dicts_recursively(dict_a, dict_b, path=None, mergefunc_keys=None, mergefunc_values=None)</code>","text":"<p>Merge two dictionaries recursively.</p> <p>Parameters:</p> Name Type Description Default <code>dict_a</code> <code>dict[str, Any]</code> <p>The first dictionary.</p> required <code>dict_b</code> <code>dict[str, Any]</code> <p>The second dictionary.</p> required <code>path</code> <code>list[str] | None</code> <p>The path to the current node.</p> <code>None</code> <code>mergefunc_keys</code> <code>Callable[[Any, Any], Any] | None</code> <p>The function to use for merging dict keys. If None, we recursively enter the dictionary.</p> <code>None</code> <code>mergefunc_values</code> <code>Callable[[Any, Any], Any] | None</code> <p>The function to use for merging dict values. If None, collisions will raise an exception.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> Source code in <code>src/scida/config.py</code> <pre><code>def merge_dicts_recursively(\n    dict_a: dict[str, Any],\n    dict_b: dict[str, Any],\n    path: list[str] | None = None,\n    mergefunc_keys: Callable[[Any, Any], Any] | None = None,\n    mergefunc_values: Callable[[Any, Any], Any] | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Merge two dictionaries recursively.\n    Parameters\n    ----------\n    dict_a\n        The first dictionary.\n    dict_b\n        The second dictionary.\n    path\n        The path to the current node.\n    mergefunc_keys: callable\n        The function to use for merging dict keys.\n        If None, we recursively enter the dictionary.\n    mergefunc_values: callable\n        The function to use for merging dict values.\n        If None, collisions will raise an exception.\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    if path is None:\n        path = []\n    for key in dict_b:\n        if key in dict_a:\n            if mergefunc_keys is not None:\n                dict_a[key] = mergefunc_keys(dict_a[key], dict_b[key])\n            elif isinstance(dict_a[key], dict) and isinstance(dict_b[key], dict):\n                merge_dicts_recursively(\n                    dict_a[key],\n                    dict_b[key],\n                    path + [str(key)],\n                    mergefunc_keys=mergefunc_keys,\n                    mergefunc_values=mergefunc_values,\n                )\n            elif dict_a[key] == dict_b[key]:\n                pass  # same leaf value\n            else:\n                if mergefunc_values is not None:\n                    dict_a[key] = mergefunc_values(dict_a[key], dict_b[key])\n                else:\n                    raise Exception(\"Conflict at %s\" % \".\".join(path + [str(key)]))\n        else:\n            dict_a[key] = dict_b[key]\n    return dict_a\n</code></pre>"},{"location":"api/moduleindex/#scida.convenience","title":"<code>convenience</code>","text":""},{"location":"api/moduleindex/#scida.convenience.download_and_extract","title":"<code>download_and_extract(url, path, progressbar=True, overwrite=False)</code>","text":"<p>Download and extract a tar.gz archive from a given url.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The url to download from.</p> required <code>path</code> <code>Path</code> <p>The path to download to.</p> required <code>progressbar</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite an existing file.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the extracted file(s).</p> Source code in <code>src/scida/convenience.py</code> <pre><code>def download_and_extract(\n    url: str, path: pathlib.Path, progressbar: bool = True, overwrite: bool = False\n):\n    \"\"\"\n    Download and extract a tar.gz archive from a given url.\n\n    Parameters\n    ----------\n    url: str\n        The url to download from.\n    path: pathlib.Path\n        The path to download to.\n    progressbar: bool\n        Whether to show a progress bar.\n    overwrite: bool\n        Whether to overwrite an existing file.\n    Returns\n    -------\n    str\n        The path to the extracted file(s).\n    \"\"\"\n    _download(url, path, progressbar=progressbar, overwrite=overwrite)\n    tar = tarfile.open(path, \"r:gz\")\n    for t in tar:\n        if t.isdir():\n            t.mode = int(\"0755\", base=8)\n        else:\n            t.mode = int(\"0644\", base=8)\n    # if python version &gt;= 3.12, need filter argument (required from 3.14 onwards)\n    if sys.version_info &gt;= (3, 12):\n        tar.extractall(path.parents[0], filter=\"fully_trusted\")\n    else:\n        tar.extractall(path.parents[0])\n    foldername = tar.getmembers()[0].name  # parent folder of extracted tar.gz\n    tar.close()\n    os.remove(path)\n    return os.path.join(path.parents[0], foldername)\n</code></pre>"},{"location":"api/moduleindex/#scida.convenience.find_path","title":"<code>find_path(path, overwrite=False)</code>","text":"<p>Find path to dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> required <code>overwrite</code> <p>Only for remote datasets. Whether to overwrite an existing download.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> Source code in <code>src/scida/convenience.py</code> <pre><code>def find_path(path, overwrite=False) -&gt; str:\n    \"\"\"\n    Find path to dataset.\n\n    Parameters\n    ----------\n    path: str\n    overwrite: bool\n        Only for remote datasets. Whether to overwrite an existing download.\n\n    Returns\n    -------\n    str\n\n    \"\"\"\n    config = get_config()\n    path = os.path.expanduser(path)\n    if os.path.exists(path):\n        # datasets on disk\n        pass\n    elif len(path.split(\":\")) &gt; 1:\n        # check different alternative backends\n        databackend = path.split(\"://\")[0]\n        dataname = path.split(\"://\")[1]\n        if databackend in [\"http\", \"https\"]:\n            # dataset on the internet\n            savepath = config.get(\"download_path\", None)\n            if savepath is None:\n                print(\n                    \"Have not specified 'download_path' in config. Using 'cache_path' instead.\"\n                )\n                savepath = config.get(\"cache_path\")\n            savepath = os.path.expanduser(savepath)\n            savepath = pathlib.Path(savepath)\n            savepath.mkdir(parents=True, exist_ok=True)\n            urlhash = str(\n                int(hashlib.sha256(path.encode(\"utf-8\")).hexdigest(), 16) % 10**8\n            )\n            savepath = savepath / (\"download\" + urlhash)\n            # determine filename and whether this is an archive\n            url_basename = os.path.basename(path.split(\"?\")[0].rstrip(\"/\"))\n            is_archive = url_basename.endswith((\".tar.gz\", \".tgz\"))\n            filename = \"archive.tar.gz\" if is_archive else (url_basename or \"download\")\n            if not savepath.exists():\n                os.makedirs(savepath, exist_ok=True)\n            elif overwrite:\n                # delete all files in folder\n                for f in os.listdir(savepath):\n                    fp = os.path.join(savepath, f)\n                    if os.path.isfile(fp):\n                        os.unlink(fp)\n                    else:\n                        shutil.rmtree(fp)\n            foldercontent = [f for f in savepath.glob(\"*\")]\n            if len(foldercontent) == 0:\n                filepath = savepath / filename\n                if is_archive:\n                    extractpath = download_and_extract(\n                        path, filepath, progressbar=True, overwrite=overwrite\n                    )\n                else:\n                    _download(\n                        path, filepath, progressbar=True, overwrite=overwrite\n                    )\n                    extractpath = savepath\n            else:\n                extractpath = savepath\n            extractpath = pathlib.Path(extractpath)\n\n            # count folders in savepath\n            nfolders = len([f for f in extractpath.glob(\"*\") if f.is_dir()])\n            nobjects = len([f for f in extractpath.glob(\"*\") if f.is_dir()])\n            if nobjects == 1 and nfolders == 1:\n                extractpath = (\n                    extractpath / [f for f in extractpath.glob(\"*\") if f.is_dir()][0]\n                )\n            path = extractpath\n        elif databackend == \"testdata\":\n            path = get_testdata(dataname)\n        else:\n            # potentially custom dataset.\n            resources = config.get(\"resources\", {})\n            if databackend not in resources:\n                raise ValueError(\"Unknown resource '%s'\" % databackend)\n            r = resources[databackend]\n            if dataname not in r:\n                raise ValueError(\n                    \"Unknown dataset '%s' in resource '%s'\" % (dataname, databackend)\n                )\n            path = os.path.expanduser(r[dataname][\"path\"])\n    else:\n        found = False\n        if \"datafolders\" in config:\n            for folder in config[\"datafolders\"]:\n                folder = os.path.expanduser(folder)\n                if os.path.exists(os.path.join(folder, path)):\n                    path = os.path.join(folder, path)\n                    found = True\n                    break\n        if not found:\n            raise ValueError(\"Specified path '%s' unknown.\" % path)\n    return path\n</code></pre>"},{"location":"api/moduleindex/#scida.convenience.get_dataset","title":"<code>get_dataset(name=None, props=None)</code>","text":"<p>Get dataset by name or properties.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>Name or alias of dataset.</p> <code>None</code> <code>props</code> <p>Properties to match.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <p>Dataset name.</p> Source code in <code>src/scida/convenience.py</code> <pre><code>def get_dataset(name=None, props=None):\n    \"\"\"\n    Get dataset by name or properties.\n    Parameters\n    ----------\n    name: str | None\n        Name or alias of dataset.\n    props: dict | None\n        Properties to match.\n\n    Returns\n    -------\n    str:\n        Dataset name.\n\n    \"\"\"\n    dnames = get_dataset_candidates(name=name, props=props)\n    if len(dnames) &gt; 1:\n        raise ValueError(\"Too many dataset candidates.\")\n    elif len(dnames) == 0:\n        raise ValueError(\"No dataset candidate found.\")\n    return dnames[0]\n</code></pre>"},{"location":"api/moduleindex/#scida.convenience.get_dataset_by_name","title":"<code>get_dataset_by_name(name)</code>","text":"<p>Get dataset name from alias or name found in the configuration files.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name or alias of dataset.</p> required <p>Returns:</p> Type Description <code>str</code> Source code in <code>src/scida/convenience.py</code> <pre><code>def get_dataset_by_name(name: str) -&gt; str | None:\n    \"\"\"\n    Get dataset name from alias or name found in the configuration files.\n\n    Parameters\n    ----------\n    name: str\n        Name or alias of dataset.\n\n    Returns\n    -------\n    str\n    \"\"\"\n    dname = None\n    c = get_config()\n    if \"datasets\" not in c:\n        return dname\n    datasets = copy.deepcopy(c[\"datasets\"])\n    if name in datasets:\n        dname = name\n    else:\n        # could still be an alias\n        for k, v in datasets.items():\n            if \"aliases\" not in v:\n                continue\n            if name in v[\"aliases\"]:\n                dname = k\n                break\n    return dname\n</code></pre>"},{"location":"api/moduleindex/#scida.convenience.get_dataset_candidates","title":"<code>get_dataset_candidates(name=None, props=None)</code>","text":"<p>Get dataset candidates by name or properties.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>Name or alias of dataset.</p> <code>None</code> <code>props</code> <p>Properties to match.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]:</code> <p>List of candidate dataset names.</p> Source code in <code>src/scida/convenience.py</code> <pre><code>def get_dataset_candidates(name=None, props=None):\n    \"\"\"\n    Get dataset candidates by name or properties.\n\n    Parameters\n    ----------\n    name: str | None\n        Name or alias of dataset.\n    props: dict | None\n        Properties to match.\n\n    Returns\n    -------\n    list[str]:\n        List of candidate dataset names.\n\n    \"\"\"\n    if name is not None:\n        dnames = [get_dataset_by_name(name)]\n        return dnames\n    if props is not None:\n        dnames = get_datasets_by_props(**props)\n        return dnames\n    raise ValueError(\"Need to specify name or properties.\")\n</code></pre>"},{"location":"api/moduleindex/#scida.convenience.get_datasets_by_props","title":"<code>get_datasets_by_props(**kwargs)</code>","text":"<p>Get dataset names by properties.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Properties to match.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[str]:</code> <p>List of dataset names.</p> Source code in <code>src/scida/convenience.py</code> <pre><code>def get_datasets_by_props(**kwargs):\n    \"\"\"\n    Get dataset names by properties.\n\n    Parameters\n    ----------\n    kwargs: dict\n        Properties to match.\n\n    Returns\n    -------\n    list[str]:\n        List of dataset names.\n    \"\"\"\n    dnames: list[str] = []\n    c = get_config()\n    if \"datasets\" not in c:\n        return dnames\n    datasets = copy.deepcopy(c[\"datasets\"])\n    for k, v in datasets.items():\n        props = v.get(\"properties\", {})\n        match = True\n        for pk, pv in kwargs.items():\n            if pk not in props:\n                match = False\n                break\n            if props[pk] != pv:\n                match = False\n                break\n        if match:\n            dnames.append(k)\n    return dnames\n</code></pre>"},{"location":"api/moduleindex/#scida.convenience.get_testdata","title":"<code>get_testdata(name)</code>","text":"<p>Get path to test data identifier.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of test data.</p> required <p>Returns:</p> Type Description <code>str</code> Source code in <code>src/scida/convenience.py</code> <pre><code>def get_testdata(name: str) -&gt; str:\n    \"\"\"\n    Get path to test data identifier.\n\n    Parameters\n    ----------\n    name: str\n        Name of test data.\n    Returns\n    -------\n    str\n    \"\"\"\n    config = get_config()\n    tdpath: str | None = config.get(\"testdata_path\", None)\n    if tdpath is None:\n        raise ValueError(\"Test data directory not specified in configuration\")\n    if not os.path.isdir(tdpath):\n        raise ValueError(\"Invalid test data path\")\n    res = {f: os.path.join(tdpath, f) for f in os.listdir(tdpath)}\n    if name not in res.keys():\n        raise ValueError(\"Specified test data not available.\")\n    return res[name]\n</code></pre>"},{"location":"api/moduleindex/#scida.convenience.load","title":"<code>load(path, units=True, unitfile='', overwrite=False, force_class=None, **kwargs)</code>","text":"<p>Load a dataset or dataset series from a given path. This function will automatically determine the best-matching class to use and return the initialized instance.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to dataset or dataset series. Usually the base folder containing all files of a given dataset/series.</p> required <code>units</code> <code>bool | str</code> <p>Whether to load units.</p> <code>True</code> <code>unitfile</code> <code>str</code> <p>Can explicitly pass path to a unitfile to use.</p> <code>''</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite an existing cache.</p> <code>False</code> <code>force_class</code> <code>type | None</code> <p>Force a specific class to be used.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Dataset, DatasetSeries]:</code> <p>Initialized dataset or dataset series.</p> Source code in <code>src/scida/convenience.py</code> <pre><code>def load(\n    path: str,\n    units: bool | str = True,\n    unitfile: str = \"\",\n    overwrite: bool = False,\n    force_class: type | None = None,\n    **kwargs: Any,\n) -&gt; BaseDataset | DatasetSeries:\n    \"\"\"\n    Load a dataset or dataset series from a given path.\n    This function will automatically determine the best-matching\n    class to use and return the initialized instance.\n\n    Parameters\n    ----------\n    path: str\n        Path to dataset or dataset series. Usually the base folder containing all files of a given dataset/series.\n    units: bool\n        Whether to load units.\n    unitfile: str\n        Can explicitly pass path to a unitfile to use.\n    overwrite: bool\n        Whether to overwrite an existing cache.\n    force_class: object\n        Force a specific class to be used.\n    kwargs: dict\n        Additional keyword arguments to pass to the class.\n\n    Returns\n    -------\n    Union[Dataset, DatasetSeries]:\n        Initialized dataset or dataset series.\n    \"\"\"\n\n    path = find_path(path, overwrite=overwrite)\n\n    if \"catalog\" in kwargs:\n        c = kwargs[\"catalog\"]\n        query_path = True\n        query_path &amp;= c is not None\n        query_path &amp;= not isinstance(c, bool)\n        query_path &amp;= not isinstance(c, list)\n        query_path &amp;= c != \"none\"\n        if query_path:\n            kwargs[\"catalog\"] = find_path(c, overwrite=overwrite)\n\n    # determine dataset class\n    reg: dict[str, type] = dict()\n    reg.update(**dataset_type_registry)\n    reg.update(**dataseries_type_registry)\n\n    path = os.path.realpath(path)\n    cls = _determine_type(path, **kwargs)[1][0]\n\n    msg = \"Dataset is identified as '%s' via _determine_type.\" % cls\n    log.debug(msg)\n\n    # any identifying metadata?\n    classtype = \"dataset\"\n    if issubclass(cls, DatasetSeries):\n        classtype = \"series\"\n    cls_simconf = _determine_type_from_simconfig(path, classtype=classtype, reg=reg)\n\n    if cls_simconf and not issubclass(cls, cls_simconf):\n        oldcls = cls\n        cls = cls_simconf\n        if oldcls != cls:\n            msg = \"Dataset is identified as '%s' via the simulation config replacing prior candidate '%s'.\"\n            log.debug(msg % (cls, oldcls))\n        else:\n            msg = \"Dataset is identified as '%s' via the simulation config, identical to prior candidate.\"\n            log.debug(msg % cls)\n\n    if force_class is not None:\n        cls = force_class\n\n    # determine additional mixins not set by class\n    mixins = []\n    if hasattr(cls, \"_unitfile\"):\n        unitfile = cls._unitfile\n    if unitfile:\n        if not units:\n            units = True\n        kwargs[\"unitfile\"] = unitfile\n\n    if units:\n        mixins.append(UnitMixin)\n        kwargs[\"units\"] = units\n\n    msg = \"Inconsistent overwrite_cache, please only use 'overwrite' in load().\"\n    assert kwargs.get(\"overwrite_cache\", overwrite) == overwrite, msg\n    kwargs[\"overwrite_cache\"] = overwrite\n\n    # we append since unit mixin is added outside of this func right now\n    metadata_raw = dict()\n    if classtype == \"dataset\":\n        metadata_raw = load_metadata(path, fileprefix=None)\n    other_mixins = _determine_mixins(path=path, metadata_raw=metadata_raw)\n    mixins += other_mixins\n\n    log.debug(\"Adding mixins '%s' to dataset.\" % mixins)\n    if hasattr(cls, \"_mixins\"):\n        cls_mixins = cls._mixins\n        for m in cls_mixins:\n            # remove duplicates\n            if m in mixins:\n                mixins.remove(m)\n\n    instance = cls(path, mixins=mixins, **kwargs)\n    return instance\n</code></pre>"},{"location":"api/moduleindex/#scida.customs","title":"<code>customs</code>","text":""},{"location":"api/moduleindex/#scida.customs.arepo","title":"<code>arepo</code>","text":""},{"location":"api/moduleindex/#scida.customs.arepo.MTNG","title":"<code>MTNG</code>","text":""},{"location":"api/moduleindex/#scida.customs.arepo.MTNG.dataset","title":"<code>dataset</code>","text":"<p>Support for MTNG-Arepo datasets, see https://www.mtng-project.org/</p>"},{"location":"api/moduleindex/#scida.customs.arepo.MTNG.dataset.MTNGArepoCatalog","title":"<code>MTNGArepoCatalog</code>","text":"<p>               Bases: <code>ArepoCatalog</code></p> <p>A dataset representing a MTNG-Arepo catalog.</p> Source code in <code>src/scida/customs/arepo/MTNG/dataset.py</code> <pre><code>class MTNGArepoCatalog(ArepoCatalog):\n    \"\"\"\n    A dataset representing a MTNG-Arepo catalog.\n    \"\"\"\n\n    _fileprefix = \"fof_subhalo_tab\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize an MTNGArepoCatalog object.\n\n        Parameters\n        ----------\n        args: list\n        kwargs: dict\n        \"\"\"\n        kwargs[\"iscatalog\"] = True\n        if \"fileprefix\" not in kwargs:\n            kwargs[\"fileprefix\"] = \"fof_subhalo_tab\"\n        kwargs[\"choose_prefix\"] = True\n        super().__init__(*args, **kwargs)\n\n    @classmethod\n    def validate_path(cls, path: str | os.PathLike, *args, **kwargs) -&gt; CandidateStatus:\n        \"\"\"\n        Validate a path as a candidate for the MTNG-Arepo catalog class.\n\n        Parameters\n        ----------\n        path: str\n            Path to validate.\n        args: list\n        kwargs: dict\n\n        Returns\n        -------\n        CandidateStatus\n            Whether the path is a candidate for this dataset class.\n        \"\"\"\n        tkwargs = dict(fileprefix=cls._fileprefix)\n        tkwargs.update(**kwargs)\n        valid = super().validate_path(path, *args, **tkwargs)\n        if valid == CandidateStatus.NO:\n            return valid\n        metadata_raw = load_metadata(path, **tkwargs)\n        if \"/Config\" not in metadata_raw:\n            return CandidateStatus.NO\n        if \"MTNG\" not in metadata_raw[\"/Config\"]:\n            return CandidateStatus.NO\n        return CandidateStatus.YES\n</code></pre> <code>__init__(*args, **kwargs)</code> <p>Initialize an MTNGArepoCatalog object.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> Source code in <code>src/scida/customs/arepo/MTNG/dataset.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Initialize an MTNGArepoCatalog object.\n\n    Parameters\n    ----------\n    args: list\n    kwargs: dict\n    \"\"\"\n    kwargs[\"iscatalog\"] = True\n    if \"fileprefix\" not in kwargs:\n        kwargs[\"fileprefix\"] = \"fof_subhalo_tab\"\n    kwargs[\"choose_prefix\"] = True\n    super().__init__(*args, **kwargs)\n</code></pre> <code>validate_path(path, *args, **kwargs)</code> <code>classmethod</code> <p>Validate a path as a candidate for the MTNG-Arepo catalog class.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | PathLike</code> <p>Path to validate.</p> required <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>CandidateStatus</code> <p>Whether the path is a candidate for this dataset class.</p> Source code in <code>src/scida/customs/arepo/MTNG/dataset.py</code> <pre><code>@classmethod\ndef validate_path(cls, path: str | os.PathLike, *args, **kwargs) -&gt; CandidateStatus:\n    \"\"\"\n    Validate a path as a candidate for the MTNG-Arepo catalog class.\n\n    Parameters\n    ----------\n    path: str\n        Path to validate.\n    args: list\n    kwargs: dict\n\n    Returns\n    -------\n    CandidateStatus\n        Whether the path is a candidate for this dataset class.\n    \"\"\"\n    tkwargs = dict(fileprefix=cls._fileprefix)\n    tkwargs.update(**kwargs)\n    valid = super().validate_path(path, *args, **tkwargs)\n    if valid == CandidateStatus.NO:\n        return valid\n    metadata_raw = load_metadata(path, **tkwargs)\n    if \"/Config\" not in metadata_raw:\n        return CandidateStatus.NO\n    if \"MTNG\" not in metadata_raw[\"/Config\"]:\n        return CandidateStatus.NO\n    return CandidateStatus.YES\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.MTNG.dataset.MTNGArepoSnapshot","title":"<code>MTNGArepoSnapshot</code>","text":"<p>               Bases: <code>ArepoSnapshot</code></p> <p>MTNGArepoSnapshot is a snapshot class for the MTNG project.</p> Source code in <code>src/scida/customs/arepo/MTNG/dataset.py</code> <pre><code>class MTNGArepoSnapshot(ArepoSnapshot):\n    \"\"\"\n    MTNGArepoSnapshot is a snapshot class for the MTNG project.\n    \"\"\"\n\n    _fileprefix_catalog = \"fof_subhalo_tab\"\n    _fileprefix = \"snapshot_\"  # underscore is important!\n\n    def __init__(self, path, chunksize=\"auto\", catalog=None, **kwargs) -&gt; None:\n        \"\"\"\n        Initialize an MTNGArepoSnapshot object.\n\n        Parameters\n        ----------\n        path: str\n            Path to the snapshot folder, should contain \"output\" folder.\n        chunksize: int\n            Chunksize for the data.\n        catalog: str\n            Explicitly state catalog path to use.\n        kwargs:\n            Additional keyword arguments.\n        \"\"\"\n        tkwargs = dict(\n            fileprefix=self._fileprefix,\n            fileprefix_catalog=self._fileprefix_catalog,\n            catalog_cls=MTNGArepoCatalog,\n        )\n        tkwargs.update(**kwargs)\n\n        # in MTNG, we have two kinds of snapshots:\n        # 1. regular (prefix: \"snapshot_\"), contains all particle types\n        # 2. mostbound (prefix: \"snapshot-prevmostboundonly_\"), only contains DM particles\n        # Most snapshots are of type 2, but some selected snapshots have type 1 and type 2.\n\n        # attempt to load regular snapshot\n        super().__init__(path, chunksize=chunksize, catalog=catalog, **tkwargs)\n        # need to add mtng unit peculiarities\n        # later unit file takes precedence\n        self._defaultunitfiles += [\"units/mtng.yaml\"]\n\n        if tkwargs[\"fileprefix\"] == \"snapshot-prevmostboundonly_\":\n            # this is a mostbound snapshot, so we are done\n            return\n\n        # next, attempt to load mostbound snapshot. This is done by loading into sub-object.\n        self.mostbound = None\n        tkwargs.update(fileprefix=\"snapshot-prevmostboundonly_\", catalog=\"none\")\n        self.mostbound = MTNGArepoSnapshot(path, chunksize=chunksize, **tkwargs)\n        # hacky: remove unused containers from mostbound snapshot\n        for k in [\n            \"PartType0\",\n            \"PartType2\",\n            \"PartType3\",\n            \"PartType4\",\n            \"PartType5\",\n            \"Group\",\n            \"Subhalo\",\n        ]:\n            if k in self.mostbound.data:\n                del self.mostbound.data[k]\n        self.merge_data(self.mostbound, fieldname_suffix=\"_mostbound\")\n\n    @classmethod\n    def validate_path(cls, path: str | os.PathLike, *args, **kwargs) -&gt; CandidateStatus:\n        \"\"\"\n        Validate a path as a candidate for the MTNG-Arepo snapshot class.\n\n        Parameters\n        ----------\n        path:\n            Path to validate.\n        args:  list\n        kwargs: dict\n\n        Returns\n        -------\n        CandidateStatus\n            Whether the path is a candidate for this dataset class.\n\n        \"\"\"\n        tkwargs = dict(\n            fileprefix=cls._fileprefix, fileprefix_catalog=cls._fileprefix_catalog\n        )\n        tkwargs.update(**kwargs)\n        try:\n            valid = super().validate_path(path, *args, **tkwargs)\n        except ValueError:\n            valid = CandidateStatus.NO\n            # might raise ValueError in case of partial snap\n\n        if valid == CandidateStatus.NO:\n            # check for partial snap\n            tkwargs.update(fileprefix=\"snapshot-prevmostboundonly_\")\n            try:\n                valid = super().validate_path(path, *args, **tkwargs)\n            except ValueError:\n                valid = CandidateStatus.NO\n\n        if valid == CandidateStatus.NO:\n            return valid\n        metadata_raw = load_metadata(path, **tkwargs)\n        if \"/Config\" not in metadata_raw:\n            return CandidateStatus.NO\n        if \"MTNG\" not in metadata_raw[\"/Config\"]:\n            return CandidateStatus.NO\n        if \"Ngroups_Total\" in metadata_raw[\"/Header\"]:\n            return CandidateStatus.NO  # this is a catalog\n        return CandidateStatus.YES\n</code></pre> <code>__init__(path, chunksize='auto', catalog=None, **kwargs)</code> <p>Initialize an MTNGArepoSnapshot object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>Path to the snapshot folder, should contain \"output\" folder.</p> required <code>chunksize</code> <p>Chunksize for the data.</p> <code>'auto'</code> <code>catalog</code> <p>Explicitly state catalog path to use.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>src/scida/customs/arepo/MTNG/dataset.py</code> <pre><code>def __init__(self, path, chunksize=\"auto\", catalog=None, **kwargs) -&gt; None:\n    \"\"\"\n    Initialize an MTNGArepoSnapshot object.\n\n    Parameters\n    ----------\n    path: str\n        Path to the snapshot folder, should contain \"output\" folder.\n    chunksize: int\n        Chunksize for the data.\n    catalog: str\n        Explicitly state catalog path to use.\n    kwargs:\n        Additional keyword arguments.\n    \"\"\"\n    tkwargs = dict(\n        fileprefix=self._fileprefix,\n        fileprefix_catalog=self._fileprefix_catalog,\n        catalog_cls=MTNGArepoCatalog,\n    )\n    tkwargs.update(**kwargs)\n\n    # in MTNG, we have two kinds of snapshots:\n    # 1. regular (prefix: \"snapshot_\"), contains all particle types\n    # 2. mostbound (prefix: \"snapshot-prevmostboundonly_\"), only contains DM particles\n    # Most snapshots are of type 2, but some selected snapshots have type 1 and type 2.\n\n    # attempt to load regular snapshot\n    super().__init__(path, chunksize=chunksize, catalog=catalog, **tkwargs)\n    # need to add mtng unit peculiarities\n    # later unit file takes precedence\n    self._defaultunitfiles += [\"units/mtng.yaml\"]\n\n    if tkwargs[\"fileprefix\"] == \"snapshot-prevmostboundonly_\":\n        # this is a mostbound snapshot, so we are done\n        return\n\n    # next, attempt to load mostbound snapshot. This is done by loading into sub-object.\n    self.mostbound = None\n    tkwargs.update(fileprefix=\"snapshot-prevmostboundonly_\", catalog=\"none\")\n    self.mostbound = MTNGArepoSnapshot(path, chunksize=chunksize, **tkwargs)\n    # hacky: remove unused containers from mostbound snapshot\n    for k in [\n        \"PartType0\",\n        \"PartType2\",\n        \"PartType3\",\n        \"PartType4\",\n        \"PartType5\",\n        \"Group\",\n        \"Subhalo\",\n    ]:\n        if k in self.mostbound.data:\n            del self.mostbound.data[k]\n    self.merge_data(self.mostbound, fieldname_suffix=\"_mostbound\")\n</code></pre> <code>validate_path(path, *args, **kwargs)</code> <code>classmethod</code> <p>Validate a path as a candidate for the MTNG-Arepo snapshot class.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | PathLike</code> <p>Path to validate.</p> required <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>CandidateStatus</code> <p>Whether the path is a candidate for this dataset class.</p> Source code in <code>src/scida/customs/arepo/MTNG/dataset.py</code> <pre><code>@classmethod\ndef validate_path(cls, path: str | os.PathLike, *args, **kwargs) -&gt; CandidateStatus:\n    \"\"\"\n    Validate a path as a candidate for the MTNG-Arepo snapshot class.\n\n    Parameters\n    ----------\n    path:\n        Path to validate.\n    args:  list\n    kwargs: dict\n\n    Returns\n    -------\n    CandidateStatus\n        Whether the path is a candidate for this dataset class.\n\n    \"\"\"\n    tkwargs = dict(\n        fileprefix=cls._fileprefix, fileprefix_catalog=cls._fileprefix_catalog\n    )\n    tkwargs.update(**kwargs)\n    try:\n        valid = super().validate_path(path, *args, **tkwargs)\n    except ValueError:\n        valid = CandidateStatus.NO\n        # might raise ValueError in case of partial snap\n\n    if valid == CandidateStatus.NO:\n        # check for partial snap\n        tkwargs.update(fileprefix=\"snapshot-prevmostboundonly_\")\n        try:\n            valid = super().validate_path(path, *args, **tkwargs)\n        except ValueError:\n            valid = CandidateStatus.NO\n\n    if valid == CandidateStatus.NO:\n        return valid\n    metadata_raw = load_metadata(path, **tkwargs)\n    if \"/Config\" not in metadata_raw:\n        return CandidateStatus.NO\n    if \"MTNG\" not in metadata_raw[\"/Config\"]:\n        return CandidateStatus.NO\n    if \"Ngroups_Total\" in metadata_raw[\"/Header\"]:\n        return CandidateStatus.NO  # this is a catalog\n    return CandidateStatus.YES\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.TNGcluster","title":"<code>TNGcluster</code>","text":""},{"location":"api/moduleindex/#scida.customs.arepo.TNGcluster.dataset","title":"<code>dataset</code>","text":""},{"location":"api/moduleindex/#scida.customs.arepo.TNGcluster.dataset.TNGClusterSelector","title":"<code>TNGClusterSelector</code>","text":"<p>               Bases: <code>Selector</code></p> <p>Selector for TNGClusterSnapshot.  Can select for zoomID, which selects a given zoom target. Can specify withfuzz=True to include the \"fuzz\" particles for a given zoom target. Can specify onlyfuzz=True to only return the \"fuzz\" particles for a given zoom target.</p> Source code in <code>src/scida/customs/arepo/TNGcluster/dataset.py</code> <pre><code>class TNGClusterSelector(Selector):\n    \"\"\"\n    Selector for TNGClusterSnapshot.  Can select for zoomID, which selects a given zoom target.\n    Can specify withfuzz=True to include the \"fuzz\" particles for a given zoom target.\n    Can specify onlyfuzz=True to only return the \"fuzz\" particles for a given zoom target.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"\n        Initialize the selector.\n        \"\"\"\n        super().__init__()\n        self.keys = [\"zoomID\", \"withfuzz\", \"onlyfuzz\"]\n\n    def prepare(self, *args, **kwargs) -&gt; None:\n        \"\"\"\n        Prepare the selector.\n\n        Parameters\n        ----------\n        args: list\n        kwargs: dict\n\n        Returns\n        -------\n        None\n        \"\"\"\n        snap: TNGClusterSnapshot = args[0]\n        zoom_id = kwargs.get(\"zoomID\", None)\n        fuzz = kwargs.get(\"withfuzz\", None)\n        onlyfuzz = kwargs.get(\"onlylfuzz\", None)\n        if zoom_id is None:\n            return\n        if zoom_id &lt; 0 or zoom_id &gt; (snap.ntargets - 1):\n            raise ValueError(\"zoomID must be in range 0-%i\" % (snap.ntargets - 1))\n\n        for p in self.data_backup:\n            if p.startswith(\"PartType\"):\n                key = \"particles\"\n            elif p == \"Group\":\n                key = \"groups\"\n            elif p == \"Subhalo\":\n                key = \"subgroups\"\n            else:\n                continue\n            lengths = snap.lengths_zoom[key][zoom_id]\n            offsets = snap.offsets_zoom[key][zoom_id]\n            length_fuzz = None\n            offset_fuzz = None\n\n            if fuzz and key == \"particles\":  # fuzz files only for particles\n                lengths_fuzz = snap.lengths_zoom[key][zoom_id + snap.ntargets]\n                offsets_fuzz = snap.offsets_zoom[key][zoom_id + snap.ntargets]\n\n                splt = p.split(\"PartType\")\n                pnum = int(splt[1])\n                offset_fuzz = offsets_fuzz[pnum]\n                length_fuzz = lengths_fuzz[pnum]\n\n            if key == \"particles\":\n                splt = p.split(\"PartType\")\n                pnum = int(splt[1])\n                offset = offsets[pnum]\n                length = lengths[pnum]\n            else:\n                offset = offsets\n                length = lengths\n\n            def get_slicedarr(\n                v, offset, length, offset_fuzz, length_fuzz, key, fuzz=False\n            ):\n                \"\"\"\n                Get a sliced dask array for a given (length, offset) and (length_fuzz, offset_fuzz).\n\n                Parameters\n                ----------\n                v: da.Array\n                    The array to slice.\n                offset: int\n                length: int\n                offset_fuzz: int\n                length_fuzz: int\n                key: str\n                    ?\n                fuzz: bool\n\n                Returns\n                -------\n                da.Array\n                    The sliced array.\n                \"\"\"\n                arr = v[offset : offset + length]\n                if offset_fuzz is not None:\n                    arr_fuzz = v[offset_fuzz : offset_fuzz + length_fuzz]\n                    if onlyfuzz:\n                        arr = arr_fuzz\n                    else:\n                        arr = np.concatenate([arr, arr_fuzz])\n                return arr\n\n            def get_slicedfunc(\n                func, offset, length, offset_fuzz, length_fuzz, key, fuzz=False\n            ):\n                \"\"\"\n                Slice a functions output for a given (length, offset) and (length_fuzz, offset_fuzz).\n\n                Parameters\n                ----------\n                func: callable\n                offset: int\n                length: int\n                offset_fuzz: int\n                length_fuzz: int\n                key: str\n                fuzz: bool\n\n                Returns\n                -------\n                callable\n                    The sliced function.\n                \"\"\"\n\n                def newfunc(\n                    arrs, o=offset, ln=length, of=offset_fuzz, lnf=length_fuzz, **kwargs\n                ):\n                    arr_all = func(arrs, **kwargs)\n                    arr = arr_all[o : o + ln]\n                    if of is None:\n                        return arr\n                    arr_fuzz = arr_all[of : of + lnf]\n                    if onlyfuzz:\n                        return arr_fuzz\n                    else:\n                        return np.concatenate([arr, arr_fuzz])\n\n                return newfunc\n\n            # need to evaluate without recipes first\n            for k, v in self.data_backup[p].items(withrecipes=False):\n                self.data[p][k] = get_slicedarr(\n                    v, offset, length, offset_fuzz, length_fuzz, key, fuzz\n                )\n\n            for k, v in self.data_backup[p].items(\n                withfields=False, withrecipes=True, evaluate=False\n            ):\n                if not isinstance(v, FieldRecipe):\n                    continue  # already evaluated, no need to port recipe (?)\n                rcp: FieldRecipe = v\n                func = get_slicedfunc(\n                    v.func, offset, length, offset_fuzz, length_fuzz, key, fuzz\n                )\n                newrcp = DerivedFieldRecipe(rcp.name, func)\n                newrcp.type = rcp.type\n                newrcp.description = rcp.description\n                newrcp.units = rcp.units\n                self.data[p][k] = newrcp\n        snap.data = self.data\n</code></pre> <code>__init__()</code> <p>Initialize the selector.</p> Source code in <code>src/scida/customs/arepo/TNGcluster/dataset.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Initialize the selector.\n    \"\"\"\n    super().__init__()\n    self.keys = [\"zoomID\", \"withfuzz\", \"onlyfuzz\"]\n</code></pre> <code>prepare(*args, **kwargs)</code> <p>Prepare the selector.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/customs/arepo/TNGcluster/dataset.py</code> <pre><code>def prepare(self, *args, **kwargs) -&gt; None:\n    \"\"\"\n    Prepare the selector.\n\n    Parameters\n    ----------\n    args: list\n    kwargs: dict\n\n    Returns\n    -------\n    None\n    \"\"\"\n    snap: TNGClusterSnapshot = args[0]\n    zoom_id = kwargs.get(\"zoomID\", None)\n    fuzz = kwargs.get(\"withfuzz\", None)\n    onlyfuzz = kwargs.get(\"onlylfuzz\", None)\n    if zoom_id is None:\n        return\n    if zoom_id &lt; 0 or zoom_id &gt; (snap.ntargets - 1):\n        raise ValueError(\"zoomID must be in range 0-%i\" % (snap.ntargets - 1))\n\n    for p in self.data_backup:\n        if p.startswith(\"PartType\"):\n            key = \"particles\"\n        elif p == \"Group\":\n            key = \"groups\"\n        elif p == \"Subhalo\":\n            key = \"subgroups\"\n        else:\n            continue\n        lengths = snap.lengths_zoom[key][zoom_id]\n        offsets = snap.offsets_zoom[key][zoom_id]\n        length_fuzz = None\n        offset_fuzz = None\n\n        if fuzz and key == \"particles\":  # fuzz files only for particles\n            lengths_fuzz = snap.lengths_zoom[key][zoom_id + snap.ntargets]\n            offsets_fuzz = snap.offsets_zoom[key][zoom_id + snap.ntargets]\n\n            splt = p.split(\"PartType\")\n            pnum = int(splt[1])\n            offset_fuzz = offsets_fuzz[pnum]\n            length_fuzz = lengths_fuzz[pnum]\n\n        if key == \"particles\":\n            splt = p.split(\"PartType\")\n            pnum = int(splt[1])\n            offset = offsets[pnum]\n            length = lengths[pnum]\n        else:\n            offset = offsets\n            length = lengths\n\n        def get_slicedarr(\n            v, offset, length, offset_fuzz, length_fuzz, key, fuzz=False\n        ):\n            \"\"\"\n            Get a sliced dask array for a given (length, offset) and (length_fuzz, offset_fuzz).\n\n            Parameters\n            ----------\n            v: da.Array\n                The array to slice.\n            offset: int\n            length: int\n            offset_fuzz: int\n            length_fuzz: int\n            key: str\n                ?\n            fuzz: bool\n\n            Returns\n            -------\n            da.Array\n                The sliced array.\n            \"\"\"\n            arr = v[offset : offset + length]\n            if offset_fuzz is not None:\n                arr_fuzz = v[offset_fuzz : offset_fuzz + length_fuzz]\n                if onlyfuzz:\n                    arr = arr_fuzz\n                else:\n                    arr = np.concatenate([arr, arr_fuzz])\n            return arr\n\n        def get_slicedfunc(\n            func, offset, length, offset_fuzz, length_fuzz, key, fuzz=False\n        ):\n            \"\"\"\n            Slice a functions output for a given (length, offset) and (length_fuzz, offset_fuzz).\n\n            Parameters\n            ----------\n            func: callable\n            offset: int\n            length: int\n            offset_fuzz: int\n            length_fuzz: int\n            key: str\n            fuzz: bool\n\n            Returns\n            -------\n            callable\n                The sliced function.\n            \"\"\"\n\n            def newfunc(\n                arrs, o=offset, ln=length, of=offset_fuzz, lnf=length_fuzz, **kwargs\n            ):\n                arr_all = func(arrs, **kwargs)\n                arr = arr_all[o : o + ln]\n                if of is None:\n                    return arr\n                arr_fuzz = arr_all[of : of + lnf]\n                if onlyfuzz:\n                    return arr_fuzz\n                else:\n                    return np.concatenate([arr, arr_fuzz])\n\n            return newfunc\n\n        # need to evaluate without recipes first\n        for k, v in self.data_backup[p].items(withrecipes=False):\n            self.data[p][k] = get_slicedarr(\n                v, offset, length, offset_fuzz, length_fuzz, key, fuzz\n            )\n\n        for k, v in self.data_backup[p].items(\n            withfields=False, withrecipes=True, evaluate=False\n        ):\n            if not isinstance(v, FieldRecipe):\n                continue  # already evaluated, no need to port recipe (?)\n            rcp: FieldRecipe = v\n            func = get_slicedfunc(\n                v.func, offset, length, offset_fuzz, length_fuzz, key, fuzz\n            )\n            newrcp = DerivedFieldRecipe(rcp.name, func)\n            newrcp.type = rcp.type\n            newrcp.description = rcp.description\n            newrcp.units = rcp.units\n            self.data[p][k] = newrcp\n    snap.data = self.data\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.TNGcluster.dataset.TNGClusterSnapshot","title":"<code>TNGClusterSnapshot</code>","text":"<p>               Bases: <code>ArepoSnapshot</code></p> <p>Dataset class for the TNG-Cluster simulation.</p> Source code in <code>src/scida/customs/arepo/TNGcluster/dataset.py</code> <pre><code>class TNGClusterSnapshot(ArepoSnapshot):\n    \"\"\"\n    Dataset class for the TNG-Cluster simulation.\n    \"\"\"\n\n    _fileprefix_catalog = \"fof_subhalo_tab_\"\n    _fileprefix = \"snap_\"\n    ntargets = 352\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize a TNGClusterSnapshot object.\n\n        Parameters\n        ----------\n        args\n        kwargs\n        \"\"\"\n        super().__init__(*args, **kwargs)\n\n        # we can get the offsets from the header as our load routine concats the various values from different files\n        # these offsets can be used to select a given halo.\n        # see: https://tng-project.org/w/index.php/TNG-Cluster\n        # each zoom-target has two entries i and i+N, where N=352 is the number of zoom targets.\n        # the first file contains the particles that were contained in the original low-res run\n        # the second file contains all other remaining particles in a given zoom target\n        def len_to_offsets(lengths):\n            \"\"\"\n            From the particle count (field length), get the offset of all zoom targets.\n\n            Parameters\n            ----------\n            lengths\n\n            Returns\n            -------\n            np.ndarray\n            \"\"\"\n            lengths = np.array(lengths)\n            shp = len(lengths.shape)\n            n = lengths.shape[-1]\n            if shp == 1:\n                res = np.concatenate(\n                    [np.zeros(1, dtype=np.int64), np.cumsum(lengths.astype(np.int64))]\n                )[:-1]\n            else:\n                res = np.cumsum(\n                    np.vstack([np.zeros(n, dtype=np.int64), lengths.astype(np.int64)]),\n                    axis=0,\n                )[:-1]\n            return res\n\n        self.lengths_zoom = dict(particles=self.header[\"NumPart_ThisFile\"])\n        self.offsets_zoom = dict(\n            particles=len_to_offsets(self.lengths_zoom[\"particles\"])\n        )\n\n        if hasattr(self, \"catalog\") and self.catalog is not None:\n            self.lengths_zoom[\"groups\"] = self.catalog.header[\"Ngroups_ThisFile\"]\n            self.offsets_zoom[\"groups\"] = len_to_offsets(self.lengths_zoom[\"groups\"])\n            self.lengths_zoom[\"subgroups\"] = self.catalog.header[\"Nsubgroups_ThisFile\"]\n            self.offsets_zoom[\"subgroups\"] = len_to_offsets(\n                self.lengths_zoom[\"subgroups\"]\n            )\n\n    @TNGClusterSelector()\n    def return_data(self) -&gt; FieldContainer:\n        \"\"\"\n        Return the data object.\n\n        Returns\n        -------\n        FieldContainer\n            The data object.\n        \"\"\"\n        return super().return_data()\n\n    @classmethod\n    def validate_path(cls, path: str | os.PathLike, *args, **kwargs) -&gt; CandidateStatus:\n        \"\"\"\n        Validate a path as a candidate for TNG-Cluster snapshot class.\n\n        Parameters\n        ----------\n        path: str\n            Path to validate.\n        args: list\n        kwargs: dict\n\n        Returns\n        -------\n        CandidateStatus\n            Whether the path is a candidate for this simulation class.\n        \"\"\"\n\n        tkwargs = dict(\n            fileprefix=cls._fileprefix, fileprefix_catalog=cls._fileprefix_catalog\n        )\n        tkwargs.update(**kwargs)\n        valid = super().validate_path(path, *args, **tkwargs)\n        if valid == CandidateStatus.NO:\n            return valid\n        metadata_raw = load_metadata(path, **tkwargs)\n\n        matchingattrs = True\n\n        parameters = metadata_raw[\"/Parameters\"]\n        if \"InitCondFile\" in parameters:\n            matchingattrs &amp;= parameters[\"InitCondFile\"] == \"various\"\n        else:\n            return CandidateStatus.NO\n        header = metadata_raw[\"/Header\"]\n        matchingattrs &amp;= header[\"BoxSize\"] == 680000.0\n        matchingattrs &amp;= header[\"NumPart_Total\"][1] == 1944529344\n        matchingattrs &amp;= header[\"NumPart_Total\"][2] == 586952200\n\n        if matchingattrs:\n            valid = CandidateStatus.YES\n        else:\n            valid = CandidateStatus.NO\n        return valid\n</code></pre> <code>__init__(*args, **kwargs)</code> <p>Initialize a TNGClusterSnapshot object.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> Source code in <code>src/scida/customs/arepo/TNGcluster/dataset.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Initialize a TNGClusterSnapshot object.\n\n    Parameters\n    ----------\n    args\n    kwargs\n    \"\"\"\n    super().__init__(*args, **kwargs)\n\n    # we can get the offsets from the header as our load routine concats the various values from different files\n    # these offsets can be used to select a given halo.\n    # see: https://tng-project.org/w/index.php/TNG-Cluster\n    # each zoom-target has two entries i and i+N, where N=352 is the number of zoom targets.\n    # the first file contains the particles that were contained in the original low-res run\n    # the second file contains all other remaining particles in a given zoom target\n    def len_to_offsets(lengths):\n        \"\"\"\n        From the particle count (field length), get the offset of all zoom targets.\n\n        Parameters\n        ----------\n        lengths\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        lengths = np.array(lengths)\n        shp = len(lengths.shape)\n        n = lengths.shape[-1]\n        if shp == 1:\n            res = np.concatenate(\n                [np.zeros(1, dtype=np.int64), np.cumsum(lengths.astype(np.int64))]\n            )[:-1]\n        else:\n            res = np.cumsum(\n                np.vstack([np.zeros(n, dtype=np.int64), lengths.astype(np.int64)]),\n                axis=0,\n            )[:-1]\n        return res\n\n    self.lengths_zoom = dict(particles=self.header[\"NumPart_ThisFile\"])\n    self.offsets_zoom = dict(\n        particles=len_to_offsets(self.lengths_zoom[\"particles\"])\n    )\n\n    if hasattr(self, \"catalog\") and self.catalog is not None:\n        self.lengths_zoom[\"groups\"] = self.catalog.header[\"Ngroups_ThisFile\"]\n        self.offsets_zoom[\"groups\"] = len_to_offsets(self.lengths_zoom[\"groups\"])\n        self.lengths_zoom[\"subgroups\"] = self.catalog.header[\"Nsubgroups_ThisFile\"]\n        self.offsets_zoom[\"subgroups\"] = len_to_offsets(\n            self.lengths_zoom[\"subgroups\"]\n        )\n</code></pre> <code>return_data()</code> <p>Return the data object.</p> <p>Returns:</p> Type Description <code>FieldContainer</code> <p>The data object.</p> Source code in <code>src/scida/customs/arepo/TNGcluster/dataset.py</code> <pre><code>@TNGClusterSelector()\ndef return_data(self) -&gt; FieldContainer:\n    \"\"\"\n    Return the data object.\n\n    Returns\n    -------\n    FieldContainer\n        The data object.\n    \"\"\"\n    return super().return_data()\n</code></pre> <code>validate_path(path, *args, **kwargs)</code> <code>classmethod</code> <p>Validate a path as a candidate for TNG-Cluster snapshot class.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | PathLike</code> <p>Path to validate.</p> required <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>CandidateStatus</code> <p>Whether the path is a candidate for this simulation class.</p> Source code in <code>src/scida/customs/arepo/TNGcluster/dataset.py</code> <pre><code>@classmethod\ndef validate_path(cls, path: str | os.PathLike, *args, **kwargs) -&gt; CandidateStatus:\n    \"\"\"\n    Validate a path as a candidate for TNG-Cluster snapshot class.\n\n    Parameters\n    ----------\n    path: str\n        Path to validate.\n    args: list\n    kwargs: dict\n\n    Returns\n    -------\n    CandidateStatus\n        Whether the path is a candidate for this simulation class.\n    \"\"\"\n\n    tkwargs = dict(\n        fileprefix=cls._fileprefix, fileprefix_catalog=cls._fileprefix_catalog\n    )\n    tkwargs.update(**kwargs)\n    valid = super().validate_path(path, *args, **tkwargs)\n    if valid == CandidateStatus.NO:\n        return valid\n    metadata_raw = load_metadata(path, **tkwargs)\n\n    matchingattrs = True\n\n    parameters = metadata_raw[\"/Parameters\"]\n    if \"InitCondFile\" in parameters:\n        matchingattrs &amp;= parameters[\"InitCondFile\"] == \"various\"\n    else:\n        return CandidateStatus.NO\n    header = metadata_raw[\"/Header\"]\n    matchingattrs &amp;= header[\"BoxSize\"] == 680000.0\n    matchingattrs &amp;= header[\"NumPart_Total\"][1] == 1944529344\n    matchingattrs &amp;= header[\"NumPart_Total\"][2] == 586952200\n\n    if matchingattrs:\n        valid = CandidateStatus.YES\n    else:\n        valid = CandidateStatus.NO\n    return valid\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset","title":"<code>dataset</code>","text":""},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ArepoCatalog","title":"<code>ArepoCatalog</code>","text":"<p>               Bases: <code>ArepoSnapshot</code></p> <p>Dataset class for Arepo group catalogs.</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>class ArepoCatalog(ArepoSnapshot):\n    \"\"\"\n    Dataset class for Arepo group catalogs.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize an ArepoCatalog object.\n\n        Parameters\n        ----------\n        args\n        kwargs\n        \"\"\"\n        kwargs[\"iscatalog\"] = True\n        if \"fileprefix\" not in kwargs:\n            kwargs[\"fileprefix\"] = \"groups\"\n        super().__init__(*args, **kwargs)\n\n    @classmethod\n    def validate_path(cls, path: str | os.PathLike, *args, **kwargs) -&gt; CandidateStatus:\n        \"\"\"\n        Validate a path to use for instantiation of this class.\n\n        Parameters\n        ----------\n        path: str or pathlib.Path\n        args:\n        kwargs:\n\n        Returns\n        -------\n        CandidateStatus\n        \"\"\"\n        kwargs[\"fileprefix\"] = cls._get_fileprefix(path)\n        valid = super().validate_path(path, *args, expect_grp=True, **kwargs)\n        return valid\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ArepoCatalog.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize an ArepoCatalog object.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Initialize an ArepoCatalog object.\n\n    Parameters\n    ----------\n    args\n    kwargs\n    \"\"\"\n    kwargs[\"iscatalog\"] = True\n    if \"fileprefix\" not in kwargs:\n        kwargs[\"fileprefix\"] = \"groups\"\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ArepoCatalog.validate_path","title":"<code>validate_path(path, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Validate a path to use for instantiation of this class.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | PathLike</code> required <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>CandidateStatus</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>@classmethod\ndef validate_path(cls, path: str | os.PathLike, *args, **kwargs) -&gt; CandidateStatus:\n    \"\"\"\n    Validate a path to use for instantiation of this class.\n\n    Parameters\n    ----------\n    path: str or pathlib.Path\n    args:\n    kwargs:\n\n    Returns\n    -------\n    CandidateStatus\n    \"\"\"\n    kwargs[\"fileprefix\"] = cls._get_fileprefix(path)\n    valid = super().validate_path(path, *args, expect_grp=True, **kwargs)\n    return valid\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ArepoSnapshot","title":"<code>ArepoSnapshot</code>","text":"<p>               Bases: <code>SpatialCartesian3DMixin</code>, <code>GadgetStyleSnapshot</code></p> <p>Dataset class for Arepo snapshots.</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>class ArepoSnapshot(SpatialCartesian3DMixin, GadgetStyleSnapshot):\n    \"\"\"\n    Dataset class for Arepo snapshots.\n    \"\"\"\n\n    _fileprefix_catalog = \"groups\"\n\n    def __init__(self, path, chunksize=\"auto\", catalog=None, **kwargs) -&gt; None:\n        \"\"\"\n        Initialize an ArepoSnapshot object.\n\n        Parameters\n        ----------\n        path: str or pathlib.Path\n            Path to snapshot, typically a directory containing multiple hdf5 files.\n        chunksize:\n            Chunksize to use for dask arrays. Can be \"auto\" to automatically determine chunksize.\n        catalog:\n            Path to group catalog. If None, the catalog is searched for in the parent directories.\n        kwargs:\n            Additional keyword arguments.\n        \"\"\"\n        self.iscatalog = kwargs.pop(\"iscatalog\", False)\n        self.header: dict[str, Any] = {}\n        self.config: dict[str, Any] = {}\n        # check whether we have a cosmology mixin\n        self._defaultunitfiles: list[str] = [\"units/gadget_base.yaml\"]\n        if hasattr(self, \"_mixins\") and \"cosmology\" in self._mixins:\n            self._defaultunitfiles += [\"units/gadget_cosmological.yaml\"]\n        self.parameters: dict[str, Any] = {}\n        self._grouplengths: dict[str, Any] = {}\n        self._subhalolengths: dict[str, Any] = {}\n        # not needed for group catalogs as entries are back-to-back there, we will provide a property for this\n        self._subhalooffsets: dict[str, Any] = {}\n        self.misc: dict[str, Any] = {}  # for storing misc info\n        prfx = kwargs.pop(\"fileprefix\", None)\n        if prfx is None:\n            prfx = self._get_fileprefix(path)\n        super().__init__(path, chunksize=chunksize, fileprefix=prfx, **kwargs)\n\n        self.catalog = catalog\n        if not self.iscatalog:\n            if self.catalog is None:\n                self.discover_catalog()\n                # try to discover group catalog in parent directories.\n            if self.catalog == \"none\":\n                pass  # this string can be set to explicitly disable catalog\n            elif self.catalog:\n                catalog_cls = kwargs.get(\"catalog_cls\", None)\n                cosmological = False\n                if hasattr(self, \"_mixins\") and \"cosmology\" in self._mixins:\n                    cosmological = True\n                self.load_catalog(\n                    overwrite_cache=kwargs.get(\"overwrite_cache\", False),\n                    catalog_cls=catalog_cls,\n                    units=self.withunits,\n                    cosmological=cosmological,\n                )\n\n        # add aliases\n        aliases = dict(\n            PartType0=[\"gas\", \"baryons\"],\n            PartType1=[\"dm\", \"dark matter\"],\n            PartType2=[\"lowres\", \"lowres dm\"],\n            PartType3=[\"tracer\", \"tracers\"],\n            PartType4=[\"stars\"],\n            PartType5=[\"bh\", \"black holes\"],\n        )\n        for k, lst in aliases.items():\n            if k not in self.data:\n                continue\n            for v in lst:\n                self.data.add_alias(v, k)\n\n        # set metadata\n        self._set_metadata()\n\n        # add some default fields\n        overwrite_fields = False\n        for kfield in fielddefs.fielddefs:\n            parttype = fielddefs.fielddefs[kfield].parttype\n            if parttype in self.data:\n                if overwrite_fields or kfield not in self.data[parttype]:\n                    func = fielddefs.fielddefs[kfield].func\n                    # verify that we have the dependencies\n                    dependencies = fielddefs.fielddefs[kfield].dependencies\n                    deps_fulfilled = True\n                    for dep in dependencies:\n                        if dep not in self.data[parttype]:\n                            deps_fulfilled = False\n                            break\n                    if deps_fulfilled:\n                        self.data.register_field(parttype, name=kfield)(func)\n\n    def load_catalog(\n        self, overwrite_cache=False, units=False, cosmological=False, catalog_cls=None\n    ):\n        \"\"\"\n        Load the group catalog.\n\n        Parameters\n        ----------\n        kwargs: dict\n            Keyword arguments passed to the catalog class.\n        catalog_cls: type\n            Class to use for the catalog. If None, the default catalog class is used.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        virtualcache = True\n        # fileprefix = catalog_kwargs.get(\"fileprefix\", self._fileprefix_catalog)\n        prfx = self._get_fileprefix(self.catalog)\n\n        # explicitly need to create unitaware class for catalog as needed\n        # TODO: should just be determined from mixins of parent?\n        if catalog_cls is None:\n            cls = ArepoCatalog\n        else:\n            cls = catalog_cls\n        withunits = units\n        mixins = []\n        if withunits:\n            mixins += [UnitMixin]\n\n        other_mixins = _determine_mixins(path=self.path)\n        mixins += other_mixins\n        if cosmological and CosmologyMixin not in mixins:\n            mixins.append(CosmologyMixin)\n\n        cls = create_datasetclass_with_mixins(cls, mixins)\n\n        ureg = None\n        if hasattr(self, \"ureg\"):\n            ureg = self.ureg\n\n        # non-virtual catalog fields for better performance\n        nonvirtual_datasets = [\n            \"Group/GroupFirstSub\",\n            \"Group/GroupLenType\",\n            \"Group/GroupNsubs\",\n            \"Subhalo/SubhaloGrNr\",\n            \"Subhalo/SubhaloGroupNr\",\n            \"Subhalo/SubhaloLenType\",\n        ]\n\n        self.catalog = cls(\n            self.catalog,\n            overwrite_cache=overwrite_cache,\n            virtualcache=virtualcache,\n            nonvirtual_datasets=nonvirtual_datasets,\n            fileprefix=prfx,\n            units=self.withunits,\n            ureg=ureg,\n            hints=self.hints,\n            metadata_raw_parent=self._metadata_raw,\n        )\n        if \"Redshift\" in self.catalog.header and \"Redshift\" in self.header:\n            z_catalog = self.catalog.header[\"Redshift\"]\n            z_snap = self.header[\"Redshift\"]\n            if not np.isclose(z_catalog, z_snap):\n                raise ValueError(\n                    \"Redshift mismatch between snapshot and catalog: \"\n                    f\"{z_snap:.2f} vs {z_catalog:.2f}\"\n                )\n\n        # merge data\n        self.merge_data(self.catalog)\n\n        # first snapshots often do not have groups\n        if \"Group\" in self.catalog.data:\n            ngkeys = self.catalog.data[\"Group\"].keys()\n            if len(ngkeys) &gt; 0:\n                self.add_catalogIDs()\n\n        # merge hints from snap and catalog\n        self.merge_hints(self.catalog)\n\n    @classmethod\n    def validate_path(cls, path: str | os.PathLike, *args, **kwargs) -&gt; CandidateStatus:\n        \"\"\"\n        Validate a path to use for instantiation of this class.\n\n        Parameters\n        ----------\n        path: str or pathlib.Path\n        args:\n        kwargs:\n\n        Returns\n        -------\n        CandidateStatus\n        \"\"\"\n        valid = super().validate_path(path, *args, **kwargs)\n        if valid.value &gt; CandidateStatus.MAYBE.value:\n            valid = CandidateStatus.MAYBE\n        else:\n            return valid\n        # Arepo has no dedicated attribute to identify such runs.\n        # lets just query a bunch of attributes that are present for arepo runs\n        metadata_raw = load_metadata(path, **kwargs)\n        matchingattrs = True\n        matchingattrs &amp;= \"Git_commit\" in metadata_raw[\"/Header\"]\n        # not existent for any arepo run?\n        matchingattrs &amp;= \"Compactify_Version\" not in metadata_raw[\"/Header\"]\n\n        if matchingattrs:\n            valid = CandidateStatus.MAYBE\n\n        return valid\n\n    @ArepoSelector()\n    def return_data(self) -&gt; FieldContainer:\n        \"\"\"\n        Return data object of this snapshot.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        return super().return_data()\n\n    def discover_catalog(self):\n        \"\"\"\n        Discover the group catalog given the current path\n\n        Returns\n        -------\n        None\n        \"\"\"\n        p = str(self.path)\n        # order of candidates matters. For Illustris \"groups\" must precede \"fof_subhalo_tab\"\n        candidates = [\n            p.replace(\"snapshot\", \"group\"),\n            p.replace(\"snapshot\", \"groups\"),\n            p.replace(\"snap\", \"groups\"),\n            p.replace(\"snap\", \"group\"),\n            p.replace(\"snapdir\", \"groups\").replace(\"snap\", \"groups\"),\n            p.replace(\"snapdir\", \"groups\").replace(\"snap\", \"fof_subhalo_tab\"),\n        ]\n        for candidate in candidates:\n            if not os.path.exists(candidate):\n                continue\n            if candidate == p:\n                continue  # do not set catalog to snapshot itself\n            if candidate == self.path:\n                continue\n            self.catalog = candidate\n            break\n\n    def register_field(\n        self, parttype: str, name: str | None = None, construct: bool = False\n    ):\n        \"\"\"\n        Register a field.\n        Parameters\n        ----------\n        parttype: str\n            name of particle type\n        name: str\n            name of field\n        construct: bool\n            construct field immediately\n\n        Returns\n        -------\n        None\n        \"\"\"\n        num = part_type_num(parttype)\n        if construct:  # TODO: introduce (immediate) construct option later\n            raise NotImplementedError\n        if num == -1:  # TODO: all particle species\n            key = \"all\"\n            raise NotImplementedError\n        elif isinstance(num, int):\n            key = \"PartType\" + str(num)\n        else:\n            key = parttype\n        return super().register_field(key, name=name)\n\n    def add_catalogIDs(self) -&gt; None:\n        \"\"\"\n        Add field for halo and subgroup IDs for all particle types.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        # TODO: make these delayed objects and properly pass into (delayed?) numba functions:\n        # https://docs.dask.org/en/stable/delayed-best-practices.html#avoid-repeatedly-putting-large-inputs-into-delayed-calls\n\n        maxint = np.iinfo(np.int64).max\n        self.misc[\"unboundID\"] = maxint\n\n        # Group ID\n        if \"Group\" not in self.data:  # can happen for empty catalogs\n            for key in self.data:\n                if not (key.startswith(\"PartType\")):\n                    continue\n                uid = self.data[key][\"uid\"]\n                self.data[key][\"GroupID\"] = self.misc[\"unboundID\"] * da.ones_like(\n                    uid, dtype=np.int64\n                )\n                self.data[key][\"SubhaloID\"] = self.misc[\"unboundID\"] * da.ones_like(\n                    uid, dtype=np.int64\n                )\n            return\n\n        glen = self.data[\"Group\"][\"GroupLenType\"]\n        ngrp = glen.shape[0]\n        da_halocelloffsets = da.concatenate(\n            [\n                np.zeros((1, 6), dtype=np.int64),\n                da.cumsum(glen, axis=0, dtype=np.int64),\n            ]\n        )\n        # remove last entry to match shapematch shape\n        self.data[\"Group\"][\"GroupOffsetsType\"] = da_halocelloffsets[:-1].rechunk(\n            glen.chunks\n        )\n        halocelloffsets = da_halocelloffsets.rechunk(-1)\n\n        index_unbound = self.misc[\"unboundID\"]\n\n        for key in self.data:\n            if not (key.startswith(\"PartType\")):\n                continue\n            num = int(key[-1])\n            if \"uid\" not in self.data[key]:\n                continue  # can happen for empty containers\n            gidx = self.data[key][\"uid\"]\n            hidx = compute_haloindex(\n                gidx, halocelloffsets[:, num], index_unbound=index_unbound\n            )\n            self.data[key][\"GroupID\"] = hidx\n\n        # Subhalo ID\n        if \"Subhalo\" not in self.data:  # can happen for empty catalogs\n            for key in self.data:\n                if not (key.startswith(\"PartType\")):\n                    continue\n                self.data[key][\"SubhaloID\"] = -1 * da.ones_like(\n                    da[key][\"uid\"], dtype=np.int64\n                )\n            return\n\n        shnr_attr = \"SubhaloGrNr\"\n        if shnr_attr not in self.data[\"Subhalo\"]:\n            shnr_attr = \"SubhaloGroupNr\"  # what MTNG does\n        if shnr_attr not in self.data[\"Subhalo\"]:\n            raise ValueError(\n                f\"Could not find 'SubhaloGrNr' or 'SubhaloGroupNr' in {self.catalog}\"\n            )\n\n        subhalogrnr = self.data[\"Subhalo\"][shnr_attr]\n        subhalocellcounts = self.data[\"Subhalo\"][\"SubhaloLenType\"]\n\n        # remove \"units\" for numba funcs\n        if hasattr(subhalogrnr, \"magnitude\"):\n            subhalogrnr = subhalogrnr.magnitude\n        if hasattr(subhalocellcounts, \"magnitude\"):\n            subhalocellcounts = subhalocellcounts.magnitude\n\n        grp = self.data[\"Group\"]\n        if \"GroupFirstSub\" not in grp or \"GroupNsubs\" not in grp:\n            # if not provided, we calculate:\n            # \"GroupFirstSub\": First subhalo index for each halo\n            # \"GroupNsubs\": Number of subhalos for each halo\n            dlyd = delayed(get_shcounts_shcells)(subhalogrnr, ngrp)\n            grp[\"GroupFirstSub\"] = dask.compute(dlyd[1])[0]\n            grp[\"GroupNsubs\"] = dask.compute(dlyd[0])[0]\n\n        # remove \"units\" for numba funcs\n        grpfirstsub = grp[\"GroupFirstSub\"]\n        if hasattr(grpfirstsub, \"magnitude\"):\n            grpfirstsub = grpfirstsub.magnitude\n        grpnsubs = grp[\"GroupNsubs\"]\n        if hasattr(grpnsubs, \"magnitude\"):\n            grpnsubs = grpnsubs.magnitude\n\n        for key in self.data:\n            if not (key.startswith(\"PartType\")):\n                continue\n            num = int(key[-1])\n            pdata = self.data[key]\n            if \"uid\" not in self.data[key]:\n                continue  # can happen for empty containers\n            gidx = pdata[\"uid\"]\n\n            # we need to make other dask arrays delayed,\n            # map_block does not incorrectly infer output shape from these\n            halocelloffsets_dlyd = delayed(halocelloffsets[:, num])\n            grpfirstsub_dlyd = delayed(grpfirstsub)\n            grpnsubs_dlyd = delayed(grpnsubs)\n            subhalocellcounts_dlyd = delayed(subhalocellcounts[:, num])\n\n            sidx = compute_localsubhaloindex(\n                gidx,\n                halocelloffsets_dlyd,\n                grpfirstsub_dlyd,\n                grpnsubs_dlyd,\n                subhalocellcounts_dlyd,\n                index_unbound=index_unbound,\n            )\n\n            pdata[\"LocalSubhaloID\"] = sidx\n\n            # reconstruct SubhaloID from Group's GroupFirstSub and LocalSubhaloID\n            # should be easier to do it directly, but quicker to write down like this:\n\n            # calculate first subhalo of each halo that a particle belongs to\n            self.add_groupquantity_to_particles(\"GroupFirstSub\", parttype=key)\n            pdata[\"SubhaloID\"] = pdata[\"GroupFirstSub\"] + pdata[\"LocalSubhaloID\"]\n            pdata[\"SubhaloID\"] = da.where(\n                pdata[\"SubhaloID\"] == index_unbound, index_unbound, pdata[\"SubhaloID\"]\n            )\n\n        # add GroupID and SubhaloID to catalogs/groups themselves\n        self.data[\"Group\"][\"GroupID\"] = self.data[\"Group\"][\"uid\"]\n        self.data[\"Subhalo\"][\"SubhaloID\"] = self.data[\"Subhalo\"][\"uid\"]\n\n    @computedecorator\n    def map_group_operation(\n        self,\n        func,\n        cpucost_halo=1e4,\n        nchunks_min=None,\n        chunksize_bytes=None,\n        nmax=None,\n        idxlist=None,\n        objtype=\"halo\",\n    ):\n        \"\"\"\n        Apply a function to each halo in the catalog.\n\n        Parameters\n        ----------\n        objtype: str\n            Type of object to process. Can be \"halo\" or \"subhalo\". Default: \"halo\"\n        idxlist: np.ndarray | None\n            List of halo indices to process. If not provided, all halos are processed.\n        func: function\n            Function to apply to each halo. Must take a dictionary of arrays as input.\n        cpucost_halo:\n            \"CPU cost\" of processing a single halo. This is a relative value to the processing time per input particle\n            used for calculating the dask chunks. Default: 1e4\n        nchunks_min: int | None\n            Minimum number of particles in a halo to process it. Default: None\n        chunksize_bytes: int | None\n        nmax: int | None\n            Only process the first nmax halos.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        dfltkwargs = get_kwargs(func)\n        fieldnames = dfltkwargs.get(\"fieldnames\", None)\n        if fieldnames is None:\n            fieldnames = get_args(func)\n        parttype = dfltkwargs.get(\"parttype\", \"PartType0\")\n        entry_nbytes_in = np.sum([self.data[parttype][f][0].nbytes for f in fieldnames])\n        objtype = grp_type_str(objtype)\n        if objtype == \"halo\":\n            lengths = self.get_grouplengths(parttype=parttype)\n            offsets = self.get_groupoffsets(parttype=parttype)\n        elif objtype == \"subhalo\":\n            lengths = self.get_subhalolengths(parttype=parttype)\n            offsets = self.get_subhalooffsets(parttype=parttype)\n        else:\n            raise ValueError(f\"objtype must be 'halo' or 'subhalo', not {objtype}\")\n        arrdict = self.data[parttype]\n        return map_group_operation(\n            func,\n            offsets,\n            lengths,\n            arrdict,\n            cpucost_halo=cpucost_halo,\n            nchunks_min=nchunks_min,\n            chunksize_bytes=chunksize_bytes,\n            entry_nbytes_in=entry_nbytes_in,\n            nmax=nmax,\n            idxlist=idxlist,\n        )\n\n    def add_groupquantity_to_particles(self, name, parttype=\"PartType0\"):\n        \"\"\"\n        Map a quantity from the group catalog to the particles based on a particle's group index.\n\n        Parameters\n        ----------\n        name: str\n            Name of quantity to map\n        parttype: str\n            Name of particle type\n\n        Returns\n        -------\n        None\n        \"\"\"\n        pdata = self.data[parttype]\n        assert (\n            name not in pdata\n        )  # we simply map the name from Group to Particle for now. Should work (?)\n        glen = self.data[\"Group\"][\"GroupLenType\"]\n        da_halocelloffsets = da.concatenate(\n            [np.zeros((1, 6), dtype=np.int64), da.cumsum(glen, axis=0)]\n        )\n        if \"GroupOffsetsType\" not in self.data[\"Group\"]:\n            self.data[\"Group\"][\"GroupOffsetsType\"] = da_halocelloffsets[:-1].rechunk(\n                glen.chunks\n            )  # remove last entry to match shape\n        halocelloffsets = da_halocelloffsets.compute()\n\n        gidx = pdata[\"uid\"]\n        num = int(parttype[-1])\n        hquantity = compute_haloquantity(\n            gidx, halocelloffsets[:, num], self.data[\"Group\"][name]\n        )\n        pdata[name] = hquantity\n\n    def get_grouplengths(self, parttype=\"PartType0\"):\n        \"\"\"\n        Get the lengths, i.e. the total number of particles, of a given type in all halos.\n\n        Parameters\n        ----------\n        parttype: str\n            Name of particle type\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        pnum = part_type_num(parttype)\n        ptype = \"PartType%i\" % pnum\n        if ptype not in self._grouplengths:\n            lengths = self.data[\"Group\"][\"GroupLenType\"][:, pnum].compute()\n            if isinstance(lengths, pint.Quantity):\n                lengths = lengths.magnitude\n            self._grouplengths[ptype] = lengths\n        return self._grouplengths[ptype]\n\n    def get_groupoffsets(self, parttype=\"PartType0\"):\n        \"\"\"\n        Get the array index offset of the first particle of a given type in each halo.\n\n        Parameters\n        ----------\n        parttype: str\n            Name of particle type\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        if parttype not in self._grouplengths:\n            # need to calculate group lengths first\n            self.get_grouplengths(parttype=parttype)\n        return self._groupoffsets[parttype]\n\n    @property\n    def _groupoffsets(self):\n        lengths = self._grouplengths\n        offsets = {\n            k: np.concatenate([[0], np.cumsum(v)[:-1]]) for k, v in lengths.items()\n        }\n        return offsets\n\n    def get_subhalolengths(self, parttype=\"PartType0\"):\n        \"\"\"\n        Get the lengths, i.e. the total number of particles, of a given type in all subhalos.\n\n        Parameters\n        ----------\n        parttype: str\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        pnum = part_type_num(parttype)\n        ptype = \"PartType%i\" % pnum\n        if ptype in self._subhalolengths:\n            return self._subhalolengths[ptype]\n        lengths = self.data[\"Subhalo\"][\"SubhaloLenType\"][:, pnum].compute()\n        if isinstance(lengths, pint.Quantity):\n            lengths = lengths.magnitude\n        self._subhalolengths[ptype] = lengths\n        return self._subhalolengths[ptype]\n\n    def get_subhalooffsets(self, parttype=\"PartType0\"):\n        \"\"\"\n        Get the array index offset of the first particle of a given type in each subhalo.\n\n        Parameters\n        ----------\n        parttype: str\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n\n        pnum = part_type_num(parttype)\n        ptype = \"PartType%i\" % pnum\n        if ptype in self._subhalooffsets:\n            return self._subhalooffsets[ptype]  # use cached result\n        goffsets = self.get_groupoffsets(ptype)\n        shgrnr = self.data[\"Subhalo\"][\"SubhaloGrNr\"]\n        # calculate the index of the first particle for the central subhalo of each subhalos's parent halo\n        shoffset_central = goffsets[shgrnr]\n\n        grpfirstsub = self.data[\"Group\"][\"GroupFirstSub\"]\n        shlens = self.get_subhalolengths(ptype)\n        shoffsets = np.concatenate([[0], np.cumsum(shlens)[:-1]])\n\n        # particle offset for the first subhalo of each group that a subhalo belongs to\n        shfirstshoffset = shoffsets[grpfirstsub[shgrnr]]\n\n        # \"LocalSubhaloOffset\": particle offset of each subhalo in the parent group\n        shoffset_local = shoffsets - shfirstshoffset\n\n        # \"SubhaloOffset\": particle offset of each subhalo in the simulation\n        offsets = shoffset_central + shoffset_local\n\n        self._subhalooffsets[ptype] = offsets\n\n        return offsets\n\n    def grouped(\n        self,\n        fields: str | list[str] | dict[str, da.Array] = \"\",\n        parttype=\"PartType0\",\n        objtype=\"halo\",\n    ):\n        \"\"\"\n        Create a GroupAwareOperation object for applying operations to groups.\n\n        Parameters\n        ----------\n        fields: str | list[str] | dict[str, da.Array]\n            Fields to pass to the operation. Can be a string, a dask array, a list of strings or a dictionary of dask arrays.\n        parttype: str\n            Particle type to operate on.\n        objtype: str\n            Type of object to operate on. Can be \"halo\" or \"subhalo\". Default: \"halo\"\n\n        Returns\n        -------\n        GroupAwareOperation\n        \"\"\"\n        inputfields = None\n        if isinstance(fields, str):\n            if fields == \"\":  # if nothing is specified, we pass all we have.\n                arrdict = self.data[parttype]\n            else:\n                arrdict = dict(field=self.data[parttype][fields])\n                inputfields = [fields]\n        elif isinstance(fields, da.Array):\n            arrdict = dict(daskarr=fields)\n            inputfields = [fields.name]\n        elif isinstance(fields, pint.Quantity):\n            field_mag = fields.magnitude\n            if isinstance(field_mag, da.Array):\n                arrdict = dict(daskarr=fields)\n                inputfields = [field_mag.name]\n            elif isinstance(field_mag, np.ndarray):\n                field_u = fields.units\n                arrdict = dict(\n                    daskarr=da.from_array(field_mag, chunks=\"auto\") * field_u\n                )\n                inputfields = [\"field\"]\n            else:\n                raise ValueError(\n                    \"Unknown input type '%s' decorated with pint quantity.\"\n                    % type(field_mag)\n                )\n        elif isinstance(fields, np.ndarray):\n            arrdict = dict(daskarr=da.from_array(fields, chunks=\"auto\"))\n            inputfields = [\"field\"]\n        elif isinstance(fields, list):\n            arrdict = {k: self.data[parttype][k] for k in fields}\n            inputfields = fields\n        elif isinstance(fields, dict):\n            arrdict = {}\n            arrdict.update(**fields)\n            inputfields = list(arrdict.keys())\n        else:\n            raise ValueError(\"Unknown input type '%s'.\" % type(fields))\n        check_shape_plausability = True\n        if check_shape_plausability and \"Coordinates\" in self.data[parttype]:\n            shape0_ref = self.data[parttype][\"Coordinates\"].shape[0]\n            for k, arr in arrdict.items():\n                if arr.shape[0] != shape0_ref:\n                    raise ValueError(\n                        f\"Shape mismatch: {k} has shape {arr.shape} while Coordinates has shape {shape0_ref}. \"\n                        f\"Check parttype passed to grouped()?\"\n                    )\n        objtype = grp_type_str(objtype)\n        if objtype == \"halo\":\n            offsets = self.get_groupoffsets(parttype=parttype)\n            lengths = self.get_grouplengths(parttype=parttype)\n        elif objtype == \"subhalo\":\n            offsets = self.get_subhalooffsets(parttype=parttype)\n            lengths = self.get_subhalolengths(parttype=parttype)\n        else:\n            raise ValueError(\"Unknown object type '%s'.\" % objtype)\n\n        gop = GroupAwareOperation(\n            offsets,\n            lengths,\n            arrdict,\n            inputfields=inputfields,\n        )\n        return gop\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ArepoSnapshot.__init__","title":"<code>__init__(path, chunksize='auto', catalog=None, **kwargs)</code>","text":"<p>Initialize an ArepoSnapshot object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>Path to snapshot, typically a directory containing multiple hdf5 files.</p> required <code>chunksize</code> <p>Chunksize to use for dask arrays. Can be \"auto\" to automatically determine chunksize.</p> <code>'auto'</code> <code>catalog</code> <p>Path to group catalog. If None, the catalog is searched for in the parent directories.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def __init__(self, path, chunksize=\"auto\", catalog=None, **kwargs) -&gt; None:\n    \"\"\"\n    Initialize an ArepoSnapshot object.\n\n    Parameters\n    ----------\n    path: str or pathlib.Path\n        Path to snapshot, typically a directory containing multiple hdf5 files.\n    chunksize:\n        Chunksize to use for dask arrays. Can be \"auto\" to automatically determine chunksize.\n    catalog:\n        Path to group catalog. If None, the catalog is searched for in the parent directories.\n    kwargs:\n        Additional keyword arguments.\n    \"\"\"\n    self.iscatalog = kwargs.pop(\"iscatalog\", False)\n    self.header: dict[str, Any] = {}\n    self.config: dict[str, Any] = {}\n    # check whether we have a cosmology mixin\n    self._defaultunitfiles: list[str] = [\"units/gadget_base.yaml\"]\n    if hasattr(self, \"_mixins\") and \"cosmology\" in self._mixins:\n        self._defaultunitfiles += [\"units/gadget_cosmological.yaml\"]\n    self.parameters: dict[str, Any] = {}\n    self._grouplengths: dict[str, Any] = {}\n    self._subhalolengths: dict[str, Any] = {}\n    # not needed for group catalogs as entries are back-to-back there, we will provide a property for this\n    self._subhalooffsets: dict[str, Any] = {}\n    self.misc: dict[str, Any] = {}  # for storing misc info\n    prfx = kwargs.pop(\"fileprefix\", None)\n    if prfx is None:\n        prfx = self._get_fileprefix(path)\n    super().__init__(path, chunksize=chunksize, fileprefix=prfx, **kwargs)\n\n    self.catalog = catalog\n    if not self.iscatalog:\n        if self.catalog is None:\n            self.discover_catalog()\n            # try to discover group catalog in parent directories.\n        if self.catalog == \"none\":\n            pass  # this string can be set to explicitly disable catalog\n        elif self.catalog:\n            catalog_cls = kwargs.get(\"catalog_cls\", None)\n            cosmological = False\n            if hasattr(self, \"_mixins\") and \"cosmology\" in self._mixins:\n                cosmological = True\n            self.load_catalog(\n                overwrite_cache=kwargs.get(\"overwrite_cache\", False),\n                catalog_cls=catalog_cls,\n                units=self.withunits,\n                cosmological=cosmological,\n            )\n\n    # add aliases\n    aliases = dict(\n        PartType0=[\"gas\", \"baryons\"],\n        PartType1=[\"dm\", \"dark matter\"],\n        PartType2=[\"lowres\", \"lowres dm\"],\n        PartType3=[\"tracer\", \"tracers\"],\n        PartType4=[\"stars\"],\n        PartType5=[\"bh\", \"black holes\"],\n    )\n    for k, lst in aliases.items():\n        if k not in self.data:\n            continue\n        for v in lst:\n            self.data.add_alias(v, k)\n\n    # set metadata\n    self._set_metadata()\n\n    # add some default fields\n    overwrite_fields = False\n    for kfield in fielddefs.fielddefs:\n        parttype = fielddefs.fielddefs[kfield].parttype\n        if parttype in self.data:\n            if overwrite_fields or kfield not in self.data[parttype]:\n                func = fielddefs.fielddefs[kfield].func\n                # verify that we have the dependencies\n                dependencies = fielddefs.fielddefs[kfield].dependencies\n                deps_fulfilled = True\n                for dep in dependencies:\n                    if dep not in self.data[parttype]:\n                        deps_fulfilled = False\n                        break\n                if deps_fulfilled:\n                    self.data.register_field(parttype, name=kfield)(func)\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ArepoSnapshot.add_catalogIDs","title":"<code>add_catalogIDs()</code>","text":"<p>Add field for halo and subgroup IDs for all particle types.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def add_catalogIDs(self) -&gt; None:\n    \"\"\"\n    Add field for halo and subgroup IDs for all particle types.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    # TODO: make these delayed objects and properly pass into (delayed?) numba functions:\n    # https://docs.dask.org/en/stable/delayed-best-practices.html#avoid-repeatedly-putting-large-inputs-into-delayed-calls\n\n    maxint = np.iinfo(np.int64).max\n    self.misc[\"unboundID\"] = maxint\n\n    # Group ID\n    if \"Group\" not in self.data:  # can happen for empty catalogs\n        for key in self.data:\n            if not (key.startswith(\"PartType\")):\n                continue\n            uid = self.data[key][\"uid\"]\n            self.data[key][\"GroupID\"] = self.misc[\"unboundID\"] * da.ones_like(\n                uid, dtype=np.int64\n            )\n            self.data[key][\"SubhaloID\"] = self.misc[\"unboundID\"] * da.ones_like(\n                uid, dtype=np.int64\n            )\n        return\n\n    glen = self.data[\"Group\"][\"GroupLenType\"]\n    ngrp = glen.shape[0]\n    da_halocelloffsets = da.concatenate(\n        [\n            np.zeros((1, 6), dtype=np.int64),\n            da.cumsum(glen, axis=0, dtype=np.int64),\n        ]\n    )\n    # remove last entry to match shapematch shape\n    self.data[\"Group\"][\"GroupOffsetsType\"] = da_halocelloffsets[:-1].rechunk(\n        glen.chunks\n    )\n    halocelloffsets = da_halocelloffsets.rechunk(-1)\n\n    index_unbound = self.misc[\"unboundID\"]\n\n    for key in self.data:\n        if not (key.startswith(\"PartType\")):\n            continue\n        num = int(key[-1])\n        if \"uid\" not in self.data[key]:\n            continue  # can happen for empty containers\n        gidx = self.data[key][\"uid\"]\n        hidx = compute_haloindex(\n            gidx, halocelloffsets[:, num], index_unbound=index_unbound\n        )\n        self.data[key][\"GroupID\"] = hidx\n\n    # Subhalo ID\n    if \"Subhalo\" not in self.data:  # can happen for empty catalogs\n        for key in self.data:\n            if not (key.startswith(\"PartType\")):\n                continue\n            self.data[key][\"SubhaloID\"] = -1 * da.ones_like(\n                da[key][\"uid\"], dtype=np.int64\n            )\n        return\n\n    shnr_attr = \"SubhaloGrNr\"\n    if shnr_attr not in self.data[\"Subhalo\"]:\n        shnr_attr = \"SubhaloGroupNr\"  # what MTNG does\n    if shnr_attr not in self.data[\"Subhalo\"]:\n        raise ValueError(\n            f\"Could not find 'SubhaloGrNr' or 'SubhaloGroupNr' in {self.catalog}\"\n        )\n\n    subhalogrnr = self.data[\"Subhalo\"][shnr_attr]\n    subhalocellcounts = self.data[\"Subhalo\"][\"SubhaloLenType\"]\n\n    # remove \"units\" for numba funcs\n    if hasattr(subhalogrnr, \"magnitude\"):\n        subhalogrnr = subhalogrnr.magnitude\n    if hasattr(subhalocellcounts, \"magnitude\"):\n        subhalocellcounts = subhalocellcounts.magnitude\n\n    grp = self.data[\"Group\"]\n    if \"GroupFirstSub\" not in grp or \"GroupNsubs\" not in grp:\n        # if not provided, we calculate:\n        # \"GroupFirstSub\": First subhalo index for each halo\n        # \"GroupNsubs\": Number of subhalos for each halo\n        dlyd = delayed(get_shcounts_shcells)(subhalogrnr, ngrp)\n        grp[\"GroupFirstSub\"] = dask.compute(dlyd[1])[0]\n        grp[\"GroupNsubs\"] = dask.compute(dlyd[0])[0]\n\n    # remove \"units\" for numba funcs\n    grpfirstsub = grp[\"GroupFirstSub\"]\n    if hasattr(grpfirstsub, \"magnitude\"):\n        grpfirstsub = grpfirstsub.magnitude\n    grpnsubs = grp[\"GroupNsubs\"]\n    if hasattr(grpnsubs, \"magnitude\"):\n        grpnsubs = grpnsubs.magnitude\n\n    for key in self.data:\n        if not (key.startswith(\"PartType\")):\n            continue\n        num = int(key[-1])\n        pdata = self.data[key]\n        if \"uid\" not in self.data[key]:\n            continue  # can happen for empty containers\n        gidx = pdata[\"uid\"]\n\n        # we need to make other dask arrays delayed,\n        # map_block does not incorrectly infer output shape from these\n        halocelloffsets_dlyd = delayed(halocelloffsets[:, num])\n        grpfirstsub_dlyd = delayed(grpfirstsub)\n        grpnsubs_dlyd = delayed(grpnsubs)\n        subhalocellcounts_dlyd = delayed(subhalocellcounts[:, num])\n\n        sidx = compute_localsubhaloindex(\n            gidx,\n            halocelloffsets_dlyd,\n            grpfirstsub_dlyd,\n            grpnsubs_dlyd,\n            subhalocellcounts_dlyd,\n            index_unbound=index_unbound,\n        )\n\n        pdata[\"LocalSubhaloID\"] = sidx\n\n        # reconstruct SubhaloID from Group's GroupFirstSub and LocalSubhaloID\n        # should be easier to do it directly, but quicker to write down like this:\n\n        # calculate first subhalo of each halo that a particle belongs to\n        self.add_groupquantity_to_particles(\"GroupFirstSub\", parttype=key)\n        pdata[\"SubhaloID\"] = pdata[\"GroupFirstSub\"] + pdata[\"LocalSubhaloID\"]\n        pdata[\"SubhaloID\"] = da.where(\n            pdata[\"SubhaloID\"] == index_unbound, index_unbound, pdata[\"SubhaloID\"]\n        )\n\n    # add GroupID and SubhaloID to catalogs/groups themselves\n    self.data[\"Group\"][\"GroupID\"] = self.data[\"Group\"][\"uid\"]\n    self.data[\"Subhalo\"][\"SubhaloID\"] = self.data[\"Subhalo\"][\"uid\"]\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ArepoSnapshot.add_groupquantity_to_particles","title":"<code>add_groupquantity_to_particles(name, parttype='PartType0')</code>","text":"<p>Map a quantity from the group catalog to the particles based on a particle's group index.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>Name of quantity to map</p> required <code>parttype</code> <p>Name of particle type</p> <code>'PartType0'</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def add_groupquantity_to_particles(self, name, parttype=\"PartType0\"):\n    \"\"\"\n    Map a quantity from the group catalog to the particles based on a particle's group index.\n\n    Parameters\n    ----------\n    name: str\n        Name of quantity to map\n    parttype: str\n        Name of particle type\n\n    Returns\n    -------\n    None\n    \"\"\"\n    pdata = self.data[parttype]\n    assert (\n        name not in pdata\n    )  # we simply map the name from Group to Particle for now. Should work (?)\n    glen = self.data[\"Group\"][\"GroupLenType\"]\n    da_halocelloffsets = da.concatenate(\n        [np.zeros((1, 6), dtype=np.int64), da.cumsum(glen, axis=0)]\n    )\n    if \"GroupOffsetsType\" not in self.data[\"Group\"]:\n        self.data[\"Group\"][\"GroupOffsetsType\"] = da_halocelloffsets[:-1].rechunk(\n            glen.chunks\n        )  # remove last entry to match shape\n    halocelloffsets = da_halocelloffsets.compute()\n\n    gidx = pdata[\"uid\"]\n    num = int(parttype[-1])\n    hquantity = compute_haloquantity(\n        gidx, halocelloffsets[:, num], self.data[\"Group\"][name]\n    )\n    pdata[name] = hquantity\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ArepoSnapshot.discover_catalog","title":"<code>discover_catalog()</code>","text":"<p>Discover the group catalog given the current path</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def discover_catalog(self):\n    \"\"\"\n    Discover the group catalog given the current path\n\n    Returns\n    -------\n    None\n    \"\"\"\n    p = str(self.path)\n    # order of candidates matters. For Illustris \"groups\" must precede \"fof_subhalo_tab\"\n    candidates = [\n        p.replace(\"snapshot\", \"group\"),\n        p.replace(\"snapshot\", \"groups\"),\n        p.replace(\"snap\", \"groups\"),\n        p.replace(\"snap\", \"group\"),\n        p.replace(\"snapdir\", \"groups\").replace(\"snap\", \"groups\"),\n        p.replace(\"snapdir\", \"groups\").replace(\"snap\", \"fof_subhalo_tab\"),\n    ]\n    for candidate in candidates:\n        if not os.path.exists(candidate):\n            continue\n        if candidate == p:\n            continue  # do not set catalog to snapshot itself\n        if candidate == self.path:\n            continue\n        self.catalog = candidate\n        break\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ArepoSnapshot.get_grouplengths","title":"<code>get_grouplengths(parttype='PartType0')</code>","text":"<p>Get the lengths, i.e. the total number of particles, of a given type in all halos.</p> <p>Parameters:</p> Name Type Description Default <code>parttype</code> <p>Name of particle type</p> <code>'PartType0'</code> <p>Returns:</p> Type Description <code>ndarray</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def get_grouplengths(self, parttype=\"PartType0\"):\n    \"\"\"\n    Get the lengths, i.e. the total number of particles, of a given type in all halos.\n\n    Parameters\n    ----------\n    parttype: str\n        Name of particle type\n\n    Returns\n    -------\n    np.ndarray\n    \"\"\"\n    pnum = part_type_num(parttype)\n    ptype = \"PartType%i\" % pnum\n    if ptype not in self._grouplengths:\n        lengths = self.data[\"Group\"][\"GroupLenType\"][:, pnum].compute()\n        if isinstance(lengths, pint.Quantity):\n            lengths = lengths.magnitude\n        self._grouplengths[ptype] = lengths\n    return self._grouplengths[ptype]\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ArepoSnapshot.get_groupoffsets","title":"<code>get_groupoffsets(parttype='PartType0')</code>","text":"<p>Get the array index offset of the first particle of a given type in each halo.</p> <p>Parameters:</p> Name Type Description Default <code>parttype</code> <p>Name of particle type</p> <code>'PartType0'</code> <p>Returns:</p> Type Description <code>ndarray</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def get_groupoffsets(self, parttype=\"PartType0\"):\n    \"\"\"\n    Get the array index offset of the first particle of a given type in each halo.\n\n    Parameters\n    ----------\n    parttype: str\n        Name of particle type\n\n    Returns\n    -------\n    np.ndarray\n    \"\"\"\n    if parttype not in self._grouplengths:\n        # need to calculate group lengths first\n        self.get_grouplengths(parttype=parttype)\n    return self._groupoffsets[parttype]\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ArepoSnapshot.get_subhalolengths","title":"<code>get_subhalolengths(parttype='PartType0')</code>","text":"<p>Get the lengths, i.e. the total number of particles, of a given type in all subhalos.</p> <p>Parameters:</p> Name Type Description Default <code>parttype</code> <code>'PartType0'</code> <p>Returns:</p> Type Description <code>ndarray</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def get_subhalolengths(self, parttype=\"PartType0\"):\n    \"\"\"\n    Get the lengths, i.e. the total number of particles, of a given type in all subhalos.\n\n    Parameters\n    ----------\n    parttype: str\n\n    Returns\n    -------\n    np.ndarray\n    \"\"\"\n    pnum = part_type_num(parttype)\n    ptype = \"PartType%i\" % pnum\n    if ptype in self._subhalolengths:\n        return self._subhalolengths[ptype]\n    lengths = self.data[\"Subhalo\"][\"SubhaloLenType\"][:, pnum].compute()\n    if isinstance(lengths, pint.Quantity):\n        lengths = lengths.magnitude\n    self._subhalolengths[ptype] = lengths\n    return self._subhalolengths[ptype]\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ArepoSnapshot.get_subhalooffsets","title":"<code>get_subhalooffsets(parttype='PartType0')</code>","text":"<p>Get the array index offset of the first particle of a given type in each subhalo.</p> <p>Parameters:</p> Name Type Description Default <code>parttype</code> <code>'PartType0'</code> <p>Returns:</p> Type Description <code>ndarray</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def get_subhalooffsets(self, parttype=\"PartType0\"):\n    \"\"\"\n    Get the array index offset of the first particle of a given type in each subhalo.\n\n    Parameters\n    ----------\n    parttype: str\n\n    Returns\n    -------\n    np.ndarray\n    \"\"\"\n\n    pnum = part_type_num(parttype)\n    ptype = \"PartType%i\" % pnum\n    if ptype in self._subhalooffsets:\n        return self._subhalooffsets[ptype]  # use cached result\n    goffsets = self.get_groupoffsets(ptype)\n    shgrnr = self.data[\"Subhalo\"][\"SubhaloGrNr\"]\n    # calculate the index of the first particle for the central subhalo of each subhalos's parent halo\n    shoffset_central = goffsets[shgrnr]\n\n    grpfirstsub = self.data[\"Group\"][\"GroupFirstSub\"]\n    shlens = self.get_subhalolengths(ptype)\n    shoffsets = np.concatenate([[0], np.cumsum(shlens)[:-1]])\n\n    # particle offset for the first subhalo of each group that a subhalo belongs to\n    shfirstshoffset = shoffsets[grpfirstsub[shgrnr]]\n\n    # \"LocalSubhaloOffset\": particle offset of each subhalo in the parent group\n    shoffset_local = shoffsets - shfirstshoffset\n\n    # \"SubhaloOffset\": particle offset of each subhalo in the simulation\n    offsets = shoffset_central + shoffset_local\n\n    self._subhalooffsets[ptype] = offsets\n\n    return offsets\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ArepoSnapshot.grouped","title":"<code>grouped(fields='', parttype='PartType0', objtype='halo')</code>","text":"<p>Create a GroupAwareOperation object for applying operations to groups.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>str | list[str] | dict[str, Array]</code> <p>Fields to pass to the operation. Can be a string, a dask array, a list of strings or a dictionary of dask arrays.</p> <code>''</code> <code>parttype</code> <p>Particle type to operate on.</p> <code>'PartType0'</code> <code>objtype</code> <p>Type of object to operate on. Can be \"halo\" or \"subhalo\". Default: \"halo\"</p> <code>'halo'</code> <p>Returns:</p> Type Description <code>GroupAwareOperation</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def grouped(\n    self,\n    fields: str | list[str] | dict[str, da.Array] = \"\",\n    parttype=\"PartType0\",\n    objtype=\"halo\",\n):\n    \"\"\"\n    Create a GroupAwareOperation object for applying operations to groups.\n\n    Parameters\n    ----------\n    fields: str | list[str] | dict[str, da.Array]\n        Fields to pass to the operation. Can be a string, a dask array, a list of strings or a dictionary of dask arrays.\n    parttype: str\n        Particle type to operate on.\n    objtype: str\n        Type of object to operate on. Can be \"halo\" or \"subhalo\". Default: \"halo\"\n\n    Returns\n    -------\n    GroupAwareOperation\n    \"\"\"\n    inputfields = None\n    if isinstance(fields, str):\n        if fields == \"\":  # if nothing is specified, we pass all we have.\n            arrdict = self.data[parttype]\n        else:\n            arrdict = dict(field=self.data[parttype][fields])\n            inputfields = [fields]\n    elif isinstance(fields, da.Array):\n        arrdict = dict(daskarr=fields)\n        inputfields = [fields.name]\n    elif isinstance(fields, pint.Quantity):\n        field_mag = fields.magnitude\n        if isinstance(field_mag, da.Array):\n            arrdict = dict(daskarr=fields)\n            inputfields = [field_mag.name]\n        elif isinstance(field_mag, np.ndarray):\n            field_u = fields.units\n            arrdict = dict(\n                daskarr=da.from_array(field_mag, chunks=\"auto\") * field_u\n            )\n            inputfields = [\"field\"]\n        else:\n            raise ValueError(\n                \"Unknown input type '%s' decorated with pint quantity.\"\n                % type(field_mag)\n            )\n    elif isinstance(fields, np.ndarray):\n        arrdict = dict(daskarr=da.from_array(fields, chunks=\"auto\"))\n        inputfields = [\"field\"]\n    elif isinstance(fields, list):\n        arrdict = {k: self.data[parttype][k] for k in fields}\n        inputfields = fields\n    elif isinstance(fields, dict):\n        arrdict = {}\n        arrdict.update(**fields)\n        inputfields = list(arrdict.keys())\n    else:\n        raise ValueError(\"Unknown input type '%s'.\" % type(fields))\n    check_shape_plausability = True\n    if check_shape_plausability and \"Coordinates\" in self.data[parttype]:\n        shape0_ref = self.data[parttype][\"Coordinates\"].shape[0]\n        for k, arr in arrdict.items():\n            if arr.shape[0] != shape0_ref:\n                raise ValueError(\n                    f\"Shape mismatch: {k} has shape {arr.shape} while Coordinates has shape {shape0_ref}. \"\n                    f\"Check parttype passed to grouped()?\"\n                )\n    objtype = grp_type_str(objtype)\n    if objtype == \"halo\":\n        offsets = self.get_groupoffsets(parttype=parttype)\n        lengths = self.get_grouplengths(parttype=parttype)\n    elif objtype == \"subhalo\":\n        offsets = self.get_subhalooffsets(parttype=parttype)\n        lengths = self.get_subhalolengths(parttype=parttype)\n    else:\n        raise ValueError(\"Unknown object type '%s'.\" % objtype)\n\n    gop = GroupAwareOperation(\n        offsets,\n        lengths,\n        arrdict,\n        inputfields=inputfields,\n    )\n    return gop\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ArepoSnapshot.load_catalog","title":"<code>load_catalog(overwrite_cache=False, units=False, cosmological=False, catalog_cls=None)</code>","text":"<p>Load the group catalog.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Keyword arguments passed to the catalog class.</p> required <code>catalog_cls</code> <p>Class to use for the catalog. If None, the default catalog class is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def load_catalog(\n    self, overwrite_cache=False, units=False, cosmological=False, catalog_cls=None\n):\n    \"\"\"\n    Load the group catalog.\n\n    Parameters\n    ----------\n    kwargs: dict\n        Keyword arguments passed to the catalog class.\n    catalog_cls: type\n        Class to use for the catalog. If None, the default catalog class is used.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    virtualcache = True\n    # fileprefix = catalog_kwargs.get(\"fileprefix\", self._fileprefix_catalog)\n    prfx = self._get_fileprefix(self.catalog)\n\n    # explicitly need to create unitaware class for catalog as needed\n    # TODO: should just be determined from mixins of parent?\n    if catalog_cls is None:\n        cls = ArepoCatalog\n    else:\n        cls = catalog_cls\n    withunits = units\n    mixins = []\n    if withunits:\n        mixins += [UnitMixin]\n\n    other_mixins = _determine_mixins(path=self.path)\n    mixins += other_mixins\n    if cosmological and CosmologyMixin not in mixins:\n        mixins.append(CosmologyMixin)\n\n    cls = create_datasetclass_with_mixins(cls, mixins)\n\n    ureg = None\n    if hasattr(self, \"ureg\"):\n        ureg = self.ureg\n\n    # non-virtual catalog fields for better performance\n    nonvirtual_datasets = [\n        \"Group/GroupFirstSub\",\n        \"Group/GroupLenType\",\n        \"Group/GroupNsubs\",\n        \"Subhalo/SubhaloGrNr\",\n        \"Subhalo/SubhaloGroupNr\",\n        \"Subhalo/SubhaloLenType\",\n    ]\n\n    self.catalog = cls(\n        self.catalog,\n        overwrite_cache=overwrite_cache,\n        virtualcache=virtualcache,\n        nonvirtual_datasets=nonvirtual_datasets,\n        fileprefix=prfx,\n        units=self.withunits,\n        ureg=ureg,\n        hints=self.hints,\n        metadata_raw_parent=self._metadata_raw,\n    )\n    if \"Redshift\" in self.catalog.header and \"Redshift\" in self.header:\n        z_catalog = self.catalog.header[\"Redshift\"]\n        z_snap = self.header[\"Redshift\"]\n        if not np.isclose(z_catalog, z_snap):\n            raise ValueError(\n                \"Redshift mismatch between snapshot and catalog: \"\n                f\"{z_snap:.2f} vs {z_catalog:.2f}\"\n            )\n\n    # merge data\n    self.merge_data(self.catalog)\n\n    # first snapshots often do not have groups\n    if \"Group\" in self.catalog.data:\n        ngkeys = self.catalog.data[\"Group\"].keys()\n        if len(ngkeys) &gt; 0:\n            self.add_catalogIDs()\n\n    # merge hints from snap and catalog\n    self.merge_hints(self.catalog)\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ArepoSnapshot.map_group_operation","title":"<code>map_group_operation(func, cpucost_halo=10000.0, nchunks_min=None, chunksize_bytes=None, nmax=None, idxlist=None, objtype='halo')</code>","text":"<p>Apply a function to each halo in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>objtype</code> <p>Type of object to process. Can be \"halo\" or \"subhalo\". Default: \"halo\"</p> <code>'halo'</code> <code>idxlist</code> <p>List of halo indices to process. If not provided, all halos are processed.</p> <code>None</code> <code>func</code> <p>Function to apply to each halo. Must take a dictionary of arrays as input.</p> required <code>cpucost_halo</code> <p>\"CPU cost\" of processing a single halo. This is a relative value to the processing time per input particle used for calculating the dask chunks. Default: 1e4</p> <code>10000.0</code> <code>nchunks_min</code> <p>Minimum number of particles in a halo to process it. Default: None</p> <code>None</code> <code>chunksize_bytes</code> <code>None</code> <code>nmax</code> <p>Only process the first nmax halos.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>@computedecorator\ndef map_group_operation(\n    self,\n    func,\n    cpucost_halo=1e4,\n    nchunks_min=None,\n    chunksize_bytes=None,\n    nmax=None,\n    idxlist=None,\n    objtype=\"halo\",\n):\n    \"\"\"\n    Apply a function to each halo in the catalog.\n\n    Parameters\n    ----------\n    objtype: str\n        Type of object to process. Can be \"halo\" or \"subhalo\". Default: \"halo\"\n    idxlist: np.ndarray | None\n        List of halo indices to process. If not provided, all halos are processed.\n    func: function\n        Function to apply to each halo. Must take a dictionary of arrays as input.\n    cpucost_halo:\n        \"CPU cost\" of processing a single halo. This is a relative value to the processing time per input particle\n        used for calculating the dask chunks. Default: 1e4\n    nchunks_min: int | None\n        Minimum number of particles in a halo to process it. Default: None\n    chunksize_bytes: int | None\n    nmax: int | None\n        Only process the first nmax halos.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    dfltkwargs = get_kwargs(func)\n    fieldnames = dfltkwargs.get(\"fieldnames\", None)\n    if fieldnames is None:\n        fieldnames = get_args(func)\n    parttype = dfltkwargs.get(\"parttype\", \"PartType0\")\n    entry_nbytes_in = np.sum([self.data[parttype][f][0].nbytes for f in fieldnames])\n    objtype = grp_type_str(objtype)\n    if objtype == \"halo\":\n        lengths = self.get_grouplengths(parttype=parttype)\n        offsets = self.get_groupoffsets(parttype=parttype)\n    elif objtype == \"subhalo\":\n        lengths = self.get_subhalolengths(parttype=parttype)\n        offsets = self.get_subhalooffsets(parttype=parttype)\n    else:\n        raise ValueError(f\"objtype must be 'halo' or 'subhalo', not {objtype}\")\n    arrdict = self.data[parttype]\n    return map_group_operation(\n        func,\n        offsets,\n        lengths,\n        arrdict,\n        cpucost_halo=cpucost_halo,\n        nchunks_min=nchunks_min,\n        chunksize_bytes=chunksize_bytes,\n        entry_nbytes_in=entry_nbytes_in,\n        nmax=nmax,\n        idxlist=idxlist,\n    )\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ArepoSnapshot.register_field","title":"<code>register_field(parttype, name=None, construct=False)</code>","text":"<p>Register a field.</p> <p>Parameters:</p> Name Type Description Default <code>parttype</code> <code>str</code> <p>name of particle type</p> required <code>name</code> <code>str | None</code> <p>name of field</p> <code>None</code> <code>construct</code> <code>bool</code> <p>construct field immediately</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def register_field(\n    self, parttype: str, name: str | None = None, construct: bool = False\n):\n    \"\"\"\n    Register a field.\n    Parameters\n    ----------\n    parttype: str\n        name of particle type\n    name: str\n        name of field\n    construct: bool\n        construct field immediately\n\n    Returns\n    -------\n    None\n    \"\"\"\n    num = part_type_num(parttype)\n    if construct:  # TODO: introduce (immediate) construct option later\n        raise NotImplementedError\n    if num == -1:  # TODO: all particle species\n        key = \"all\"\n        raise NotImplementedError\n    elif isinstance(num, int):\n        key = \"PartType\" + str(num)\n    else:\n        key = parttype\n    return super().register_field(key, name=name)\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ArepoSnapshot.return_data","title":"<code>return_data()</code>","text":"<p>Return data object of this snapshot.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>@ArepoSelector()\ndef return_data(self) -&gt; FieldContainer:\n    \"\"\"\n    Return data object of this snapshot.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    return super().return_data()\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ArepoSnapshot.validate_path","title":"<code>validate_path(path, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Validate a path to use for instantiation of this class.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | PathLike</code> required <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>CandidateStatus</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>@classmethod\ndef validate_path(cls, path: str | os.PathLike, *args, **kwargs) -&gt; CandidateStatus:\n    \"\"\"\n    Validate a path to use for instantiation of this class.\n\n    Parameters\n    ----------\n    path: str or pathlib.Path\n    args:\n    kwargs:\n\n    Returns\n    -------\n    CandidateStatus\n    \"\"\"\n    valid = super().validate_path(path, *args, **kwargs)\n    if valid.value &gt; CandidateStatus.MAYBE.value:\n        valid = CandidateStatus.MAYBE\n    else:\n        return valid\n    # Arepo has no dedicated attribute to identify such runs.\n    # lets just query a bunch of attributes that are present for arepo runs\n    metadata_raw = load_metadata(path, **kwargs)\n    matchingattrs = True\n    matchingattrs &amp;= \"Git_commit\" in metadata_raw[\"/Header\"]\n    # not existent for any arepo run?\n    matchingattrs &amp;= \"Compactify_Version\" not in metadata_raw[\"/Header\"]\n\n    if matchingattrs:\n        valid = CandidateStatus.MAYBE\n\n    return valid\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ChainOps","title":"<code>ChainOps</code>","text":"<p>Chain operations together.</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>class ChainOps:\n    \"\"\"\n    Chain operations together.\n    \"\"\"\n\n    def __init__(self, *funcs):\n        \"\"\"\n        Initialize a ChainOps object.\n\n        Parameters\n        ----------\n        funcs: list[function]\n            Functions to chain together.\n        \"\"\"\n        self.funcs = funcs\n        self.kwargs = get_kwargs(\n            funcs[-1]\n        )  # so we can pass info from kwargs to map_halo_operation\n        if self.kwargs.get(\"dtype\") is None:\n            self.kwargs[\"dtype\"] = float\n\n        def chained_call(*args):\n            cf = None\n            for i, f in enumerate(funcs):\n                # first chain element can be multiple fields. treat separately\n                if i == 0:\n                    cf = f(*args)\n                else:\n                    cf = f(cf)\n            return cf\n\n        self.call = chained_call\n\n    def __call__(self, *args, **kwargs):\n        return self.call(*args, **kwargs)\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.ChainOps.__init__","title":"<code>__init__(*funcs)</code>","text":"<p>Initialize a ChainOps object.</p> <p>Parameters:</p> Name Type Description Default <code>funcs</code> <p>Functions to chain together.</p> <code>()</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def __init__(self, *funcs):\n    \"\"\"\n    Initialize a ChainOps object.\n\n    Parameters\n    ----------\n    funcs: list[function]\n        Functions to chain together.\n    \"\"\"\n    self.funcs = funcs\n    self.kwargs = get_kwargs(\n        funcs[-1]\n    )  # so we can pass info from kwargs to map_halo_operation\n    if self.kwargs.get(\"dtype\") is None:\n        self.kwargs[\"dtype\"] = float\n\n    def chained_call(*args):\n        cf = None\n        for i, f in enumerate(funcs):\n            # first chain element can be multiple fields. treat separately\n            if i == 0:\n                cf = f(*args)\n            else:\n                cf = f(cf)\n        return cf\n\n    self.call = chained_call\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.GroupAwareOperation","title":"<code>GroupAwareOperation</code>","text":"<p>Class for applying operations to groups.</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>class GroupAwareOperation:\n    \"\"\"\n    Class for applying operations to groups.\n    \"\"\"\n\n    opfuncs = dict(min=np.min, max=np.max, sum=np.sum, half=lambda x: x[::2])\n    finalops = {\"min\", \"max\", \"sum\"}\n    __slots__ = (\n        \"arrs\",\n        \"ops\",\n        \"offsets\",\n        \"lengths\",\n        \"final\",\n        \"inputfields\",\n        \"opfuncs_custom\",\n    )\n\n    def __init__(\n        self,\n        offsets: NDArray,\n        lengths: NDArray,\n        arrs: dict[str, da.Array],\n        ops=None,\n        inputfields=None,\n    ):\n        self.offsets = offsets\n        self.lengths = lengths\n        self.arrs = arrs\n        self.opfuncs_custom: dict[str, Any] = {}\n        self.final = False\n        self.inputfields = inputfields\n        if ops is None:\n            self.ops = []\n        else:\n            self.ops = ops\n\n    def chain(self, add_op=None, final=False):\n        \"\"\"\n        Chain another operation to this one.\n\n        Parameters\n        ----------\n        add_op: str or function\n            Operation to add. Can be a string (e.g. \"min\", \"max\", \"sum\") or a function.\n        final: bool\n            Whether this is the final operation in the chain.\n\n        Returns\n        -------\n        GroupAwareOperation\n        \"\"\"\n        if self.final:\n            raise ValueError(\"Cannot chain any additional operation.\")\n        c = copy.copy(self)\n        c.final = final\n        c.opfuncs_custom = self.opfuncs_custom\n        if add_op is not None:\n            if isinstance(add_op, str):\n                c.ops.append(add_op)\n            elif callable(add_op):\n                name = \"custom\" + str(len(self.opfuncs_custom) + 1)\n                c.opfuncs_custom[name] = add_op\n                c.ops.append(name)\n            else:\n                raise ValueError(\"Unknown operation of type '%s'\" % str(type(add_op)))\n        return c\n\n    def min(self, field=None):\n        \"\"\"\n        Get the minimum value for each group member.\n        \"\"\"\n        if field is not None:\n            if self.inputfields is not None:\n                raise ValueError(\"Cannot change input field anymore.\")\n            self.inputfields = [field]\n        return self.chain(add_op=\"min\", final=True)\n\n    def max(self, field=None):\n        \"\"\"\n        Get the maximum value for each group member.\n        \"\"\"\n        if field is not None:\n            if self.inputfields is not None:\n                raise ValueError(\"Cannot change input field anymore.\")\n            self.inputfields = [field]\n        return self.chain(add_op=\"max\", final=True)\n\n    def sum(self, field=None):\n        \"\"\"\n        Sum the values for each group member.\n        \"\"\"\n        if field is not None:\n            if self.inputfields is not None:\n                raise ValueError(\"Cannot change input field anymore.\")\n            self.inputfields = [field]\n        return self.chain(add_op=\"sum\", final=True)\n\n    def half(self):\n        \"\"\"\n        Half the number of particles in each group member. For testing purposes.\n\n        Returns\n        -------\n        GroupAwareOperation\n        \"\"\"\n        return self.chain(add_op=\"half\", final=False)\n\n    def apply(self, func, final=False):\n        \"\"\"\n        Apply a passed function.\n\n        Parameters\n        ----------\n        func: function\n            Function to apply.\n        final: bool\n            Whether this is the final operation in the chain.\n\n        Returns\n        -------\n        GroupAwareOperation\n        \"\"\"\n        return self.chain(add_op=func, final=final)\n\n    def __copy__(self):\n        # overwrite method so that copy holds a new ops list.\n        c = type(self)(\n            self.offsets,\n            self.lengths,\n            self.arrs,\n            ops=list(self.ops),\n            inputfields=self.inputfields,\n        )\n        return c\n\n    def evaluate(self, nmax=None, idxlist=None, compute=True):\n        \"\"\"\n        Evaluate the operation.\n\n        Parameters\n        ----------\n        nmax: int | None\n            Maximum number of halos to process.\n        idxlist: np.ndarray | None\n            List of halo indices to process. If not provided, (and nmax not set) all halos are processed.\n        compute: bool\n            Whether to compute the result immediately or return a dask object to compute later.\n\n        Returns\n        -------\n\n        \"\"\"\n        # TODO: figure out return type\n        # final operations: those that can only be at end of chain\n        # intermediate operations: those that can only be prior to end of chain\n        funcdict: dict[str, Any] = dict()\n        funcdict.update(**self.opfuncs)\n        funcdict.update(**self.opfuncs_custom)\n\n        func = ChainOps(*[funcdict[k] for k in self.ops])\n\n        fieldnames = list(self.arrs.keys())\n        if self.inputfields is None:\n            opname = self.ops[0]\n            if opname.startswith(\"custom\"):\n                dfltkwargs = get_kwargs(self.opfuncs_custom[opname])\n                fieldnames = dfltkwargs.get(\"fieldnames\", None)\n                if isinstance(fieldnames, str):\n                    fieldnames = [fieldnames]\n                if fieldnames is None:\n                    raise ValueError(\n                        \"Either pass fields to grouped(fields=...) \"\n                        \"or specify fieldnames=... in applied func.\"\n                    )\n            else:\n                raise ValueError(\n                    \"Specify field to operate on in operation or grouped().\"\n                )\n\n        res = map_group_operation(\n            func,\n            self.offsets,\n            self.lengths,\n            self.arrs,\n            fieldnames=fieldnames,\n            nmax=nmax,\n            idxlist=idxlist,\n        )\n        if compute:\n            res = res.compute()\n        return res\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.GroupAwareOperation.apply","title":"<code>apply(func, final=False)</code>","text":"<p>Apply a passed function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <p>Function to apply.</p> required <code>final</code> <p>Whether this is the final operation in the chain.</p> <code>False</code> <p>Returns:</p> Type Description <code>GroupAwareOperation</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def apply(self, func, final=False):\n    \"\"\"\n    Apply a passed function.\n\n    Parameters\n    ----------\n    func: function\n        Function to apply.\n    final: bool\n        Whether this is the final operation in the chain.\n\n    Returns\n    -------\n    GroupAwareOperation\n    \"\"\"\n    return self.chain(add_op=func, final=final)\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.GroupAwareOperation.chain","title":"<code>chain(add_op=None, final=False)</code>","text":"<p>Chain another operation to this one.</p> <p>Parameters:</p> Name Type Description Default <code>add_op</code> <p>Operation to add. Can be a string (e.g. \"min\", \"max\", \"sum\") or a function.</p> <code>None</code> <code>final</code> <p>Whether this is the final operation in the chain.</p> <code>False</code> <p>Returns:</p> Type Description <code>GroupAwareOperation</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def chain(self, add_op=None, final=False):\n    \"\"\"\n    Chain another operation to this one.\n\n    Parameters\n    ----------\n    add_op: str or function\n        Operation to add. Can be a string (e.g. \"min\", \"max\", \"sum\") or a function.\n    final: bool\n        Whether this is the final operation in the chain.\n\n    Returns\n    -------\n    GroupAwareOperation\n    \"\"\"\n    if self.final:\n        raise ValueError(\"Cannot chain any additional operation.\")\n    c = copy.copy(self)\n    c.final = final\n    c.opfuncs_custom = self.opfuncs_custom\n    if add_op is not None:\n        if isinstance(add_op, str):\n            c.ops.append(add_op)\n        elif callable(add_op):\n            name = \"custom\" + str(len(self.opfuncs_custom) + 1)\n            c.opfuncs_custom[name] = add_op\n            c.ops.append(name)\n        else:\n            raise ValueError(\"Unknown operation of type '%s'\" % str(type(add_op)))\n    return c\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.GroupAwareOperation.evaluate","title":"<code>evaluate(nmax=None, idxlist=None, compute=True)</code>","text":"<p>Evaluate the operation.</p> <p>Parameters:</p> Name Type Description Default <code>nmax</code> <p>Maximum number of halos to process.</p> <code>None</code> <code>idxlist</code> <p>List of halo indices to process. If not provided, (and nmax not set) all halos are processed.</p> <code>None</code> <code>compute</code> <p>Whether to compute the result immediately or return a dask object to compute later.</p> <code>True</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def evaluate(self, nmax=None, idxlist=None, compute=True):\n    \"\"\"\n    Evaluate the operation.\n\n    Parameters\n    ----------\n    nmax: int | None\n        Maximum number of halos to process.\n    idxlist: np.ndarray | None\n        List of halo indices to process. If not provided, (and nmax not set) all halos are processed.\n    compute: bool\n        Whether to compute the result immediately or return a dask object to compute later.\n\n    Returns\n    -------\n\n    \"\"\"\n    # TODO: figure out return type\n    # final operations: those that can only be at end of chain\n    # intermediate operations: those that can only be prior to end of chain\n    funcdict: dict[str, Any] = dict()\n    funcdict.update(**self.opfuncs)\n    funcdict.update(**self.opfuncs_custom)\n\n    func = ChainOps(*[funcdict[k] for k in self.ops])\n\n    fieldnames = list(self.arrs.keys())\n    if self.inputfields is None:\n        opname = self.ops[0]\n        if opname.startswith(\"custom\"):\n            dfltkwargs = get_kwargs(self.opfuncs_custom[opname])\n            fieldnames = dfltkwargs.get(\"fieldnames\", None)\n            if isinstance(fieldnames, str):\n                fieldnames = [fieldnames]\n            if fieldnames is None:\n                raise ValueError(\n                    \"Either pass fields to grouped(fields=...) \"\n                    \"or specify fieldnames=... in applied func.\"\n                )\n        else:\n            raise ValueError(\n                \"Specify field to operate on in operation or grouped().\"\n            )\n\n    res = map_group_operation(\n        func,\n        self.offsets,\n        self.lengths,\n        self.arrs,\n        fieldnames=fieldnames,\n        nmax=nmax,\n        idxlist=idxlist,\n    )\n    if compute:\n        res = res.compute()\n    return res\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.GroupAwareOperation.half","title":"<code>half()</code>","text":"<p>Half the number of particles in each group member. For testing purposes.</p> <p>Returns:</p> Type Description <code>GroupAwareOperation</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def half(self):\n    \"\"\"\n    Half the number of particles in each group member. For testing purposes.\n\n    Returns\n    -------\n    GroupAwareOperation\n    \"\"\"\n    return self.chain(add_op=\"half\", final=False)\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.GroupAwareOperation.max","title":"<code>max(field=None)</code>","text":"<p>Get the maximum value for each group member.</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def max(self, field=None):\n    \"\"\"\n    Get the maximum value for each group member.\n    \"\"\"\n    if field is not None:\n        if self.inputfields is not None:\n            raise ValueError(\"Cannot change input field anymore.\")\n        self.inputfields = [field]\n    return self.chain(add_op=\"max\", final=True)\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.GroupAwareOperation.min","title":"<code>min(field=None)</code>","text":"<p>Get the minimum value for each group member.</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def min(self, field=None):\n    \"\"\"\n    Get the minimum value for each group member.\n    \"\"\"\n    if field is not None:\n        if self.inputfields is not None:\n            raise ValueError(\"Cannot change input field anymore.\")\n        self.inputfields = [field]\n    return self.chain(add_op=\"min\", final=True)\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.GroupAwareOperation.sum","title":"<code>sum(field=None)</code>","text":"<p>Sum the values for each group member.</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def sum(self, field=None):\n    \"\"\"\n    Sum the values for each group member.\n    \"\"\"\n    if field is not None:\n        if self.inputfields is not None:\n            raise ValueError(\"Cannot change input field anymore.\")\n        self.inputfields = [field]\n    return self.chain(add_op=\"sum\", final=True)\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.compute_haloindex","title":"<code>compute_haloindex(gidx, halocelloffsets, *args, index_unbound=None)</code>","text":"<p>Computes the halo index for each particle with dask.</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def compute_haloindex(gidx, halocelloffsets, *args, index_unbound=None):\n    \"\"\"Computes the halo index for each particle with dask.\"\"\"\n    return da.map_blocks(\n        get_hidx_daskwrap,\n        gidx,\n        halocelloffsets,\n        index_unbound=index_unbound,\n        meta=np.array((), dtype=np.int64),\n    )\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.compute_haloquantity","title":"<code>compute_haloquantity(gidx, halocelloffsets, hvals, *args)</code>","text":"<p>Computes a halo quantity for each particle with dask.</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def compute_haloquantity(gidx, halocelloffsets, hvals, *args):\n    \"\"\"Computes a halo quantity for each particle with dask.\"\"\"\n    units = None\n    if hasattr(hvals, \"units\"):\n        units = hvals.units\n    res = map_blocks(\n        get_haloquantity_daskwrap,\n        gidx,\n        halocelloffsets,\n        hvals,\n        meta=np.array((), dtype=hvals.dtype),\n        output_units=units,\n    )\n    return res\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.compute_localsubhaloindex","title":"<code>compute_localsubhaloindex(gidx, halocelloffsets, shnumber, shcounts, shcellcounts, index_unbound=None)</code>","text":"<p>Compute the local subhalo index for each particle with dask. The local subhalo index is the index of the subhalo within each halo, starting at 0 for the central subhalo.</p> <p>Parameters:</p> Name Type Description Default <code>gidx</code> required <code>halocelloffsets</code> required <code>shnumber</code> required <code>shcounts</code> required <code>shcellcounts</code> required <code>index_unbound</code> <code>None</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def compute_localsubhaloindex(\n    gidx, halocelloffsets, shnumber, shcounts, shcellcounts, index_unbound=None\n) -&gt; da.Array:\n    \"\"\"\n    Compute the local subhalo index for each particle with dask.\n    The local subhalo index is the index of the subhalo within each halo,\n    starting at 0 for the central subhalo.\n\n    Parameters\n    ----------\n    gidx\n    halocelloffsets\n    shnumber\n    shcounts\n    shcellcounts\n    index_unbound\n\n    Returns\n    -------\n\n    \"\"\"\n    res = da.map_blocks(\n        get_local_shidx_daskwrap,\n        gidx,\n        halocelloffsets,\n        shnumber,\n        shcounts,\n        shcellcounts,\n        index_unbound=index_unbound,\n        meta=np.array((), dtype=np.int64),\n    )\n    return res\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.get_hidx","title":"<code>get_hidx(gidx_start, gidx_count, celloffsets, index_unbound=None)</code>","text":"<p>Get halo index of a given cell</p> <p>Parameters:</p> Name Type Description Default <code>gidx_start</code> <p>The first unique integer ID for the first particle</p> required <code>gidx_count</code> <p>The amount of halo indices we are querying after \"gidx_start\"</p> required <code>celloffsets</code> <code>array</code> <p>An array holding the starting cell offset for each halo. Needs to include the offset after the last halo. The required shape is thus (Nhalo+1,).</p> required <code>index_unbound</code> <code>integer</code> <p>The index to use for unbound particles. If None, the maximum integer value of the dtype is used.</p> <code>None</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>@jit(nopython=True)\ndef get_hidx(gidx_start, gidx_count, celloffsets, index_unbound=None):\n    \"\"\"Get halo index of a given cell\n\n    Parameters\n    ----------\n    gidx_start: integer\n        The first unique integer ID for the first particle\n    gidx_count: integer\n        The amount of halo indices we are querying after \"gidx_start\"\n    celloffsets : array\n        An array holding the starting cell offset for each halo. Needs to include the\n        offset after the last halo. The required shape is thus (Nhalo+1,).\n    index_unbound : integer, optional\n        The index to use for unbound particles. If None, the maximum integer value\n        of the dtype is used.\n    \"\"\"\n    dtype = np.int64\n    if index_unbound is None:\n        index_unbound = np.iinfo(dtype).max\n    res = index_unbound * np.ones(gidx_count, dtype=dtype)\n    # find initial celloffset\n    hidx_idx = np.searchsorted(celloffsets, gidx_start, side=\"right\") - 1\n    if hidx_idx + 1 &gt;= celloffsets.shape[0]:\n        # we are done. Already out of scope of lookup =&gt; all unbound gas.\n        return res\n    celloffset = celloffsets[hidx_idx + 1]\n    endid = celloffset - gidx_start\n    startid = 0\n\n    # Now iterate through list.\n    while startid &lt; gidx_count:\n        res[startid:endid] = hidx_idx\n        hidx_idx += 1\n        startid = endid\n        if hidx_idx &gt;= celloffsets.shape[0] - 1:\n            break\n        count = celloffsets[hidx_idx + 1] - celloffsets[hidx_idx]\n        endid = startid + count\n    return res\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.get_localshidx","title":"<code>get_localshidx(gidx_start, gidx_count, celloffsets, shnumber, shcounts, shcellcounts, index_unbound=None)</code>","text":"<p>Get the local subhalo index for each particle. This is the subhalo index within each halo group. Particles belonging to the central galaxies will have index 0, particles belonging to the first satellite will have index 1, etc.</p> <p>Parameters:</p> Name Type Description Default <code>gidx_start</code> <code>int</code> required <code>gidx_count</code> <code>int</code> required <code>celloffsets</code> <code>NDArray[int64]</code> required <code>shnumber</code> required <code>shcounts</code> required <code>shcellcounts</code> required <code>index_unbound</code> <p>The index to use for unbound particles. If None, the maximum integer value of the dtype is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>@jit(nopython=True)\ndef get_localshidx(\n    gidx_start: int,\n    gidx_count: int,\n    celloffsets: NDArray[np.int64],\n    shnumber,\n    shcounts,\n    shcellcounts,\n    index_unbound=None,\n):\n    \"\"\"\n    Get the local subhalo index for each particle. This is the subhalo index within each\n    halo group. Particles belonging to the central galaxies will have index 0, particles\n    belonging to the first satellite will have index 1, etc.\n    Parameters\n    ----------\n    gidx_start\n    gidx_count\n    celloffsets\n    shnumber\n    shcounts\n    shcellcounts\n    index_unbound: integer, optional\n        The index to use for unbound particles. If None, the maximum integer value\n        of the dtype is used.\n\n    Returns\n    -------\n    np.ndarray\n    \"\"\"\n    dtype = np.int32\n    if index_unbound is None:\n        index_unbound = np.iinfo(dtype).max\n    res = index_unbound * np.ones(gidx_count, dtype=dtype)  # fuzz has negative index.\n\n    # find initial Group we are in\n    hidx_start_idx = np.searchsorted(celloffsets, gidx_start, side=\"right\") - 1\n    if hidx_start_idx + 1 &gt;= celloffsets.shape[0]:\n        # we are done. Already out of scope of lookup =&gt; all unbound gas.\n        return res\n    celloffset = celloffsets[hidx_start_idx + 1]\n    endid = celloffset - gidx_start\n    startid = 0\n\n    # find initial subhalo we are in\n    hidx = hidx_start_idx\n    shcumsum = np.zeros(shcounts[hidx] + 1, dtype=np.int64)\n    shcumsum[1:] = np.cumsum(\n        shcellcounts[shnumber[hidx] : shnumber[hidx] + shcounts[hidx]]\n    )  # collect halo's subhalo offsets\n    shcumsum += celloffsets[hidx_start_idx]\n    sidx_start_idx: int = int(np.searchsorted(shcumsum, gidx_start, side=\"right\") - 1)\n    if sidx_start_idx &lt; shcounts[hidx]:\n        endid = shcumsum[sidx_start_idx + 1] - gidx_start\n\n    # Now iterate through list.\n    cont = True\n    while cont and (startid &lt; gidx_count):\n        res[startid:endid] = (\n            sidx_start_idx if sidx_start_idx + 1 &lt; shcumsum.shape[0] else -1\n        )\n        sidx_start_idx += 1\n        if sidx_start_idx &lt; shcounts[hidx_start_idx]:\n            # we prepare to fill the next available subhalo for current halo\n            count = shcumsum[sidx_start_idx + 1] - shcumsum[sidx_start_idx]\n            startid = endid\n        else:\n            # we need to find the next halo to start filling its subhalos\n            dbgcount = 0\n            while dbgcount &lt; 100:  # find next halo with &gt;0 subhalos\n                hidx_start_idx += 1\n                if hidx_start_idx &gt;= shcounts.shape[0]:\n                    cont = False\n                    break\n                if shcounts[hidx_start_idx] &gt; 0:\n                    break\n                dbgcount += 1\n            hidx = hidx_start_idx\n            if hidx_start_idx &gt;= celloffsets.shape[0] - 1:\n                startid = gidx_count\n            else:\n                count = celloffsets[hidx_start_idx + 1] - celloffsets[hidx_start_idx]\n                if hidx &lt; shcounts.shape[0]:\n                    shcumsum = np.zeros(shcounts[hidx] + 1, dtype=np.int64)\n                    shcumsum[1:] = np.cumsum(\n                        shcellcounts[shnumber[hidx] : shnumber[hidx] + shcounts[hidx]]\n                    )\n                    shcumsum += celloffsets[hidx_start_idx]\n                    sidx_start_idx = 0\n                    if sidx_start_idx &lt; shcounts[hidx]:\n                        count = shcumsum[sidx_start_idx + 1] - shcumsum[sidx_start_idx]\n                    startid = celloffsets[hidx_start_idx] - gidx_start\n        endid = startid + count\n    return res\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.get_shcounts_shcells","title":"<code>get_shcounts_shcells(SubhaloGrNr, hlength)</code>","text":"<p>Returns the id of the first subhalo and count of subhalos per halo.</p> <p>Parameters:</p> Name Type Description Default <code>SubhaloGrNr</code> <p>The group identifier that each subhalo belongs to respectively</p> required <code>hlength</code> <p>The number of halos in the snapshot</p> required <p>Returns:</p> Name Type Description <code>shcounts</code> <code>ndarray</code> <p>The number of subhalos per halo</p> <code>shnumber</code> <code>ndarray</code> <p>The index of the first subhalo per halo</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>@jit(nopython=True)\ndef get_shcounts_shcells(SubhaloGrNr, hlength):\n    \"\"\"\n    Returns the id of the first subhalo and count of subhalos per halo.\n\n    Parameters\n    ----------\n    SubhaloGrNr: np.ndarray\n        The group identifier that each subhalo belongs to respectively\n    hlength: int\n        The number of halos in the snapshot\n\n    Returns\n    -------\n    shcounts: np.ndarray\n        The number of subhalos per halo\n    shnumber: np.ndarray\n        The index of the first subhalo per halo\n    \"\"\"\n    shcounts = np.zeros(hlength, dtype=np.int32)  # number of subhalos per halo\n    shnumber = np.zeros(hlength, dtype=np.int32)  # index of first subhalo per halo\n    i = 0\n    hid_old = 0\n    while i &lt; SubhaloGrNr.shape[0]:\n        hid = SubhaloGrNr[i]\n        if hid == hid_old:\n            shcounts[hid] += 1\n        else:\n            shnumber[hid] = i\n            shcounts[hid] += 1\n            hid_old = hid\n        i += 1\n    return shcounts, shnumber\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.map_group_operation","title":"<code>map_group_operation(func, offsets, lengths, arrdict, cpucost_halo=10000.0, nchunks_min=None, chunksize_bytes=None, entry_nbytes_in=4, fieldnames=None, nmax=None, idxlist=None)</code>","text":"<p>Map a function to all halos in a halo catalog.</p> <p>Parameters:</p> Name Type Description Default <code>idxlist</code> <code>ndarray | None</code> <p>Only process the halos with these indices.</p> <code>None</code> <code>nmax</code> <code>int | None</code> <p>Only process the first nmax halos.</p> <code>None</code> <code>func</code> required <code>offsets</code> <p>Offset of each group in the particle catalog.</p> required <code>lengths</code> <p>Number of particles per halo.</p> required <code>arrdict</code> required <code>cpucost_halo</code> <code>10000.0</code> <code>nchunks_min</code> <code>int | None</code> <p>Lower bound on the number of halos per chunk.</p> <code>None</code> <code>chunksize_bytes</code> <code>int | None</code> <code>None</code> <code>entry_nbytes_in</code> <code>int | None</code> <code>4</code> <code>fieldnames</code> <code>list[str] | None</code> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def map_group_operation(\n    func,\n    offsets,\n    lengths,\n    arrdict,\n    cpucost_halo=1e4,\n    nchunks_min: int | None = None,\n    chunksize_bytes: int | None = None,\n    entry_nbytes_in: int | None = 4,\n    fieldnames: list[str] | None = None,\n    nmax: int | None = None,\n    idxlist: np.ndarray | None = None,\n) -&gt; da.Array:\n    \"\"\"\n    Map a function to all halos in a halo catalog.\n    Parameters\n    ----------\n    idxlist: np.ndarray | None\n        Only process the halos with these indices.\n    nmax: int | None\n        Only process the first nmax halos.\n    func\n    offsets: np.ndarray\n        Offset of each group in the particle catalog.\n    lengths: np.ndarray\n        Number of particles per halo.\n    arrdict\n    cpucost_halo\n    nchunks_min: int | None\n        Lower bound on the number of halos per chunk.\n    chunksize_bytes\n    entry_nbytes_in\n    fieldnames\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if isinstance(func, ChainOps):\n        dfltkwargs = func.kwargs\n    else:\n        dfltkwargs = get_kwargs(func)\n    if fieldnames is None:\n        fieldnames = dfltkwargs.get(\"fieldnames\", None)\n    if fieldnames is None:\n        fieldnames = get_args(func)\n    units = dfltkwargs.get(\"units\", None)\n    shape = dfltkwargs.get(\"shape\", None)\n    dtype = dfltkwargs.get(\"dtype\", \"float64\")\n    fill_value = dfltkwargs.get(\"fill_value\", 0)\n\n    if idxlist is not None and nmax is not None:\n        raise ValueError(\"Cannot specify both idxlist and nmax.\")\n\n    lengths_all = lengths\n    offsets_all = offsets\n    if len(lengths) == len(offsets):\n        # the offsets array here is one longer here, holding the total number of particles in the last halo.\n        offsets_all = np.concatenate([offsets_all, [offsets_all[-1] + lengths[-1]]])\n\n    if nmax is not None:\n        lengths = lengths[:nmax]\n        offsets = offsets[:nmax]\n\n    if idxlist is not None:\n        # make sure idxlist is sorted and unique\n        if not np.all(np.diff(idxlist) &gt; 0):\n            raise ValueError(\"idxlist must be sorted and unique.\")\n        # make sure idxlist is within range\n        if np.min(idxlist) &lt; 0 or np.max(idxlist) &gt;= lengths.shape[0]:\n            raise ValueError(\n                \"idxlist elements must be in [%i, %i), but covers range [%i, %i].\"\n                % (0, lengths.shape[0], np.min(idxlist), np.max(idxlist))\n            )\n        offsets = offsets[idxlist]\n        lengths = lengths[idxlist]\n\n    if len(lengths) == len(offsets):\n        # the offsets array here is one longer here, holding the total number of particles in the last halo.\n        offsets = np.concatenate([offsets, [offsets[-1] + lengths[-1]]])\n\n    # shape/units inference\n    infer_shape = shape is None or (isinstance(shape, str) and shape == \"auto\")\n    infer_units = units is None\n    infer = infer_shape or infer_units\n    if infer:\n        # attempt to determine shape.\n        if infer_shape:\n            log.debug(\n                \"No shape specified. Attempting to determine shape of func output.\"\n            )\n        if infer_units:\n            log.debug(\n                \"No units specified. Attempting to determine units of func output.\"\n            )\n        arrs = [arrdict[f][:1].compute() for f in fieldnames]\n        # remove units if present\n        # arrs = [arr.magnitude if hasattr(arr, \"magnitude\") else arr for arr in arrs]\n        # arrs = [arr.magnitude for arr in arrs]\n        dummyres = None\n        try:\n            dummyres = func(*arrs)\n        except Exception as e:  # noqa\n            log.warning(\"Exception during shape/unit inference: %s.\" % str(e))\n        if dummyres is not None:\n            if infer_units and hasattr(dummyres, \"units\"):\n                units = dummyres.units\n            log.debug(\"Shape inference: %s.\" % str(shape))\n        if infer_units and dummyres is None:\n            units_present = any([hasattr(arr, \"units\") for arr in arrs])\n            if units_present:\n                log.warning(\"Exception during unit inference. Assuming no units.\")\n        if dummyres is None and infer_shape:\n            # due to https://github.com/hgrecco/pint/issues/1037 innocent np.array operations on unit scalars can fail.\n            # we can still attempt to infer shape by removing units prior to calling func.\n            arrs = [arr.magnitude if hasattr(arr, \"magnitude\") else arr for arr in arrs]\n            try:\n                dummyres = func(*arrs)\n            except Exception as e:  # noqa\n                # no more logging needed here\n                pass\n        if dummyres is not None and infer_shape:\n            if np.isscalar(dummyres):\n                shape = (1,)\n            else:\n                shape = dummyres.shape\n        if infer_shape and dummyres is None and shape is None:\n            log.warning(\"Exception during shape inference. Using shape (1,).\")\n            shape = ()\n    # unit inference\n\n    # Determine chunkedges automatically\n    # TODO: very messy and inefficient routine. improve some time.\n    # TODO: Set entry_bytes_out\n    nbytes_dtype_out = 4  # TODO: hardcode 4 byte output dtype as estimate for now\n    entry_nbytes_out = nbytes_dtype_out * np.prod(shape)\n\n    # list_chunkedges refers to bounds of index intervals to be processed together\n    # if idxlist is specified, then these indices do not have to refer to group indices.\n    # if idxlist is given, we enforce that particle data is contiguous\n    # by putting each idx from idxlist into its own chunk.\n    # in the future, we should optimize this\n    if idxlist is not None:\n        list_chunkedges = [[idx, idx + 1] for idx in np.arange(len(idxlist))]\n    else:\n        list_chunkedges = map_group_operation_get_chunkedges(\n            lengths,\n            entry_nbytes_in,\n            entry_nbytes_out,\n            cpucost_halo=cpucost_halo,\n            nchunks_min=nchunks_min,\n            chunksize_bytes=chunksize_bytes,\n        )\n\n    minentry = offsets[0]\n    maxentry = offsets[-1]  # the last particle that needs to be processed\n\n    # chunks specify the number of groups in each chunk\n    chunks = [tuple(np.diff(list_chunkedges, axis=1).flatten())]\n    # need to add chunk information for additional output axes if needed\n    new_axis = None\n    if isinstance(shape, tuple) and shape != (1,):\n        chunks += [(s,) for s in shape]\n        new_axis = np.arange(1, len(shape) + 1).tolist()\n\n    # slcoffsets = [offsets[chunkedge[0]] for chunkedge in list_chunkedges]\n    # the actual length of relevant data in each chunk\n    slclengths = [\n        offsets[chunkedge[1]] - offsets[chunkedge[0]] for chunkedge in list_chunkedges\n    ]\n    if idxlist is not None:\n        # the chunk length to be fed into map_blocks\n        tmplist = np.concatenate([idxlist, [len(lengths_all)]])\n        slclengths_map = [\n            offsets_all[tmplist[chunkedge[1]]] - offsets_all[tmplist[chunkedge[0]]]\n            for chunkedge in list_chunkedges\n        ]\n        slcoffsets_map = [\n            offsets_all[tmplist[chunkedge[0]]] for chunkedge in list_chunkedges\n        ]\n        slclengths_map[0] = slcoffsets_map[0]\n        slcoffsets_map[0] = 0\n    else:\n        slclengths_map = slclengths\n\n    slcs = [slice(chunkedge[0], chunkedge[1]) for chunkedge in list_chunkedges]\n    offsets_in_chunks = [offsets[slc] - offsets[slc.start] for slc in slcs]\n    lengths_in_chunks = [lengths[slc] for slc in slcs]\n    d_oic = delayed(offsets_in_chunks)\n    d_hic = delayed(lengths_in_chunks)\n\n    arrs = [arrdict[f][minentry:maxentry] for f in fieldnames]\n    for i, arr in enumerate(arrs):\n        arrchunks = ((tuple(slclengths)),)\n        if len(arr.shape) &gt; 1:\n            arrchunks = arrchunks + (arr.shape[1:],)\n        arrs[i] = arr.rechunk(chunks=arrchunks)\n    arrdims = np.array([len(arr.shape) for arr in arrs])\n\n    assert np.all(arrdims == arrdims[0])  # Cannot handle different input dims for now\n\n    drop_axis: list[int] = []\n    if arrdims[0] &gt; 1:\n        drop_axis = list(np.arange(1, arrdims[0]))\n\n    if dtype is None:\n        raise ValueError(\n            \"dtype must be specified, dask will not be able to automatically determine this here.\"\n        )\n\n    calc = map_blocks(\n        wrap_func_scalar,\n        func,\n        d_oic,\n        d_hic,\n        *arrs,\n        dtype=dtype,\n        chunks=chunks,\n        new_axis=new_axis,\n        drop_axis=drop_axis,\n        func_output_shape=shape,\n        func_output_dtype=dtype,\n        fill_value=fill_value,\n        output_units=units,\n    )\n\n    return calc\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.map_group_operation_get_chunkedges","title":"<code>map_group_operation_get_chunkedges(lengths, entry_nbytes_in, entry_nbytes_out, cpucost_halo=1.0, nchunks_min=None, chunksize_bytes=None)</code>","text":"<p>Compute the chunking of a halo operation.</p> <p>Parameters:</p> Name Type Description Default <code>lengths</code> <p>The number of particles per halo.</p> required <code>entry_nbytes_in</code> required <code>entry_nbytes_out</code> required <code>cpucost_halo</code> <code>1.0</code> <code>nchunks_min</code> <code>None</code> <code>chunksize_bytes</code> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def map_group_operation_get_chunkedges(\n    lengths,\n    entry_nbytes_in,\n    entry_nbytes_out,\n    cpucost_halo=1.0,\n    nchunks_min=None,\n    chunksize_bytes=None,\n):\n    \"\"\"\n    Compute the chunking of a halo operation.\n\n    Parameters\n    ----------\n    lengths: np.ndarray\n        The number of particles per halo.\n    entry_nbytes_in\n    entry_nbytes_out\n    cpucost_halo\n    nchunks_min\n    chunksize_bytes\n\n    Returns\n    -------\n    None\n    \"\"\"\n    cpucost_particle = 1.0  # we only care about ratio, so keep particle cost fixed.\n    cost = cpucost_particle * lengths + cpucost_halo\n    sumcost = cost.cumsum()\n\n    # let's allow a maximal chunksize of 16 times the dask default setting for an individual array [here: multiple]\n    if chunksize_bytes is None:\n        chunksize_bytes = 16 * parse_size(dask.config.get(\"array.chunk-size\"))\n    cost_memory = entry_nbytes_in * lengths + entry_nbytes_out\n\n    if not np.max(cost_memory) &lt; chunksize_bytes:\n        raise ValueError(\n            \"Some halo requires more memory than allowed (%i allowed, %i requested). Consider overriding \"\n            \"chunksize_bytes.\" % (chunksize_bytes, np.max(cost_memory))\n        )\n\n    nchunks = int(np.ceil(np.sum(cost_memory) / chunksize_bytes))\n    nchunks = int(np.ceil(1.3 * nchunks))  # fudge factor\n    if nchunks_min is not None:\n        nchunks = max(nchunks_min, nchunks)\n    targetcost = sumcost[-1] / nchunks  # chunk target cost = total cost / nchunks\n\n    arr = np.diff(sumcost % targetcost)  # find whenever exceeding modulo target cost\n    idx = [0] + list(np.where(arr &lt; 0)[0] + 1)\n    if idx[-1] != sumcost.shape[0]:\n        idx.append(sumcost.shape[0])\n    list_chunkedges = []\n    for i in range(len(idx) - 1):\n        list_chunkedges.append([idx[i], idx[i + 1]])\n\n    list_chunkedges = np.asarray(\n        memorycost_limiter(cost_memory, cost, list_chunkedges, chunksize_bytes)\n    )\n\n    # make sure we did not lose any halos.\n    assert np.all(\n        ~(list_chunkedges.flatten()[2:-1:2] - list_chunkedges.flatten()[1:-1:2]).astype(\n            bool\n        )\n    )\n    return list_chunkedges\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.memorycost_limiter","title":"<code>memorycost_limiter(cost_memory, cost_cpu, list_chunkedges, cost_memory_max)</code>","text":"<p>If a chunk too memory expensive, split into equal cpu expense operations.</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def memorycost_limiter(cost_memory, cost_cpu, list_chunkedges, cost_memory_max):\n    \"\"\"If a chunk too memory expensive, split into equal cpu expense operations.\"\"\"\n    list_chunkedges_new = []\n    for chunkedges in list_chunkedges:\n        slc = slice(*chunkedges)\n        totcost_mem = np.sum(cost_memory[slc])\n        list_chunkedges_new.append(chunkedges)\n        if totcost_mem &gt; cost_memory_max:\n            sumcost = cost_cpu[slc].cumsum()\n            sumcost /= sumcost[-1]\n            idx = slc.start + np.argmin(np.abs(sumcost - 0.5))\n            if idx == chunkedges[0]:\n                idx += 1\n            elif idx == chunkedges[-1]:\n                idx -= 1\n            chunkedges1 = [chunkedges[0], idx]\n            chunkedges2 = [idx, chunkedges[1]]\n            if idx == chunkedges[0] or idx == chunkedges[1]:\n                raise ValueError(\"This should not happen.\")\n            list_chunkedges_new.pop()\n            list_chunkedges_new += memorycost_limiter(\n                cost_memory, cost_cpu, [chunkedges1], cost_memory_max\n            )\n            list_chunkedges_new += memorycost_limiter(\n                cost_memory, cost_cpu, [chunkedges2], cost_memory_max\n            )\n    return list_chunkedges_new\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.dataset.wrap_func_scalar","title":"<code>wrap_func_scalar(func, offsets_in_chunks, lengths_in_chunks, *arrs, block_info=None, block_id=None, func_output_shape=(1,), func_output_dtype='float64', fill_value=0)</code>","text":"<p>Wrapper for applying a function to each halo in the passed chunk.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> required <code>offsets_in_chunks</code> required <code>lengths_in_chunks</code> required <code>arrs</code> <code>()</code> <code>block_info</code> <code>None</code> <code>block_id</code> <code>None</code> <code>func_output_shape</code> <code>(1,)</code> <code>func_output_dtype</code> <code>'float64'</code> <code>fill_value</code> <code>0</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def wrap_func_scalar(\n    func,\n    offsets_in_chunks,\n    lengths_in_chunks,\n    *arrs,\n    block_info=None,\n    block_id=None,\n    func_output_shape=(1,),\n    func_output_dtype=\"float64\",\n    fill_value=0,\n):\n    \"\"\"\n    Wrapper for applying a function to each halo in the passed chunk.\n    Parameters\n    ----------\n    func\n    offsets_in_chunks\n    lengths_in_chunks\n    arrs\n    block_info\n    block_id\n    func_output_shape\n    func_output_dtype\n    fill_value\n\n    Returns\n    -------\n\n    \"\"\"\n    offsets = offsets_in_chunks[block_id[0]]\n    lengths = lengths_in_chunks[block_id[0]]\n\n    res = []\n    for i, length in enumerate(lengths):\n        o = offsets[i]\n        if length == 0:\n            res.append(fill_value * np.ones(func_output_shape, dtype=func_output_dtype))\n            if func_output_shape == (1,):\n                res[-1] = res[-1].item()\n            continue\n        arrchunks = [arr[o : o + length] for arr in arrs]\n        res.append(func(*arrchunks))\n    return np.array(res)\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.extra_fields","title":"<code>extra_fields</code>","text":"<p>convenience file to add common fields whenever applicable</p>"},{"location":"api/moduleindex/#scida.customs.arepo.extra_fields.Temperature","title":"<code>Temperature(arrs, ureg=None, **kwargs)</code>","text":"<p>Compute gas temperature given (ElectronAbundance,InternalEnergy) in [K].</p> Source code in <code>src/scida/customs/arepo/extra_fields.py</code> <pre><code>@fielddefs.register_field(\n    \"Temperature\", [\"ElectronAbundance\", \"InternalEnergy\"], \"PartType0\"\n)\ndef Temperature(arrs, ureg=None, **kwargs):\n    \"\"\"Compute gas temperature given (ElectronAbundance,InternalEnergy) in [K].\"\"\"\n    xh = 0.76\n    gamma = 5.0 / 3.0\n\n    m_p = 1.672622e-24  # proton mass [g]\n    k_B = 1.380650e-16  # boltzmann constant [erg/K]\n\n    UnitEnergy_over_UnitMass = (\n        1e10  # standard unit system (TODO: can obtain from snapshot)\n    )\n    f = UnitEnergy_over_UnitMass\n    if ureg is not None:\n        f = 1.0\n        m_p = m_p * ureg.g\n        k_B = k_B * ureg.erg / ureg.K\n    else:\n        # in this case the arrs cannot have pint units\n        assert not hasattr(arrs[\"ElectronAbundance\"], \"units\")\n        assert not hasattr(arrs[\"InternalEnergy\"], \"units\")\n\n    xe = arrs[\"ElectronAbundance\"]\n    u_internal = arrs[\"InternalEnergy\"]\n\n    mu = 4 / (1 + 3 * xh + 4 * xh * xe) * m_p\n    temp = f * (gamma - 1.0) * u_internal / k_B * mu\n\n    return temp\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.helpers","title":"<code>helpers</code>","text":"<p>Helper functions for arepo snapshots/simulations.</p>"},{"location":"api/moduleindex/#scida.customs.arepo.helpers.grp_type_str","title":"<code>grp_type_str(gtype)</code>","text":"<p>Mapping between common group names and numeric group types.</p> Source code in <code>src/scida/customs/arepo/helpers.py</code> <pre><code>def grp_type_str(gtype):\n    \"\"\"Mapping between common group names and numeric group types.\"\"\"\n    if str(gtype).lower() in [\"group\", \"groups\", \"halo\", \"halos\"]:\n        return \"halo\"\n    if str(gtype).lower() in [\"subgroup\", \"subgroups\", \"subhalo\", \"subhalos\"]:\n        return \"subhalo\"\n    raise ValueError(\"Unknown group type: %s\" % gtype)\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.helpers.part_type_num","title":"<code>part_type_num(ptype)</code>","text":"<p>Mapping between common particle names and numeric particle types.</p> Source code in <code>src/scida/customs/arepo/helpers.py</code> <pre><code>def part_type_num(ptype):\n    \"\"\"Mapping between common particle names and numeric particle types.\"\"\"\n    ptype = str(ptype).replace(\"PartType\", \"\")\n    if ptype.isdigit():\n        return int(ptype)\n\n    if str(ptype).lower() in [\"gas\", \"cells\"]:\n        return 0\n    if str(ptype).lower() in [\"dm\", \"darkmatter\"]:\n        return 1\n    if str(ptype).lower() in [\"dmlowres\"]:\n        return 2  # only zoom simulations, not present in full periodic boxes\n    if str(ptype).lower() in [\"tracer\", \"tracers\", \"tracermc\", \"trmc\"]:\n        return 3\n    if str(ptype).lower() in [\"star\", \"stars\", \"stellar\"]:\n        return 4  # only those with GFM_StellarFormationTime&gt;0\n    if str(ptype).lower() in [\"wind\"]:\n        return 4  # only those with GFM_StellarFormationTime&lt;0\n    if str(ptype).lower() in [\"bh\", \"bhs\", \"blackhole\", \"blackholes\", \"black\"]:\n        return 5\n    if str(ptype).lower() in [\"all\"]:\n        return -1\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.selector","title":"<code>selector</code>","text":"<p>Selector for ArepoSnapshot</p>"},{"location":"api/moduleindex/#scida.customs.arepo.selector.ArepoSelector","title":"<code>ArepoSelector</code>","text":"<p>               Bases: <code>Selector</code></p> <p>Selector for ArepoSnapshot. Can select for haloID, subhaloID, and unbound particles.</p> Source code in <code>src/scida/customs/arepo/selector.py</code> <pre><code>class ArepoSelector(Selector):\n    \"\"\"Selector for ArepoSnapshot.\n    Can select for haloID, subhaloID, and unbound particles.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"\n        Initialize the selector.\n        \"\"\"\n        super().__init__()\n        self.keys = [\"haloID\", \"subhaloID\", \"localSubhaloID\", \"unbound\"]\n\n    def prepare(self, *args, **kwargs) -&gt; None:\n        if all([kwargs.get(k, None) is None for k in self.keys]):\n            return  # no specific selection, thus just return\n        snap: ArepoSnapshot = args[0]\n        halo_id = kwargs.get(\"haloID\", None)\n        subhalo_id = kwargs.get(\"subhaloID\", None)\n        subhalo_id_local: int | None = kwargs.get(\"localSubhaloID\", None)\n        unbound = kwargs.get(\"unbound\", None)\n\n        if halo_id is not None and subhalo_id is not None:\n            raise ValueError(\"Cannot select for haloID and subhaloID at the same time.\")\n\n        if unbound is True and (halo_id is not None or subhalo_id is not None):\n            raise ValueError(\n                \"Cannot select haloID/subhaloID and unbound particles at the same time.\"\n            )\n\n        if subhalo_id_local is not None and subhalo_id is not None:\n            raise ValueError(\n                \"Cannot select for localSubhaloID and subhaloID at the same time.\"\n            )\n\n        if snap.catalog is None:\n            raise ValueError(\"Cannot select for haloID without catalog loaded.\")\n\n        # select for halo\n        if subhalo_id_local is not None:\n            if halo_id is None:\n                raise ValueError(\"Cannot select for localSubhaloID without haloID.\")\n            # compute subhalo_id from subhalo_id_local\n            shid_of_first_sh = snap.data[\"Group\"][\"GroupFirstSub\"]\n            nshs = int(snap.data[\"Group\"][\"GroupNsubs\"][halo_id].compute())\n            if subhalo_id_local &gt;= nshs:\n                raise ValueError(\"localSubhaloID exceeds number of subhalos in halo.\")\n            subhalo_id = shid_of_first_sh[halo_id] + subhalo_id_local\n\n        idx = subhalo_id if subhalo_id is not None else halo_id\n        objtype = \"subhalo\" if subhalo_id is not None else \"halo\"\n        if idx is not None:\n            self.select_group(snap, idx, objtype=objtype)\n        elif unbound is True:\n            self.select_unbound(snap)\n\n    def select_unbound(self, snap):\n        \"\"\"\n        Select unbound particles.\n\n        Parameters\n        ----------\n        snap: ArepoSnapshot\n\n        Returns\n        -------\n        None\n        \"\"\"\n        lengths = self.data_backup[\"Group\"][\"GroupLenType\"][-1, :].compute()\n        offsets = self.data_backup[\"Group\"][\"GroupOffsetsType\"][-1, :].compute()\n        # for unbound gas, we start after the last halo particles\n        offsets = offsets + lengths\n        for p in self.data_backup:\n            splt = p.split(\"PartType\")\n            if len(splt) == 1:\n                for k, v in self.data_backup[p].items():\n                    self.data[p][k] = v\n            else:\n                pnum = int(splt[1])\n                offset = offsets[pnum]\n                if hasattr(offset, \"magnitude\"):  # hack for issue 59\n                    offset = offset.magnitude\n                for k, v in self.data_backup[p].items():\n                    self.data[p][k] = v[offset:-1]\n        snap.data = self.data\n\n    def select_group(self, snap, idx, objtype=\"Group\"):\n        \"\"\"\n        Select particles for given group/subhalo index.\n\n        Parameters\n        ----------\n        snap: ArepoSnapshot\n        idx: int\n        objtype: str\n\n        Returns\n        -------\n        None\n        \"\"\"\n        # TODO: test whether works for multiple groups via idx list\n        objtype = grp_type_str(objtype)\n        if objtype == \"halo\":\n            lengths = self.data_backup[\"Group\"][\"GroupLenType\"][idx, :].compute()\n            offsets = self.data_backup[\"Group\"][\"GroupOffsetsType\"][idx, :].compute()\n        elif objtype == \"subhalo\":\n            lengths = {i: snap.get_subhalolengths(i)[idx] for i in range(6)}\n            offsets = {i: snap.get_subhalooffsets(i)[idx] for i in range(6)}\n        else:\n            raise ValueError(\"Unknown object type: %s\" % objtype)\n\n        for p in self.data_backup:\n            splt = p.split(\"PartType\")\n            if len(splt) == 1:\n                for k, v in self.data_backup[p].items():\n                    self.data[p][k] = v\n            else:\n                pnum = int(splt[1])\n                offset = offsets[pnum]\n                length = lengths[pnum]\n                if hasattr(offset, \"magnitude\"):  # hack for issue 59\n                    offset = offset.magnitude\n                if hasattr(length, \"magnitude\"):\n                    length = length.magnitude\n                for k, v in self.data_backup[p].items():\n                    self.data[p][k] = v[offset : offset + length]\n        snap.data = self.data\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.selector.ArepoSelector.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the selector.</p> Source code in <code>src/scida/customs/arepo/selector.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Initialize the selector.\n    \"\"\"\n    super().__init__()\n    self.keys = [\"haloID\", \"subhaloID\", \"localSubhaloID\", \"unbound\"]\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.selector.ArepoSelector.select_group","title":"<code>select_group(snap, idx, objtype='Group')</code>","text":"<p>Select particles for given group/subhalo index.</p> <p>Parameters:</p> Name Type Description Default <code>snap</code> required <code>idx</code> required <code>objtype</code> <code>'Group'</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/customs/arepo/selector.py</code> <pre><code>def select_group(self, snap, idx, objtype=\"Group\"):\n    \"\"\"\n    Select particles for given group/subhalo index.\n\n    Parameters\n    ----------\n    snap: ArepoSnapshot\n    idx: int\n    objtype: str\n\n    Returns\n    -------\n    None\n    \"\"\"\n    # TODO: test whether works for multiple groups via idx list\n    objtype = grp_type_str(objtype)\n    if objtype == \"halo\":\n        lengths = self.data_backup[\"Group\"][\"GroupLenType\"][idx, :].compute()\n        offsets = self.data_backup[\"Group\"][\"GroupOffsetsType\"][idx, :].compute()\n    elif objtype == \"subhalo\":\n        lengths = {i: snap.get_subhalolengths(i)[idx] for i in range(6)}\n        offsets = {i: snap.get_subhalooffsets(i)[idx] for i in range(6)}\n    else:\n        raise ValueError(\"Unknown object type: %s\" % objtype)\n\n    for p in self.data_backup:\n        splt = p.split(\"PartType\")\n        if len(splt) == 1:\n            for k, v in self.data_backup[p].items():\n                self.data[p][k] = v\n        else:\n            pnum = int(splt[1])\n            offset = offsets[pnum]\n            length = lengths[pnum]\n            if hasattr(offset, \"magnitude\"):  # hack for issue 59\n                offset = offset.magnitude\n            if hasattr(length, \"magnitude\"):\n                length = length.magnitude\n            for k, v in self.data_backup[p].items():\n                self.data[p][k] = v[offset : offset + length]\n    snap.data = self.data\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.selector.ArepoSelector.select_unbound","title":"<code>select_unbound(snap)</code>","text":"<p>Select unbound particles.</p> <p>Parameters:</p> Name Type Description Default <code>snap</code> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/customs/arepo/selector.py</code> <pre><code>def select_unbound(self, snap):\n    \"\"\"\n    Select unbound particles.\n\n    Parameters\n    ----------\n    snap: ArepoSnapshot\n\n    Returns\n    -------\n    None\n    \"\"\"\n    lengths = self.data_backup[\"Group\"][\"GroupLenType\"][-1, :].compute()\n    offsets = self.data_backup[\"Group\"][\"GroupOffsetsType\"][-1, :].compute()\n    # for unbound gas, we start after the last halo particles\n    offsets = offsets + lengths\n    for p in self.data_backup:\n        splt = p.split(\"PartType\")\n        if len(splt) == 1:\n            for k, v in self.data_backup[p].items():\n                self.data[p][k] = v\n        else:\n            pnum = int(splt[1])\n            offset = offsets[pnum]\n            if hasattr(offset, \"magnitude\"):  # hack for issue 59\n                offset = offset.magnitude\n            for k, v in self.data_backup[p].items():\n                self.data[p][k] = v[offset:-1]\n    snap.data = self.data\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.series","title":"<code>series</code>","text":"<p>Contains Series class for Arepo simulations.</p>"},{"location":"api/moduleindex/#scida.customs.arepo.series.ArepoSimulation","title":"<code>ArepoSimulation</code>","text":"<p>               Bases: <code>GadgetStyleSimulation</code></p> <p>A series representing an Arepo simulation.</p> Source code in <code>src/scida/customs/arepo/series.py</code> <pre><code>class ArepoSimulation(GadgetStyleSimulation):\n    \"\"\"A series representing an Arepo simulation.\"\"\"\n\n    def __init__(self, path, lazy=True, async_caching=False, **interface_kwargs):\n        \"\"\"\n        Initialize an ArepoSimulation object.\n\n        Parameters\n        ----------\n        path: str\n            Path to the simulation folder, should contain \"output\" folder.\n        lazy: bool\n            Whether to load data files lazily.\n        interface_kwargs: dict\n            Additional keyword arguments passed to the interface.\n        \"\"\"\n        # choose parent folder as path if we are passed \"output\" dir\n        p = pathlib.Path(path)\n        if p.name == \"output\":\n            path = str(p.parent)\n        prefix_dict = dict(paths=\"snapdir\", gpaths=\"group\")\n        arg_dict = dict(gpaths=\"catalog\")\n        super().__init__(\n            path,\n            prefix_dict=prefix_dict,\n            arg_dict=arg_dict,\n            lazy=lazy,\n            **interface_kwargs,\n        )\n\n    @classmethod\n    def validate_path(cls, path, *args, **kwargs) -&gt; CandidateStatus:\n        \"\"\"\n        Validate a path as a candidate for this simulation class.\n\n        Parameters\n        ----------\n        path: str\n            Path to validate.\n        args: list\n            Additional positional arguments.\n        kwargs:\n            Additional keyword arguments.\n\n        Returns\n        -------\n        CandidateStatus\n            Whether the path is a candidate for this simulation class.\n        \"\"\"\n        valid = CandidateStatus.NO\n        if not os.path.isdir(path):\n            return CandidateStatus.NO\n        fns = os.listdir(path)\n        if \"gizmo_parameters.txt\" in fns:\n            return CandidateStatus.NO\n        sprefixs = [\"snapdir\", \"snapshot\"]\n        opath = path\n        if \"output\" in fns:\n            opath = join(path, \"output\")\n        files = os.listdir(opath)\n        folders = [f for f in files if os.path.isdir(join(opath, f))]\n        if any([f.startswith(k) for f in folders for k in sprefixs]):\n            valid = CandidateStatus.MAYBE\n\n        # actually runs with hdf5 files exist!\n        h5files = [f for f in files if f.endswith(\".hdf5\")]\n        if len(h5files) &gt; 0:\n            # group by prefix\n            prfxs_lst = group_by_common_prefix(h5files)\n            # sort by number of files per prefix\n            prfxs_lst = sorted(prfxs_lst, key=len, reverse=True)\n            # take the longest prefix\n            prfx = prfxs_lst[0]\n            # if we have more than one file for this prefix, we might have a series...\n            if len(prfx) &gt; 1:\n                # ... but in this case all hdf5 files should only have NumFilesPerSnapshot == 1\n                fn = [f for f in h5files if f.startswith(prfx)][0]\n                fpath = join(opath, fn)\n                with h5py.File(fpath, \"r\") as f:\n                    if \"Header\" in f:\n                        if \"NumFilesPerSnapshot\" in f[\"Header\"].attrs:\n                            if f[\"Header\"].attrs[\"NumFilesPerSnapshot\"] == 1:\n                                valid = CandidateStatus.YES\n        return valid\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.series.ArepoSimulation.__init__","title":"<code>__init__(path, lazy=True, async_caching=False, **interface_kwargs)</code>","text":"<p>Initialize an ArepoSimulation object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>Path to the simulation folder, should contain \"output\" folder.</p> required <code>lazy</code> <p>Whether to load data files lazily.</p> <code>True</code> <code>interface_kwargs</code> <p>Additional keyword arguments passed to the interface.</p> <code>{}</code> Source code in <code>src/scida/customs/arepo/series.py</code> <pre><code>def __init__(self, path, lazy=True, async_caching=False, **interface_kwargs):\n    \"\"\"\n    Initialize an ArepoSimulation object.\n\n    Parameters\n    ----------\n    path: str\n        Path to the simulation folder, should contain \"output\" folder.\n    lazy: bool\n        Whether to load data files lazily.\n    interface_kwargs: dict\n        Additional keyword arguments passed to the interface.\n    \"\"\"\n    # choose parent folder as path if we are passed \"output\" dir\n    p = pathlib.Path(path)\n    if p.name == \"output\":\n        path = str(p.parent)\n    prefix_dict = dict(paths=\"snapdir\", gpaths=\"group\")\n    arg_dict = dict(gpaths=\"catalog\")\n    super().__init__(\n        path,\n        prefix_dict=prefix_dict,\n        arg_dict=arg_dict,\n        lazy=lazy,\n        **interface_kwargs,\n    )\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.arepo.series.ArepoSimulation.validate_path","title":"<code>validate_path(path, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Validate a path as a candidate for this simulation class.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>Path to validate.</p> required <code>args</code> <p>Additional positional arguments.</p> <code>()</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CandidateStatus</code> <p>Whether the path is a candidate for this simulation class.</p> Source code in <code>src/scida/customs/arepo/series.py</code> <pre><code>@classmethod\ndef validate_path(cls, path, *args, **kwargs) -&gt; CandidateStatus:\n    \"\"\"\n    Validate a path as a candidate for this simulation class.\n\n    Parameters\n    ----------\n    path: str\n        Path to validate.\n    args: list\n        Additional positional arguments.\n    kwargs:\n        Additional keyword arguments.\n\n    Returns\n    -------\n    CandidateStatus\n        Whether the path is a candidate for this simulation class.\n    \"\"\"\n    valid = CandidateStatus.NO\n    if not os.path.isdir(path):\n        return CandidateStatus.NO\n    fns = os.listdir(path)\n    if \"gizmo_parameters.txt\" in fns:\n        return CandidateStatus.NO\n    sprefixs = [\"snapdir\", \"snapshot\"]\n    opath = path\n    if \"output\" in fns:\n        opath = join(path, \"output\")\n    files = os.listdir(opath)\n    folders = [f for f in files if os.path.isdir(join(opath, f))]\n    if any([f.startswith(k) for f in folders for k in sprefixs]):\n        valid = CandidateStatus.MAYBE\n\n    # actually runs with hdf5 files exist!\n    h5files = [f for f in files if f.endswith(\".hdf5\")]\n    if len(h5files) &gt; 0:\n        # group by prefix\n        prfxs_lst = group_by_common_prefix(h5files)\n        # sort by number of files per prefix\n        prfxs_lst = sorted(prfxs_lst, key=len, reverse=True)\n        # take the longest prefix\n        prfx = prfxs_lst[0]\n        # if we have more than one file for this prefix, we might have a series...\n        if len(prfx) &gt; 1:\n            # ... but in this case all hdf5 files should only have NumFilesPerSnapshot == 1\n            fn = [f for f in h5files if f.startswith(prfx)][0]\n            fpath = join(opath, fn)\n            with h5py.File(fpath, \"r\") as f:\n                if \"Header\" in f:\n                    if \"NumFilesPerSnapshot\" in f[\"Header\"].attrs:\n                        if f[\"Header\"].attrs[\"NumFilesPerSnapshot\"] == 1:\n                            valid = CandidateStatus.YES\n    return valid\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.gadgetstyle","title":"<code>gadgetstyle</code>","text":""},{"location":"api/moduleindex/#scida.customs.gadgetstyle.dataset","title":"<code>dataset</code>","text":"<p>Defines the GadgetStyleSnapshot class, mostly used for deriving subclasses for related codes/simulations.</p>"},{"location":"api/moduleindex/#scida.customs.gadgetstyle.dataset.GadgetStyleSnapshot","title":"<code>GadgetStyleSnapshot</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A dataset representing a Gadget-style snapshot.</p> Source code in <code>src/scida/customs/gadgetstyle/dataset.py</code> <pre><code>class GadgetStyleSnapshot(Dataset):\n    \"\"\"A dataset representing a Gadget-style snapshot.\"\"\"\n\n    def __init__(self, path, chunksize=\"auto\", virtualcache=True, **kwargs) -&gt; None:\n        \"\"\"We define gadget-style snapshots as nbody/hydrodynamical simulation snapshots that follow\n        the common /PartType0, /PartType1 grouping scheme.\"\"\"\n        self.boxsize = np.nan\n        super().__init__(path, chunksize=chunksize, virtualcache=virtualcache, **kwargs)\n\n        defaultattributes = [\"config\", \"header\", \"parameters\"]\n        for k in self._metadata_raw:\n            name = k.strip(\"/\").lower()\n            if name in defaultattributes:\n                self.__dict__[name] = self._metadata_raw[k]\n                if \"BoxSize\" in self.__dict__[name]:\n                    self.boxsize = self.__dict__[name][\"BoxSize\"]\n                elif \"Boxsize\" in self.__dict__[name]:\n                    self.boxsize = self.__dict__[name][\"Boxsize\"]\n\n        sanity_check = kwargs.get(\"sanity_check\", False)\n        key_nparts = \"NumPart_Total\"\n        key_nparts_hw = \"NumPart_Total_HighWord\"\n        if sanity_check and key_nparts in self.header and key_nparts_hw in self.header:\n            nparts = self.header[key_nparts_hw] * 2**32 + self.header[key_nparts]\n            for i, n in enumerate(nparts):\n                pkey = \"PartType%i\" % i\n                if pkey in self.data:\n                    pdata = self.data[pkey]\n                    fkey = next(iter(pdata.keys()))\n                    nparts_loaded = pdata[fkey].shape[0]\n                    if nparts_loaded != n:\n                        raise ValueError(\n                            \"Number of particles in header (%i) does not match number of particles loaded (%i) \"\n                            \"for particle type %i\" % (n, nparts_loaded, i)\n                        )\n\n    @classmethod\n    def _get_fileprefix(cls, path: str | os.PathLike, **kwargs) -&gt; str:\n        \"\"\"\n        Get the fileprefix used to identify files belonging to given dataset.\n        Parameters\n        ----------\n        path: str, os.PathLike\n            path to check\n        kwargs\n\n        Returns\n        -------\n        str\n        \"\"\"\n        if os.path.isfile(path):\n            return \"\"  # nothing to do, we have a single file, not a directory\n        # order matters: groups will be taken before fof_subhalo, requires py&gt;3.7 for dict order\n        prfxs = [\"groups\", \"fof_subhalo\", \"snap\"]\n        prfxs_prfx_sim = dict.fromkeys(prfxs)\n        files = sorted(os.listdir(path))\n        prfxs_lst = []\n        for fn in files:\n            s = re.search(r\"^(\\w*)_(\\d*)\", fn)\n            if s is not None:\n                prfxs_lst.append(s.group(1))\n        prfxs_lst = [p for s in prfxs_prfx_sim for p in prfxs_lst if p.startswith(s)]\n        prfxs = dict.fromkeys(prfxs_lst)\n        prfxs = list(prfxs.keys())\n        if len(prfxs) &gt; 1:\n            log.debug(\"We have more than one prefix avail: %s\" % prfxs)\n        elif len(prfxs) == 0:\n            return \"\"\n        if set(prfxs) == {\"groups\", \"fof_subhalo_tab\"}:\n            return \"groups\"  # \"groups\" over \"fof_subhalo_tab\"\n        return prfxs[0]\n\n    @classmethod\n    def validate_path(\n        cls, path: str | os.PathLike, *args, expect_grp=False, **kwargs\n    ) -&gt; CandidateStatus:\n        \"\"\"\n        Check if path is valid for this interface.\n        Parameters\n        ----------\n        path: str, os.PathLike\n            path to check\n        args\n        kwargs\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        path = str(path)\n        possibly_valid = CandidateStatus.NO\n        iszarr = path.rstrip(\"/\").endswith(\".zarr\")\n        if path.endswith(\".hdf5\") or iszarr:\n            possibly_valid = CandidateStatus.MAYBE\n        if os.path.isdir(path):\n            files = os.listdir(path)\n            sufxs = [f.split(\".\")[-1] for f in files]\n            if not iszarr and len(set(sufxs)) &gt; 1:\n                possibly_valid = CandidateStatus.NO\n            if sufxs[0] == \"hdf5\":\n                possibly_valid = CandidateStatus.MAYBE\n        if possibly_valid != CandidateStatus.NO:\n            metadata_raw = load_metadata(path, **kwargs)\n            # need some silly combination of attributes to be sure\n            if all([k in metadata_raw for k in [\"/Header\"]]):\n                # tng-project.org api snapshots do not have \"NumPart_Total\" in header\n                is_zoom = \"CutoutID\" in metadata_raw[\"/Header\"]\n                headerattrs = [\"NumPart_ThisFile\"]\n                if not is_zoom:\n                    headerattrs.append(\"NumPart_Total\")\n\n                # identifying snapshot or group catalog\n                is_snap = all([k in metadata_raw[\"/Header\"] for k in headerattrs])\n                is_grp = all(\n                    [\n                        k in metadata_raw[\"/Header\"]\n                        for k in [\"Ngroups_ThisFile\", \"Ngroups_Total\"]\n                    ]\n                )\n                if is_grp:\n                    return CandidateStatus.MAYBE\n                if is_snap and not expect_grp:\n                    return CandidateStatus.MAYBE\n        return CandidateStatus.NO\n\n    def register_field(self, parttype, name=None, description=\"\"):\n        \"\"\"\n        Register a field for a given particle type by returning through decorator.\n\n        Parameters\n        ----------\n        parttype: str | list[str] | None\n            Particle type name to register with. If None, register for the base field container.\n        name: str | None\n            Name of the field to register.\n        description: str | None\n            Description of the field to register.\n\n        Returns\n        -------\n        callable\n\n        \"\"\"\n        res = self.data.register_field(parttype, name=name, description=description)\n        return res\n\n    def merge_data(self, secondobj, fieldname_suffix=\"\", root_group: str | None = None):\n        \"\"\"\n        Merge data from other snapshot into self.data.\n\n        Parameters\n        ----------\n        secondobj: GadgetStyleSnapshot\n        fieldname_suffix: str\n        root_group: str | None\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        data = self.data\n        if root_group is not None:\n            if root_group not in data._containers:\n                data.add_container(root_group)\n            data = self.data[root_group]\n        for k in secondobj.data:\n            key = k + fieldname_suffix\n            if key not in data:\n                data[key] = secondobj.data[k]\n            else:\n                log.debug(\"Not overwriting field '%s' during merge_data.\" % key)\n            secondobj.data.fieldrecipes_kwargs[\"snap\"] = self\n\n    def merge_hints(self, secondobj):\n        \"\"\"\n        Merge hints from other snapshot into self.hints.\n\n        Parameters\n        ----------\n        secondobj: GadgetStyleSnapshot\n            Other snapshot to merge hints from.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        # merge hints from snap and catalog\n        for h in secondobj.hints:\n            if h not in self.hints:\n                self.hints[h] = secondobj.hints[h]\n            elif isinstance(self.hints[h], dict):\n                # merge dicts\n                for k in secondobj.hints[h]:\n                    if k not in self.hints[h]:\n                        self.hints[h][k] = secondobj.hints[h][k]\n            else:\n                pass  # nothing to do; we do not overwrite with catalog props\n\n    @classmethod\n    def _clean_metadata_from_raw(cls, rawmetadata):\n        \"\"\"\n        Set metadata from raw metadata.\n        \"\"\"\n        # check whether we inherit from CosmologyMixin\n        cosmological = issubclass(cls, CosmologyMixin)\n        metadata = dict()\n        if \"/Header\" in rawmetadata:\n            header = rawmetadata[\"/Header\"]\n            if cosmological and \"Redshift\" in header:\n                metadata[\"redshift\"] = get_scalar(header[\"Redshift\"])\n                metadata[\"z\"] = metadata[\"redshift\"]\n            if \"BoxSize\" in header:\n                # can be scalar or array\n                metadata[\"boxsize\"] = header[\"BoxSize\"]\n            if \"Time\" in header:\n                metadata[\"time\"] = get_scalar(header[\"Time\"])\n                metadata[\"t\"] = metadata[\"time\"]\n        return metadata\n\n    def _set_metadata(self):\n        \"\"\"\n        Set metadata from header and config.\n        \"\"\"\n        md = self._clean_metadata_from_raw(self._metadata_raw)\n        self.metadata = md\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.gadgetstyle.dataset.GadgetStyleSnapshot.__init__","title":"<code>__init__(path, chunksize='auto', virtualcache=True, **kwargs)</code>","text":"<p>We define gadget-style snapshots as nbody/hydrodynamical simulation snapshots that follow the common /PartType0, /PartType1 grouping scheme.</p> Source code in <code>src/scida/customs/gadgetstyle/dataset.py</code> <pre><code>def __init__(self, path, chunksize=\"auto\", virtualcache=True, **kwargs) -&gt; None:\n    \"\"\"We define gadget-style snapshots as nbody/hydrodynamical simulation snapshots that follow\n    the common /PartType0, /PartType1 grouping scheme.\"\"\"\n    self.boxsize = np.nan\n    super().__init__(path, chunksize=chunksize, virtualcache=virtualcache, **kwargs)\n\n    defaultattributes = [\"config\", \"header\", \"parameters\"]\n    for k in self._metadata_raw:\n        name = k.strip(\"/\").lower()\n        if name in defaultattributes:\n            self.__dict__[name] = self._metadata_raw[k]\n            if \"BoxSize\" in self.__dict__[name]:\n                self.boxsize = self.__dict__[name][\"BoxSize\"]\n            elif \"Boxsize\" in self.__dict__[name]:\n                self.boxsize = self.__dict__[name][\"Boxsize\"]\n\n    sanity_check = kwargs.get(\"sanity_check\", False)\n    key_nparts = \"NumPart_Total\"\n    key_nparts_hw = \"NumPart_Total_HighWord\"\n    if sanity_check and key_nparts in self.header and key_nparts_hw in self.header:\n        nparts = self.header[key_nparts_hw] * 2**32 + self.header[key_nparts]\n        for i, n in enumerate(nparts):\n            pkey = \"PartType%i\" % i\n            if pkey in self.data:\n                pdata = self.data[pkey]\n                fkey = next(iter(pdata.keys()))\n                nparts_loaded = pdata[fkey].shape[0]\n                if nparts_loaded != n:\n                    raise ValueError(\n                        \"Number of particles in header (%i) does not match number of particles loaded (%i) \"\n                        \"for particle type %i\" % (n, nparts_loaded, i)\n                    )\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.gadgetstyle.dataset.GadgetStyleSnapshot.merge_data","title":"<code>merge_data(secondobj, fieldname_suffix='', root_group=None)</code>","text":"<p>Merge data from other snapshot into self.data.</p> <p>Parameters:</p> Name Type Description Default <code>secondobj</code> required <code>fieldname_suffix</code> <code>''</code> <code>root_group</code> <code>str | None</code> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/customs/gadgetstyle/dataset.py</code> <pre><code>def merge_data(self, secondobj, fieldname_suffix=\"\", root_group: str | None = None):\n    \"\"\"\n    Merge data from other snapshot into self.data.\n\n    Parameters\n    ----------\n    secondobj: GadgetStyleSnapshot\n    fieldname_suffix: str\n    root_group: str | None\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    data = self.data\n    if root_group is not None:\n        if root_group not in data._containers:\n            data.add_container(root_group)\n        data = self.data[root_group]\n    for k in secondobj.data:\n        key = k + fieldname_suffix\n        if key not in data:\n            data[key] = secondobj.data[k]\n        else:\n            log.debug(\"Not overwriting field '%s' during merge_data.\" % key)\n        secondobj.data.fieldrecipes_kwargs[\"snap\"] = self\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.gadgetstyle.dataset.GadgetStyleSnapshot.merge_hints","title":"<code>merge_hints(secondobj)</code>","text":"<p>Merge hints from other snapshot into self.hints.</p> <p>Parameters:</p> Name Type Description Default <code>secondobj</code> <p>Other snapshot to merge hints from.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/customs/gadgetstyle/dataset.py</code> <pre><code>def merge_hints(self, secondobj):\n    \"\"\"\n    Merge hints from other snapshot into self.hints.\n\n    Parameters\n    ----------\n    secondobj: GadgetStyleSnapshot\n        Other snapshot to merge hints from.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    # merge hints from snap and catalog\n    for h in secondobj.hints:\n        if h not in self.hints:\n            self.hints[h] = secondobj.hints[h]\n        elif isinstance(self.hints[h], dict):\n            # merge dicts\n            for k in secondobj.hints[h]:\n                if k not in self.hints[h]:\n                    self.hints[h][k] = secondobj.hints[h][k]\n        else:\n            pass  # nothing to do; we do not overwrite with catalog props\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.gadgetstyle.dataset.GadgetStyleSnapshot.register_field","title":"<code>register_field(parttype, name=None, description='')</code>","text":"<p>Register a field for a given particle type by returning through decorator.</p> <p>Parameters:</p> Name Type Description Default <code>parttype</code> <p>Particle type name to register with. If None, register for the base field container.</p> required <code>name</code> <p>Name of the field to register.</p> <code>None</code> <code>description</code> <p>Description of the field to register.</p> <code>''</code> <p>Returns:</p> Type Description <code>callable</code> Source code in <code>src/scida/customs/gadgetstyle/dataset.py</code> <pre><code>def register_field(self, parttype, name=None, description=\"\"):\n    \"\"\"\n    Register a field for a given particle type by returning through decorator.\n\n    Parameters\n    ----------\n    parttype: str | list[str] | None\n        Particle type name to register with. If None, register for the base field container.\n    name: str | None\n        Name of the field to register.\n    description: str | None\n        Description of the field to register.\n\n    Returns\n    -------\n    callable\n\n    \"\"\"\n    res = self.data.register_field(parttype, name=name, description=description)\n    return res\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.gadgetstyle.dataset.GadgetStyleSnapshot.validate_path","title":"<code>validate_path(path, *args, expect_grp=False, **kwargs)</code>  <code>classmethod</code>","text":"<p>Check if path is valid for this interface.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | PathLike</code> <p>path to check</p> required <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> Source code in <code>src/scida/customs/gadgetstyle/dataset.py</code> <pre><code>@classmethod\ndef validate_path(\n    cls, path: str | os.PathLike, *args, expect_grp=False, **kwargs\n) -&gt; CandidateStatus:\n    \"\"\"\n    Check if path is valid for this interface.\n    Parameters\n    ----------\n    path: str, os.PathLike\n        path to check\n    args\n    kwargs\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    path = str(path)\n    possibly_valid = CandidateStatus.NO\n    iszarr = path.rstrip(\"/\").endswith(\".zarr\")\n    if path.endswith(\".hdf5\") or iszarr:\n        possibly_valid = CandidateStatus.MAYBE\n    if os.path.isdir(path):\n        files = os.listdir(path)\n        sufxs = [f.split(\".\")[-1] for f in files]\n        if not iszarr and len(set(sufxs)) &gt; 1:\n            possibly_valid = CandidateStatus.NO\n        if sufxs[0] == \"hdf5\":\n            possibly_valid = CandidateStatus.MAYBE\n    if possibly_valid != CandidateStatus.NO:\n        metadata_raw = load_metadata(path, **kwargs)\n        # need some silly combination of attributes to be sure\n        if all([k in metadata_raw for k in [\"/Header\"]]):\n            # tng-project.org api snapshots do not have \"NumPart_Total\" in header\n            is_zoom = \"CutoutID\" in metadata_raw[\"/Header\"]\n            headerattrs = [\"NumPart_ThisFile\"]\n            if not is_zoom:\n                headerattrs.append(\"NumPart_Total\")\n\n            # identifying snapshot or group catalog\n            is_snap = all([k in metadata_raw[\"/Header\"] for k in headerattrs])\n            is_grp = all(\n                [\n                    k in metadata_raw[\"/Header\"]\n                    for k in [\"Ngroups_ThisFile\", \"Ngroups_Total\"]\n                ]\n            )\n            if is_grp:\n                return CandidateStatus.MAYBE\n            if is_snap and not expect_grp:\n                return CandidateStatus.MAYBE\n    return CandidateStatus.NO\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.gadgetstyle.series","title":"<code>series</code>","text":"<p>Defines a series representing a Gadget-style simulation.</p>"},{"location":"api/moduleindex/#scida.customs.gadgetstyle.series.GadgetStyleSimulation","title":"<code>GadgetStyleSimulation</code>","text":"<p>               Bases: <code>DatasetSeries</code></p> <p>A series representing a Gadget-style simulation.</p> Source code in <code>src/scida/customs/gadgetstyle/series.py</code> <pre><code>class GadgetStyleSimulation(DatasetSeries):\n    \"\"\"A series representing a Gadget-style simulation.\"\"\"\n\n    def __init__(\n        self,\n        path,\n        prefix_dict: dict | None = None,\n        subpath_dict: dict | None = None,\n        arg_dict: dict | None = None,\n        lazy=True,\n        **interface_kwargs,\n    ):\n        \"\"\"\n        Initialize a GadgetStyleSimulation object.\n\n        Parameters\n        ----------\n        path: str\n            Path to the simulation folder, should contain \"output\" folder.\n        prefix_dict: dict\n        subpath_dict: dict\n        arg_dict: dict\n        lazy: bool\n        interface_kwargs: dict\n        \"\"\"\n        self.path = path\n        self.name = os.path.basename(path)\n        if prefix_dict is None:\n            prefix_dict = dict()\n        if subpath_dict is None:\n            subpath_dict = dict()\n        if arg_dict is None:\n            arg_dict = dict()\n        p = Path(path)\n        if not (p.exists()):\n            raise ValueError(\"Specified path '%s' does not exist.\" % path)\n        paths_dict = dict()\n        keys = []\n        for d in [prefix_dict, subpath_dict, arg_dict]:\n            keys.extend(list(d.keys()))\n        keys = set(keys)\n        for k in keys:\n            subpath = subpath_dict.get(k, \"output\")\n            sp = p / subpath\n            # by default, we assume that we are given a folder that has an \"output\" subfolder.\n            # this is not always the case - for example for subboxes.\n            # in such case, we attempt to continue with the given path.\n            if not sp.exists():\n                sp = p\n\n            # normally, runs have subfolders for each snapshot...\n            found_prefix = False\n            prefix = _get_snapshotfolder_prefix(sp)\n            prefix = prefix_dict.get(k, prefix)\n            if not sp.exists():\n                if k != \"paths\":\n                    continue  # do not require optional sources\n                raise ValueError(\"Specified path '%s' does not exist.\" % (p / subpath))\n            fns = os.listdir(sp)\n            prfxs = set([f.split(\"_\")[0] for f in fns if f.startswith(prefix)])\n            if len(prfxs) == 0:\n                if k != \"paths\":\n                    continue  # do not require optional sources\n            else:\n                found_prefix = True\n                prfx = prfxs.pop()\n            # ... however, sometimes runs have single-file hdf5 snapshots\n            if not found_prefix:\n                h5files = [f for f in fns if f.endswith(\".hdf5\")]\n                # we only test \"snap\" prefix for now...\n                prfx_tmp = {\"gpaths\": \"group\", \"paths\": \"snap\"}.get(k, None)\n                if prfx_tmp is not None:\n                    files = [\n                        f.split(\"_\")[0] for f in h5files if f.startswith(prfx_tmp + \"_\")\n                    ]\n                    if len(files) &gt; 1:\n                        prfx = prfx_tmp\n                        found_prefix = True\n\n            if not found_prefix:\n                raise ValueError(\n                    \"Could not find any files with prefix '%s' in '%s'.\" % (prefix, sp)\n                )\n\n            paths = sorted([p for p in sp.glob(prfx + \"_*\")])\n            # sometimes there are backup folders with different suffix, exclude those.\n\n            # now sort by snapshot order\n            paths = [\n                p\n                for p in paths\n                if str(p).split(\"_\")[-1].isdigit() or str(p).endswith(\".hdf5\")\n            ]\n            # attempt sorting\n            try:\n                nmbrs = [int(str(p).replace(\".hdf5\", \"\").split(\"_\")[-1]) for p in paths]\n                paths = [p for _, p in sorted(zip(nmbrs, paths))]\n            except:  # noqa\n                pass\n            paths_dict[k] = paths\n\n        # make sure we have the same amount of paths respectively\n        length = None\n        mismatch_length = False\n        for k in paths_dict.keys():\n            paths = paths_dict[k]\n            if length is None:\n                length = len(paths)\n            else:\n                if length != len(paths):\n                    mismatch_length = True\n        if mismatch_length:\n            msg = \"\"\"Mismatch between number of groups and snapshots.\n                     Only loading groups that have a snapshot associated.\"\"\"\n            log.info(msg)\n            # extract ids\n            paths = paths_dict[\"paths\"]\n            ids = [int(str(p).split(\"_\")[-1]) for p in paths]\n            for k in paths_dict.keys():\n                if k == \"paths\":\n                    continue\n                paths = paths_dict[k]\n                paths_dict[k] = [p for p in paths if int(str(p).split(\"_\")[-1]) in ids]\n\n        paths = paths_dict.pop(\"paths\", None)\n        if paths is None:\n            raise ValueError(\"Could not find any snapshot paths.\")\n        p = paths[0]\n        cls = _determine_type(p)[1][0]\n\n        mixins = _determine_mixins(path=p)\n        cls = create_datasetclass_with_mixins(cls, mixins)\n\n        kwargs = {arg_dict.get(k, \"catalog\"): paths_dict[k] for k in paths_dict.keys()}\n        kwargs.update(**interface_kwargs)\n\n        super().__init__(paths, datasetclass=cls, lazy=lazy, **kwargs)\n</code></pre>"},{"location":"api/moduleindex/#scida.customs.gadgetstyle.series.GadgetStyleSimulation.__init__","title":"<code>__init__(path, prefix_dict=None, subpath_dict=None, arg_dict=None, lazy=True, **interface_kwargs)</code>","text":"<p>Initialize a GadgetStyleSimulation object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>Path to the simulation folder, should contain \"output\" folder.</p> required <code>prefix_dict</code> <code>dict | None</code> <code>None</code> <code>subpath_dict</code> <code>dict | None</code> <code>None</code> <code>arg_dict</code> <code>dict | None</code> <code>None</code> <code>lazy</code> <code>True</code> <code>interface_kwargs</code> <code>{}</code> Source code in <code>src/scida/customs/gadgetstyle/series.py</code> <pre><code>def __init__(\n    self,\n    path,\n    prefix_dict: dict | None = None,\n    subpath_dict: dict | None = None,\n    arg_dict: dict | None = None,\n    lazy=True,\n    **interface_kwargs,\n):\n    \"\"\"\n    Initialize a GadgetStyleSimulation object.\n\n    Parameters\n    ----------\n    path: str\n        Path to the simulation folder, should contain \"output\" folder.\n    prefix_dict: dict\n    subpath_dict: dict\n    arg_dict: dict\n    lazy: bool\n    interface_kwargs: dict\n    \"\"\"\n    self.path = path\n    self.name = os.path.basename(path)\n    if prefix_dict is None:\n        prefix_dict = dict()\n    if subpath_dict is None:\n        subpath_dict = dict()\n    if arg_dict is None:\n        arg_dict = dict()\n    p = Path(path)\n    if not (p.exists()):\n        raise ValueError(\"Specified path '%s' does not exist.\" % path)\n    paths_dict = dict()\n    keys = []\n    for d in [prefix_dict, subpath_dict, arg_dict]:\n        keys.extend(list(d.keys()))\n    keys = set(keys)\n    for k in keys:\n        subpath = subpath_dict.get(k, \"output\")\n        sp = p / subpath\n        # by default, we assume that we are given a folder that has an \"output\" subfolder.\n        # this is not always the case - for example for subboxes.\n        # in such case, we attempt to continue with the given path.\n        if not sp.exists():\n            sp = p\n\n        # normally, runs have subfolders for each snapshot...\n        found_prefix = False\n        prefix = _get_snapshotfolder_prefix(sp)\n        prefix = prefix_dict.get(k, prefix)\n        if not sp.exists():\n            if k != \"paths\":\n                continue  # do not require optional sources\n            raise ValueError(\"Specified path '%s' does not exist.\" % (p / subpath))\n        fns = os.listdir(sp)\n        prfxs = set([f.split(\"_\")[0] for f in fns if f.startswith(prefix)])\n        if len(prfxs) == 0:\n            if k != \"paths\":\n                continue  # do not require optional sources\n        else:\n            found_prefix = True\n            prfx = prfxs.pop()\n        # ... however, sometimes runs have single-file hdf5 snapshots\n        if not found_prefix:\n            h5files = [f for f in fns if f.endswith(\".hdf5\")]\n            # we only test \"snap\" prefix for now...\n            prfx_tmp = {\"gpaths\": \"group\", \"paths\": \"snap\"}.get(k, None)\n            if prfx_tmp is not None:\n                files = [\n                    f.split(\"_\")[0] for f in h5files if f.startswith(prfx_tmp + \"_\")\n                ]\n                if len(files) &gt; 1:\n                    prfx = prfx_tmp\n                    found_prefix = True\n\n        if not found_prefix:\n            raise ValueError(\n                \"Could not find any files with prefix '%s' in '%s'.\" % (prefix, sp)\n            )\n\n        paths = sorted([p for p in sp.glob(prfx + \"_*\")])\n        # sometimes there are backup folders with different suffix, exclude those.\n\n        # now sort by snapshot order\n        paths = [\n            p\n            for p in paths\n            if str(p).split(\"_\")[-1].isdigit() or str(p).endswith(\".hdf5\")\n        ]\n        # attempt sorting\n        try:\n            nmbrs = [int(str(p).replace(\".hdf5\", \"\").split(\"_\")[-1]) for p in paths]\n            paths = [p for _, p in sorted(zip(nmbrs, paths))]\n        except:  # noqa\n            pass\n        paths_dict[k] = paths\n\n    # make sure we have the same amount of paths respectively\n    length = None\n    mismatch_length = False\n    for k in paths_dict.keys():\n        paths = paths_dict[k]\n        if length is None:\n            length = len(paths)\n        else:\n            if length != len(paths):\n                mismatch_length = True\n    if mismatch_length:\n        msg = \"\"\"Mismatch between number of groups and snapshots.\n                 Only loading groups that have a snapshot associated.\"\"\"\n        log.info(msg)\n        # extract ids\n        paths = paths_dict[\"paths\"]\n        ids = [int(str(p).split(\"_\")[-1]) for p in paths]\n        for k in paths_dict.keys():\n            if k == \"paths\":\n                continue\n            paths = paths_dict[k]\n            paths_dict[k] = [p for p in paths if int(str(p).split(\"_\")[-1]) in ids]\n\n    paths = paths_dict.pop(\"paths\", None)\n    if paths is None:\n        raise ValueError(\"Could not find any snapshot paths.\")\n    p = paths[0]\n    cls = _determine_type(p)[1][0]\n\n    mixins = _determine_mixins(path=p)\n    cls = create_datasetclass_with_mixins(cls, mixins)\n\n    kwargs = {arg_dict.get(k, \"catalog\"): paths_dict[k] for k in paths_dict.keys()}\n    kwargs.update(**interface_kwargs)\n\n    super().__init__(paths, datasetclass=cls, lazy=lazy, **kwargs)\n</code></pre>"},{"location":"api/moduleindex/#scida.discovertypes","title":"<code>discovertypes</code>","text":"<p>Functionality to determine the dataset or dataseries type of a given path.</p>"},{"location":"api/moduleindex/#scida.discovertypes.CandidateStatus","title":"<code>CandidateStatus</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum to indicate our confidence in a candidate.</p> Source code in <code>src/scida/discovertypes.py</code> <pre><code>class CandidateStatus(Enum):\n    \"\"\"\n    Enum to indicate our confidence in a candidate.\n    \"\"\"\n\n    # TODO: Rethink how tu use MAYBE/YES information.\n    NO = 0  # definitely not a candidate\n    MAYBE = 1  # not sure yet\n    YES = 2  # yes, this is a candidate\n</code></pre>"},{"location":"api/moduleindex/#scida.discovertypes.is_valid_candidate","title":"<code>is_valid_candidate(val)</code>","text":"<p>Map mix of old bool and new Candidate Status enum to bool.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <p>Value to map.</p> required <p>Returns:</p> Type Description <code>CandidateStatus</code> Source code in <code>src/scida/discovertypes.py</code> <pre><code>def is_valid_candidate(val):\n    \"\"\"\n    Map mix of old bool and new Candidate Status enum to bool.\n    Parameters\n    ----------\n    val: bool or CandidateStatus\n        Value to map.\n\n    Returns\n    -------\n    CandidateStatus\n    \"\"\"\n    if isinstance(val, bool):\n        if val:\n            return CandidateStatus.MAYBE\n        else:\n            return CandidateStatus.NO\n    else:\n        return val\n</code></pre>"},{"location":"api/moduleindex/#scida.fields","title":"<code>fields</code>","text":""},{"location":"api/moduleindex/#scida.fields.DerivedFieldRecipe","title":"<code>DerivedFieldRecipe</code>","text":"<p>               Bases: <code>FieldRecipe</code></p> <p>Recipe for a derived field.</p> Source code in <code>src/scida/fields.py</code> <pre><code>class DerivedFieldRecipe(FieldRecipe):\n    \"\"\"\n    Recipe for a derived field.\n    \"\"\"\n\n    def __init__(self, name, func, description=\"\", units=None):\n        \"\"\"See FieldRecipe for parameters.\"\"\"\n        super().__init__(\n            name,\n            func=func,\n            description=description,\n            units=units,\n            ftype=FieldType.DERIVED,\n        )\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.DerivedFieldRecipe.__init__","title":"<code>__init__(name, func, description='', units=None)</code>","text":"<p>See FieldRecipe for parameters.</p> Source code in <code>src/scida/fields.py</code> <pre><code>def __init__(self, name, func, description=\"\", units=None):\n    \"\"\"See FieldRecipe for parameters.\"\"\"\n    super().__init__(\n        name,\n        func=func,\n        description=description,\n        units=units,\n        ftype=FieldType.DERIVED,\n    )\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldContainer","title":"<code>FieldContainer</code>","text":"<p>               Bases: <code>MutableMapping</code></p> <p>A mutable collection of fields. Attempt to construct from derived fields recipes if needed.</p> Source code in <code>src/scida/fields.py</code> <pre><code>class FieldContainer(MutableMapping):\n    \"\"\"A mutable collection of fields. Attempt to construct from derived fields recipes\n    if needed.\"\"\"\n\n    def __init__(\n        self,\n        *args,\n        fieldrecipes_kwargs=None,\n        containers=None,\n        aliases=None,\n        withunits=False,\n        ureg=None,\n        parent: FieldContainer | None = None,\n        name: str | None = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Construct a FieldContainer.\n\n        Parameters\n        ----------\n        args\n        fieldrecipes_kwargs: dict\n            default kwargs used for field recipes\n        containers: list[FieldContainer, str]\n            list of containers to add. FieldContainers in the list will be deep copied.\n            If a list element is a string, a new FieldContainer with the given name will be created.\n        aliases\n        withunits\n        ureg\n        parent: FieldContainer | None\n            parent container\n        kwargs\n        \"\"\"\n        if aliases is None:\n            aliases = {}\n        if fieldrecipes_kwargs is None:\n            fieldrecipes_kwargs = {}\n        self.aliases = aliases\n        self.name = name\n        self._fields: dict[str, da.Array] = {}\n        self._fields.update(*args, **kwargs)\n        self._fieldrecipes: dict[str, FieldRecipe] = {}\n        self._fieldlength = None\n        self.fieldrecipes_kwargs = fieldrecipes_kwargs\n        self.withunits = withunits\n        self._ureg: pint.UnitRegistry | None = ureg\n        self._containers: dict[str, FieldContainer] = (\n            dict()\n        )  # other containers as subgroups\n        if containers is not None:\n            for k in containers:\n                self.add_container(k, deep=True)\n        self.internals = [\"uid\"]  # names of internal fields/groups\n        self.parent = parent\n\n    def set_ureg(self, ureg=None, discover=True):\n        \"\"\"\n        Set the unit registry.\n\n        Parameters\n        ----------\n        ureg: pint.UnitRegistry\n            Unit registry.\n        discover: bool\n            Attempt to discover unit registry from fields.\n\n        Returns\n        -------\n\n        \"\"\"\n        if ureg is None and not discover:\n            raise ValueError(\"Need to specify ureg or set discover=True.\")\n        if ureg is None and discover:\n            keys = self.keys(withgroups=False, withrecipes=False, withinternal=True)\n            for k in keys:\n                if hasattr(self[k], \"units\"):\n                    if isinstance(self[k].units, pint.Unit):\n                        ureg = self[k].units._REGISTRY\n        self._ureg = ureg\n\n    def get_ureg(self, discover=True):\n        \"\"\"\n        Get the unit registry.\n\n        Returns\n        -------\n\n        \"\"\"\n        if self._ureg is None and discover:\n            self.set_ureg(discover=True)\n        return self._ureg\n\n    def copy_skeleton(self) -&gt; FieldContainer:\n        \"\"\"\n        Copy the skeleton of the container (i.e., only the containers, not the fields).\n\n        Returns\n        -------\n        FieldContainer\n        \"\"\"\n        res = FieldContainer()\n        for k, cntr in self._containers.items():\n            res[k] = cntr.copy_skeleton()\n        return res\n\n    def info(self, level=0, name: str | None = None) -&gt; str:\n        \"\"\"\n        Return a string representation of the object.\n\n        Parameters\n        ----------\n        level: int\n            Level in case of nested containers.\n        name:\n            Name of the container.\n\n        Returns\n        -------\n        str\n        \"\"\"\n        rep = \"\"\n        length = self.fieldlength\n        count = self.fieldcount\n        if name is None:\n            name = self.name\n        ncontainers = len(self._containers)\n        statstrs = []\n        if length is not None and length &gt; 0:\n            statstrs.append(\"fields: %i\" % count)\n            statstrs.append(\"entries: %i\" % length)\n        if ncontainers &gt; 0:\n            statstrs.append(\"containers: %i\" % ncontainers)\n        if len(statstrs) &gt; 0:\n            statstr = \", \".join(statstrs)\n            rep += sprint((level + 1) * \"+\", name, \"(%s)\" % statstr)\n        for k in sorted(self._containers.keys()):\n            v = self._containers[k]\n            rep += v.info(level=level + 1)\n        return rep\n\n    def merge(self, collection: FieldContainer, overwrite: bool = True):\n        \"\"\"\n        Merge another FieldContainer into this one.\n\n        Parameters\n        ----------\n        collection: FieldContainer\n            Container to merge.\n        overwrite: bool\n            Overwrite existing fields if true.\n\n        Returns\n        -------\n\n        \"\"\"\n        if not isinstance(collection, FieldContainer):\n            raise TypeError(\"Can only merge FieldContainers.\")\n        # TODO: support nested containers\n        for k in collection._containers:\n            if k not in self._containers:\n                continue\n            if overwrite:\n                c1 = self._containers[k]\n                c2 = collection._containers[k]\n            else:\n                c1 = collection._containers[k]\n                c2 = self._containers[k]\n            c1._fields.update(**c2._fields)\n            c1._fieldrecipes.update(**c2._fieldrecipes)\n        # now do the fields in the root container\n        self._fields.update(**collection._fields)\n        self._fieldrecipes.update(**collection._fieldrecipes)\n\n    @property\n    def fieldcount(self):\n        \"\"\"\n        Return the number of fields.\n\n        Returns\n        -------\n        int\n        \"\"\"\n        rcps = set(self._fieldrecipes)\n        flds = set([k for k in self._fields if k not in self.internals])\n        ntot = len(rcps | flds)\n        return ntot\n\n    @property\n    def fieldlength(self):\n        \"\"\"\n        Try to infer the number of entries for the fields in this container.\n        If all fields have the same length, return this length. Otherwise, return None.\n\n        Returns\n        -------\n        Optional[int]\n        \"\"\"\n        if self._fieldlength is not None:\n            return self._fieldlength\n        fvals = self._fields.values()\n        itr = iter(fvals)\n        if len(fvals) == 0:\n            # can we infer from recipes?\n            if len(self._fieldrecipes) &gt; 0:\n                # get first recipe\n                name = next(iter(self._fieldrecipes.keys()))\n                first = self._getitem(name, evaluate_recipe=True)\n            else:\n                return None\n        else:\n            first = next(itr)\n        if all(first.shape[0] == v.shape[0] for v in self._fields.values()):\n            self._fieldlength = first.shape[0]\n            return self._fieldlength\n        else:\n            return None\n\n    def keys(\n        self,\n        withgroups: bool = True,\n        withrecipes: bool = True,\n        withinternal: bool = False,\n        withfields: bool = True,\n    ):\n        \"\"\"\n        Return a list of keys in the container.\n\n        Parameters\n        ----------\n        withgroups: bool\n            Include sub-containers.\n        withrecipes: bool\n            Include recipes (i.e. not yet instantiated fields).\n        withinternal: bool\n            Include internal fields.\n        withfields: bool\n            Include fields.\n\n        Returns\n        -------\n\n        \"\"\"\n        fieldkeys = []\n        recipekeys: list[str] = []\n        if withfields:\n            fieldkeys = list(self._fields.keys())\n            if not withinternal:\n                for ikey in self.internals:\n                    if ikey in fieldkeys:\n                        fieldkeys.remove(ikey)\n        if withrecipes:\n            recipekeys = self._fieldrecipes.keys()\n        fieldkeys = list(set(fieldkeys) | set(recipekeys))\n        if withgroups:\n            groupkeys = self._containers.keys()\n            fieldkeys = list(set(fieldkeys) | set(groupkeys))\n        return sorted(fieldkeys)\n\n    def items(self, withrecipes=True, withfields=True, evaluate=True):\n        \"\"\"\n        Return a list of tuples for keys/values in the container.\n\n        Parameters\n        ----------\n        withrecipes: bool\n            Whether to include recipes.\n        withfields: bool\n            Whether to include fields.\n        evaluate: bool\n            Whether to evaluate recipes.\n\n        Returns\n        -------\n        list\n\n        \"\"\"\n        return (\n            (k, self._getitem(k, evaluate_recipe=evaluate))\n            for k in self.keys(withrecipes=withrecipes, withfields=withfields)\n        )\n\n    def values(self, evaluate=True):\n        \"\"\"\n        Return fields/recipes the container.\n\n        Parameters\n        ----------\n        evaluate: bool\n            Whether to evaluate recipes.\n\n        Returns\n        -------\n        list\n\n        \"\"\"\n        return (self._getitem(k, evaluate_recipe=evaluate) for k in self.keys())\n\n    def register_field(\n        self,\n        containernames=None,\n        name: str | None = None,\n        description=\"\",\n        units=None,\n    ):\n        \"\"\"\n        Decorator to register a field recipe.\n\n        Parameters\n        ----------\n        containernames: str | list[str] | None\n            Name of the sub-container(s) to register to, or \"all\" for all, or None for self.\n        name: str | None\n            Name of the field. If None, the function name is used.\n        description: str\n            Description of the field.\n        units: pint.Unit | str | None\n            Units of the field.\n\n        Returns\n        -------\n        callable\n\n        \"\"\"\n        # we only construct field upon first call to it (default)\n        # if to_containers, we register to the respective children containers\n        containers = []\n        if isinstance(containernames, list):\n            containers = [self._containers[c] for c in containernames]\n        elif containernames == \"all\":\n            containers = self._containers.values()\n        elif containernames is None:\n            containers = [self]\n        elif isinstance(containernames, str):  # just a single container as a string?\n            containers.append(self._containers[containernames])\n        else:\n            raise ValueError(\"Unknown type.\")\n\n        def decorator(func, name=name, description=description, units=units):\n            \"\"\"\n            Decorator to register a field recipe.\n            \"\"\"\n            if name is None:\n                name = func.__name__\n            for container in containers:\n                drvfields = container._fieldrecipes\n                drvfields[name] = DerivedFieldRecipe(\n                    name, func, description=description, units=units\n                )\n            return func\n\n        return decorator\n\n    def __setitem__(self, key, value):\n        if key in self.aliases:\n            key = self.aliases[key]\n        if isinstance(value, FieldContainer):\n            self._containers[key] = value\n        elif isinstance(value, DerivedFieldRecipe):\n            self._fieldrecipes[key] = value\n        else:\n            self._fields[key] = value\n\n    def __getitem__(self, key):\n        return self._getitem(key)\n\n    def __iter__(self):\n        return iter(self.keys())\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Return a string representation of the object.\n        Returns\n        -------\n        str\n        \"\"\"\n        txt = \"\"\n        txt += \"FieldContainer[containers=%s, fields=%s]\" % (\n            len(self._containers),\n            self.fieldcount,\n        )\n        return txt\n\n    @property\n    def dataframe(self):\n        \"\"\"\n        Return a dask dataframe of the fields in this container.\n\n        Returns\n        -------\n        dd.DataFrame\n\n        \"\"\"\n        return self.get_dataframe()\n\n    def get_dataframe(self, fields=None):\n        \"\"\"\n        Return a dask dataframe of the fields in this container.\n\n        Parameters\n        ----------\n        fields: list[str] | None\n            List of fields to include. If None, include all.\n\n        Returns\n        -------\n        dd.DataFrame\n        \"\"\"\n        dss = {}\n        if fields is None:\n            fields = self.keys()\n        for k in fields:\n            idim = None\n            if k not in self.keys():\n                # could still be an index two 2D dataset\n                i = -1\n                while k[i:].isnumeric():\n                    i += -1\n                i += 1\n                if i == 0:\n                    raise ValueError(\"Field '%s' not found\" % k)\n                idim = int(k[i:])\n                k = k.split(k[i:])[0]\n            v = self[k]\n            assert v.ndim &lt;= 2  # cannot support more than 2 here...\n            if idim is not None:\n                if v.ndim &lt;= 1:\n                    raise ValueError(\"No second dimensional index for %s\" % k)\n                if idim &gt;= v.shape[1]:\n                    raise ValueError(\n                        \"Second dimensional index %i not defined for %s\" % (idim, k)\n                    )\n\n            if v.ndim &gt; 1:\n                for i in range(v.shape[1]):\n                    if idim is None or idim == i:\n                        dss[k + str(i)] = v[:, i]\n            else:\n                dss[k] = v\n        dfs = []\n        for k, v in dss.items():\n            if isinstance(v, pint.Quantity):\n                # pint quantities not supported yet in dd, so remove for now\n                v = v.magnitude\n            dfs.append(dd.from_dask_array(v, columns=[k]))\n        ddf = dd.concat(dfs, axis=1)\n        return ddf\n\n    def add_alias(self, alias, name):\n        \"\"\"\n        Add an alias for a field.\n\n        Parameters\n        ----------\n        alias: str\n            Alias name\n        name: str\n            Field name\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        self.aliases[alias] = name\n\n    def remove_container(self, key):\n        \"\"\"\n        Remove a sub-container.\n\n        Parameters\n        ----------\n        key: str\n            Name of the sub-container.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if key in self._containers:\n            del self._containers[key]\n        else:\n            raise KeyError(\"Unknown container '%s'\" % key)\n\n    def add_container(self, key, deep=False, **kwargs):\n        \"\"\"\n        Add a sub-container.\n\n        Parameters\n        ----------\n        key: str, FieldContainer\n        deep: bool\n            If True, make a deep copy of the container.\n        kwargs: dict\n            keyword arguments for the FieldContainer constructor.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if isinstance(key, str):\n            # create a new container with given name\n            tkwargs = dict(**kwargs)\n            if \"name\" not in tkwargs:\n                tkwargs[\"name\"] = key\n            self._containers[key] = FieldContainer(\n                fieldrecipes_kwargs=self.fieldrecipes_kwargs,\n                withunits=self.withunits,\n                ureg=self.get_ureg(),\n                parent=self,\n                **tkwargs,\n            )\n        elif isinstance(key, FieldContainer):\n            # now we do a shallow or deep copy\n            name = kwargs.pop(\"name\", key.name)\n            if deep:\n                self._containers[name] = key.copy()\n            else:\n                self._containers[name] = key\n        else:\n            raise ValueError(\"Unknown type.\")\n\n    def copy(self):\n        \"\"\"\n        Perform a deep (?) copy of the FieldContainer.\n\n        Returns\n        -------\n        FieldContainer\n        \"\"\"\n        instance = self.__class__()\n        instance._fields = self._fields.copy()\n        instance._fieldrecipes = self._fieldrecipes.copy()\n        instance.aliases = self.aliases.copy()\n        instance.fieldrecipes_kwargs = self.fieldrecipes_kwargs.copy()\n        instance.withunits = self.withunits\n        instance._ureg = self._ureg\n        instance.internals = self.internals.copy()\n        instance.parent = self.parent\n        for k, v in self._containers.items():\n            instance.add_container(v.copy(), deep=True, name=k)\n\n        return instance\n\n    def _getitem(\n        self, key, force_derived=False, update_dict=True, evaluate_recipe=True\n    ):\n        \"\"\"\n        Get an item from the container.\n\n        Parameters\n        ----------\n        key: str\n        force_derived: bool\n            Use the derived field description over instantiated fields.\n        update_dict: bool\n            Update the dictionary of instantiated fields.\n        evaluate_recipe: bool\n            Evaluate the recipe.\n\n        Returns\n        -------\n        da.Array\n\n        \"\"\"\n        if key in self.aliases:\n            key = self.aliases[key]\n        if key in self._containers:\n            return self._containers[key]\n        if key in self._fields and not force_derived:\n            return self._fields[key]\n        else:\n            if key in self._fieldrecipes:\n                if not evaluate_recipe:\n                    return self._fieldrecipes[key]\n                field = self._instantiate_field(key)\n                if update_dict:\n                    self._fields[key] = field\n                return field\n            else:\n                raise KeyError(\"Unknown field '%s'\" % key)\n\n    def _instantiate_field(self, key):\n        \"\"\"\n        Instantiate a field from a recipe, i.e. create its dask array.\n\n        Parameters\n        ----------\n        key: str\n            Name of the field.\n\n        Returns\n        -------\n        da.Array\n        \"\"\"\n        func = self._fieldrecipes[key].func\n        units = self._fieldrecipes[key].units\n        accept_kwargs = inspect.getfullargspec(func).varkw is not None\n        func_kwargs = get_kwargs(func)\n        dkwargs = self.fieldrecipes_kwargs\n        ureg = None\n        if \"ureg\" not in dkwargs or dkwargs[\"ureg\"] is None:\n            ureg = self.get_ureg()\n            dkwargs[\"ureg\"] = ureg\n        # first, we overwrite all optional arguments with class instance defaults where func kwarg is None\n        kwargs = {\n            k: dkwargs[k]\n            for k in (\n                set(dkwargs) &amp; set([k for k, v in func_kwargs.items() if v is None])\n            )\n        }\n        # next, we add all optional arguments if func is accepting **kwargs and varname not yet in signature\n        if accept_kwargs:\n            kwargs.update(\n                **{\n                    k: v\n                    for k, v in dkwargs.items()\n                    if k not in inspect.getfullargspec(func).args\n                }\n            )\n        # finally, instantiate field\n        field = func(self, **kwargs)\n        if self.withunits and units is not None:\n            if not hasattr(field, \"units\"):\n                field = field * units\n            else:\n                has_reg1 = hasattr(field.units, \"_REGISTRY\")\n                has_reg2 = hasattr(units, \"_REGISTRY\")\n                has_regs = has_reg1 and has_reg2\n                if has_regs:\n                    if field.units._REGISTRY == units._REGISTRY:\n                        if field.units != units:\n                            # if unit is present, but unit from metadata is unknown,\n                            # we stick with the former\n                            if not (\n                                hasattr(units, \"units\")\n                                and str(units.units) == \"unknown\"\n                            ):\n                                try:\n                                    field = field.to(units)\n                                except pint.errors.DimensionalityError as e:\n                                    raise ValueError(\n                                        \"Field '%s' units '%s' do not match '%s'\"\n                                        % (key, field.units, units)\n                                    ) from e\n                    else:\n                        # this should not happen. TODO: figure out when this happens\n                        log.warning(\n                            \"Unit registries of field '%s' do not match. container registry.\"\n                            % key\n                        )\n        return field\n\n    def __delitem__(self, key):\n        if key in self._fieldrecipes:\n            del self._fieldrecipes[key]\n        if key in self._containers:\n            del self._containers[key]\n        elif key in self._fields:\n            del self._fields[key]\n        else:\n            raise KeyError(\"Unknown key '%s'\" % key)\n\n    def __len__(self):\n        return len(self.keys())\n\n    def get(self, key, value=None, allow_derived=True, force_derived=False):\n        \"\"\"\n        Get a field.\n\n        Parameters\n        ----------\n        key: str\n        value: da.Array\n        allow_derived: bool\n            Allow derived fields.\n        force_derived: bool\n            Use the derived field description over instantiated fields.\n\n        Returns\n        -------\n        da.Array\n        \"\"\"\n        if key in self._fieldrecipes and not allow_derived:\n            raise KeyError(\"Field '%s' is derived (allow_derived=False)\" % key)\n        else:\n            try:\n                return self._getitem(\n                    key, force_derived=force_derived, update_dict=False\n                )\n            except KeyError:\n                return value\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.dataframe","title":"<code>dataframe</code>  <code>property</code>","text":"<p>Return a dask dataframe of the fields in this container.</p> <p>Returns:</p> Type Description <code>DataFrame</code>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.fieldcount","title":"<code>fieldcount</code>  <code>property</code>","text":"<p>Return the number of fields.</p> <p>Returns:</p> Type Description <code>int</code>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.fieldlength","title":"<code>fieldlength</code>  <code>property</code>","text":"<p>Try to infer the number of entries for the fields in this container. If all fields have the same length, return this length. Otherwise, return None.</p> <p>Returns:</p> Type Description <code>Optional[int]</code>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.__init__","title":"<code>__init__(*args, fieldrecipes_kwargs=None, containers=None, aliases=None, withunits=False, ureg=None, parent=None, name=None, **kwargs)</code>","text":"<p>Construct a FieldContainer.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>()</code> <code>fieldrecipes_kwargs</code> <p>default kwargs used for field recipes</p> <code>None</code> <code>containers</code> <p>list of containers to add. FieldContainers in the list will be deep copied. If a list element is a string, a new FieldContainer with the given name will be created.</p> <code>None</code> <code>aliases</code> <code>None</code> <code>withunits</code> <code>False</code> <code>ureg</code> <code>None</code> <code>parent</code> <code>FieldContainer | None</code> <p>parent container</p> <code>None</code> <code>kwargs</code> <code>{}</code> Source code in <code>src/scida/fields.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    fieldrecipes_kwargs=None,\n    containers=None,\n    aliases=None,\n    withunits=False,\n    ureg=None,\n    parent: FieldContainer | None = None,\n    name: str | None = None,\n    **kwargs,\n):\n    \"\"\"\n    Construct a FieldContainer.\n\n    Parameters\n    ----------\n    args\n    fieldrecipes_kwargs: dict\n        default kwargs used for field recipes\n    containers: list[FieldContainer, str]\n        list of containers to add. FieldContainers in the list will be deep copied.\n        If a list element is a string, a new FieldContainer with the given name will be created.\n    aliases\n    withunits\n    ureg\n    parent: FieldContainer | None\n        parent container\n    kwargs\n    \"\"\"\n    if aliases is None:\n        aliases = {}\n    if fieldrecipes_kwargs is None:\n        fieldrecipes_kwargs = {}\n    self.aliases = aliases\n    self.name = name\n    self._fields: dict[str, da.Array] = {}\n    self._fields.update(*args, **kwargs)\n    self._fieldrecipes: dict[str, FieldRecipe] = {}\n    self._fieldlength = None\n    self.fieldrecipes_kwargs = fieldrecipes_kwargs\n    self.withunits = withunits\n    self._ureg: pint.UnitRegistry | None = ureg\n    self._containers: dict[str, FieldContainer] = (\n        dict()\n    )  # other containers as subgroups\n    if containers is not None:\n        for k in containers:\n            self.add_container(k, deep=True)\n    self.internals = [\"uid\"]  # names of internal fields/groups\n    self.parent = parent\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the object.</p> <p>Returns:</p> Type Description <code>str</code> Source code in <code>src/scida/fields.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Return a string representation of the object.\n    Returns\n    -------\n    str\n    \"\"\"\n    txt = \"\"\n    txt += \"FieldContainer[containers=%s, fields=%s]\" % (\n        len(self._containers),\n        self.fieldcount,\n    )\n    return txt\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.add_alias","title":"<code>add_alias(alias, name)</code>","text":"<p>Add an alias for a field.</p> <p>Parameters:</p> Name Type Description Default <code>alias</code> <p>Alias name</p> required <code>name</code> <p>Field name</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/fields.py</code> <pre><code>def add_alias(self, alias, name):\n    \"\"\"\n    Add an alias for a field.\n\n    Parameters\n    ----------\n    alias: str\n        Alias name\n    name: str\n        Field name\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    self.aliases[alias] = name\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.add_container","title":"<code>add_container(key, deep=False, **kwargs)</code>","text":"<p>Add a sub-container.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> required <code>deep</code> <p>If True, make a deep copy of the container.</p> <code>False</code> <code>kwargs</code> <p>keyword arguments for the FieldContainer constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/fields.py</code> <pre><code>def add_container(self, key, deep=False, **kwargs):\n    \"\"\"\n    Add a sub-container.\n\n    Parameters\n    ----------\n    key: str, FieldContainer\n    deep: bool\n        If True, make a deep copy of the container.\n    kwargs: dict\n        keyword arguments for the FieldContainer constructor.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if isinstance(key, str):\n        # create a new container with given name\n        tkwargs = dict(**kwargs)\n        if \"name\" not in tkwargs:\n            tkwargs[\"name\"] = key\n        self._containers[key] = FieldContainer(\n            fieldrecipes_kwargs=self.fieldrecipes_kwargs,\n            withunits=self.withunits,\n            ureg=self.get_ureg(),\n            parent=self,\n            **tkwargs,\n        )\n    elif isinstance(key, FieldContainer):\n        # now we do a shallow or deep copy\n        name = kwargs.pop(\"name\", key.name)\n        if deep:\n            self._containers[name] = key.copy()\n        else:\n            self._containers[name] = key\n    else:\n        raise ValueError(\"Unknown type.\")\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.copy","title":"<code>copy()</code>","text":"<p>Perform a deep (?) copy of the FieldContainer.</p> <p>Returns:</p> Type Description <code>FieldContainer</code> Source code in <code>src/scida/fields.py</code> <pre><code>def copy(self):\n    \"\"\"\n    Perform a deep (?) copy of the FieldContainer.\n\n    Returns\n    -------\n    FieldContainer\n    \"\"\"\n    instance = self.__class__()\n    instance._fields = self._fields.copy()\n    instance._fieldrecipes = self._fieldrecipes.copy()\n    instance.aliases = self.aliases.copy()\n    instance.fieldrecipes_kwargs = self.fieldrecipes_kwargs.copy()\n    instance.withunits = self.withunits\n    instance._ureg = self._ureg\n    instance.internals = self.internals.copy()\n    instance.parent = self.parent\n    for k, v in self._containers.items():\n        instance.add_container(v.copy(), deep=True, name=k)\n\n    return instance\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.copy_skeleton","title":"<code>copy_skeleton()</code>","text":"<p>Copy the skeleton of the container (i.e., only the containers, not the fields).</p> <p>Returns:</p> Type Description <code>FieldContainer</code> Source code in <code>src/scida/fields.py</code> <pre><code>def copy_skeleton(self) -&gt; FieldContainer:\n    \"\"\"\n    Copy the skeleton of the container (i.e., only the containers, not the fields).\n\n    Returns\n    -------\n    FieldContainer\n    \"\"\"\n    res = FieldContainer()\n    for k, cntr in self._containers.items():\n        res[k] = cntr.copy_skeleton()\n    return res\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.get","title":"<code>get(key, value=None, allow_derived=True, force_derived=False)</code>","text":"<p>Get a field.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> required <code>value</code> <code>None</code> <code>allow_derived</code> <p>Allow derived fields.</p> <code>True</code> <code>force_derived</code> <p>Use the derived field description over instantiated fields.</p> <code>False</code> <p>Returns:</p> Type Description <code>Array</code> Source code in <code>src/scida/fields.py</code> <pre><code>def get(self, key, value=None, allow_derived=True, force_derived=False):\n    \"\"\"\n    Get a field.\n\n    Parameters\n    ----------\n    key: str\n    value: da.Array\n    allow_derived: bool\n        Allow derived fields.\n    force_derived: bool\n        Use the derived field description over instantiated fields.\n\n    Returns\n    -------\n    da.Array\n    \"\"\"\n    if key in self._fieldrecipes and not allow_derived:\n        raise KeyError(\"Field '%s' is derived (allow_derived=False)\" % key)\n    else:\n        try:\n            return self._getitem(\n                key, force_derived=force_derived, update_dict=False\n            )\n        except KeyError:\n            return value\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.get_dataframe","title":"<code>get_dataframe(fields=None)</code>","text":"<p>Return a dask dataframe of the fields in this container.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <p>List of fields to include. If None, include all.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> Source code in <code>src/scida/fields.py</code> <pre><code>def get_dataframe(self, fields=None):\n    \"\"\"\n    Return a dask dataframe of the fields in this container.\n\n    Parameters\n    ----------\n    fields: list[str] | None\n        List of fields to include. If None, include all.\n\n    Returns\n    -------\n    dd.DataFrame\n    \"\"\"\n    dss = {}\n    if fields is None:\n        fields = self.keys()\n    for k in fields:\n        idim = None\n        if k not in self.keys():\n            # could still be an index two 2D dataset\n            i = -1\n            while k[i:].isnumeric():\n                i += -1\n            i += 1\n            if i == 0:\n                raise ValueError(\"Field '%s' not found\" % k)\n            idim = int(k[i:])\n            k = k.split(k[i:])[0]\n        v = self[k]\n        assert v.ndim &lt;= 2  # cannot support more than 2 here...\n        if idim is not None:\n            if v.ndim &lt;= 1:\n                raise ValueError(\"No second dimensional index for %s\" % k)\n            if idim &gt;= v.shape[1]:\n                raise ValueError(\n                    \"Second dimensional index %i not defined for %s\" % (idim, k)\n                )\n\n        if v.ndim &gt; 1:\n            for i in range(v.shape[1]):\n                if idim is None or idim == i:\n                    dss[k + str(i)] = v[:, i]\n        else:\n            dss[k] = v\n    dfs = []\n    for k, v in dss.items():\n        if isinstance(v, pint.Quantity):\n            # pint quantities not supported yet in dd, so remove for now\n            v = v.magnitude\n        dfs.append(dd.from_dask_array(v, columns=[k]))\n    ddf = dd.concat(dfs, axis=1)\n    return ddf\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.get_ureg","title":"<code>get_ureg(discover=True)</code>","text":"<p>Get the unit registry.</p> Source code in <code>src/scida/fields.py</code> <pre><code>def get_ureg(self, discover=True):\n    \"\"\"\n    Get the unit registry.\n\n    Returns\n    -------\n\n    \"\"\"\n    if self._ureg is None and discover:\n        self.set_ureg(discover=True)\n    return self._ureg\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.info","title":"<code>info(level=0, name=None)</code>","text":"<p>Return a string representation of the object.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <p>Level in case of nested containers.</p> <code>0</code> <code>name</code> <code>str | None</code> <p>Name of the container.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> Source code in <code>src/scida/fields.py</code> <pre><code>def info(self, level=0, name: str | None = None) -&gt; str:\n    \"\"\"\n    Return a string representation of the object.\n\n    Parameters\n    ----------\n    level: int\n        Level in case of nested containers.\n    name:\n        Name of the container.\n\n    Returns\n    -------\n    str\n    \"\"\"\n    rep = \"\"\n    length = self.fieldlength\n    count = self.fieldcount\n    if name is None:\n        name = self.name\n    ncontainers = len(self._containers)\n    statstrs = []\n    if length is not None and length &gt; 0:\n        statstrs.append(\"fields: %i\" % count)\n        statstrs.append(\"entries: %i\" % length)\n    if ncontainers &gt; 0:\n        statstrs.append(\"containers: %i\" % ncontainers)\n    if len(statstrs) &gt; 0:\n        statstr = \", \".join(statstrs)\n        rep += sprint((level + 1) * \"+\", name, \"(%s)\" % statstr)\n    for k in sorted(self._containers.keys()):\n        v = self._containers[k]\n        rep += v.info(level=level + 1)\n    return rep\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.items","title":"<code>items(withrecipes=True, withfields=True, evaluate=True)</code>","text":"<p>Return a list of tuples for keys/values in the container.</p> <p>Parameters:</p> Name Type Description Default <code>withrecipes</code> <p>Whether to include recipes.</p> <code>True</code> <code>withfields</code> <p>Whether to include fields.</p> <code>True</code> <code>evaluate</code> <p>Whether to evaluate recipes.</p> <code>True</code> <p>Returns:</p> Type Description <code>list</code> Source code in <code>src/scida/fields.py</code> <pre><code>def items(self, withrecipes=True, withfields=True, evaluate=True):\n    \"\"\"\n    Return a list of tuples for keys/values in the container.\n\n    Parameters\n    ----------\n    withrecipes: bool\n        Whether to include recipes.\n    withfields: bool\n        Whether to include fields.\n    evaluate: bool\n        Whether to evaluate recipes.\n\n    Returns\n    -------\n    list\n\n    \"\"\"\n    return (\n        (k, self._getitem(k, evaluate_recipe=evaluate))\n        for k in self.keys(withrecipes=withrecipes, withfields=withfields)\n    )\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.keys","title":"<code>keys(withgroups=True, withrecipes=True, withinternal=False, withfields=True)</code>","text":"<p>Return a list of keys in the container.</p> <p>Parameters:</p> Name Type Description Default <code>withgroups</code> <code>bool</code> <p>Include sub-containers.</p> <code>True</code> <code>withrecipes</code> <code>bool</code> <p>Include recipes (i.e. not yet instantiated fields).</p> <code>True</code> <code>withinternal</code> <code>bool</code> <p>Include internal fields.</p> <code>False</code> <code>withfields</code> <code>bool</code> <p>Include fields.</p> <code>True</code> Source code in <code>src/scida/fields.py</code> <pre><code>def keys(\n    self,\n    withgroups: bool = True,\n    withrecipes: bool = True,\n    withinternal: bool = False,\n    withfields: bool = True,\n):\n    \"\"\"\n    Return a list of keys in the container.\n\n    Parameters\n    ----------\n    withgroups: bool\n        Include sub-containers.\n    withrecipes: bool\n        Include recipes (i.e. not yet instantiated fields).\n    withinternal: bool\n        Include internal fields.\n    withfields: bool\n        Include fields.\n\n    Returns\n    -------\n\n    \"\"\"\n    fieldkeys = []\n    recipekeys: list[str] = []\n    if withfields:\n        fieldkeys = list(self._fields.keys())\n        if not withinternal:\n            for ikey in self.internals:\n                if ikey in fieldkeys:\n                    fieldkeys.remove(ikey)\n    if withrecipes:\n        recipekeys = self._fieldrecipes.keys()\n    fieldkeys = list(set(fieldkeys) | set(recipekeys))\n    if withgroups:\n        groupkeys = self._containers.keys()\n        fieldkeys = list(set(fieldkeys) | set(groupkeys))\n    return sorted(fieldkeys)\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.merge","title":"<code>merge(collection, overwrite=True)</code>","text":"<p>Merge another FieldContainer into this one.</p> <p>Parameters:</p> Name Type Description Default <code>collection</code> <code>FieldContainer</code> <p>Container to merge.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite existing fields if true.</p> <code>True</code> Source code in <code>src/scida/fields.py</code> <pre><code>def merge(self, collection: FieldContainer, overwrite: bool = True):\n    \"\"\"\n    Merge another FieldContainer into this one.\n\n    Parameters\n    ----------\n    collection: FieldContainer\n        Container to merge.\n    overwrite: bool\n        Overwrite existing fields if true.\n\n    Returns\n    -------\n\n    \"\"\"\n    if not isinstance(collection, FieldContainer):\n        raise TypeError(\"Can only merge FieldContainers.\")\n    # TODO: support nested containers\n    for k in collection._containers:\n        if k not in self._containers:\n            continue\n        if overwrite:\n            c1 = self._containers[k]\n            c2 = collection._containers[k]\n        else:\n            c1 = collection._containers[k]\n            c2 = self._containers[k]\n        c1._fields.update(**c2._fields)\n        c1._fieldrecipes.update(**c2._fieldrecipes)\n    # now do the fields in the root container\n    self._fields.update(**collection._fields)\n    self._fieldrecipes.update(**collection._fieldrecipes)\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.register_field","title":"<code>register_field(containernames=None, name=None, description='', units=None)</code>","text":"<p>Decorator to register a field recipe.</p> <p>Parameters:</p> Name Type Description Default <code>containernames</code> <p>Name of the sub-container(s) to register to, or \"all\" for all, or None for self.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Name of the field. If None, the function name is used.</p> <code>None</code> <code>description</code> <p>Description of the field.</p> <code>''</code> <code>units</code> <p>Units of the field.</p> <code>None</code> <p>Returns:</p> Type Description <code>callable</code> Source code in <code>src/scida/fields.py</code> <pre><code>def register_field(\n    self,\n    containernames=None,\n    name: str | None = None,\n    description=\"\",\n    units=None,\n):\n    \"\"\"\n    Decorator to register a field recipe.\n\n    Parameters\n    ----------\n    containernames: str | list[str] | None\n        Name of the sub-container(s) to register to, or \"all\" for all, or None for self.\n    name: str | None\n        Name of the field. If None, the function name is used.\n    description: str\n        Description of the field.\n    units: pint.Unit | str | None\n        Units of the field.\n\n    Returns\n    -------\n    callable\n\n    \"\"\"\n    # we only construct field upon first call to it (default)\n    # if to_containers, we register to the respective children containers\n    containers = []\n    if isinstance(containernames, list):\n        containers = [self._containers[c] for c in containernames]\n    elif containernames == \"all\":\n        containers = self._containers.values()\n    elif containernames is None:\n        containers = [self]\n    elif isinstance(containernames, str):  # just a single container as a string?\n        containers.append(self._containers[containernames])\n    else:\n        raise ValueError(\"Unknown type.\")\n\n    def decorator(func, name=name, description=description, units=units):\n        \"\"\"\n        Decorator to register a field recipe.\n        \"\"\"\n        if name is None:\n            name = func.__name__\n        for container in containers:\n            drvfields = container._fieldrecipes\n            drvfields[name] = DerivedFieldRecipe(\n                name, func, description=description, units=units\n            )\n        return func\n\n    return decorator\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.remove_container","title":"<code>remove_container(key)</code>","text":"<p>Remove a sub-container.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>Name of the sub-container.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/fields.py</code> <pre><code>def remove_container(self, key):\n    \"\"\"\n    Remove a sub-container.\n\n    Parameters\n    ----------\n    key: str\n        Name of the sub-container.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if key in self._containers:\n        del self._containers[key]\n    else:\n        raise KeyError(\"Unknown container '%s'\" % key)\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.set_ureg","title":"<code>set_ureg(ureg=None, discover=True)</code>","text":"<p>Set the unit registry.</p> <p>Parameters:</p> Name Type Description Default <code>ureg</code> <p>Unit registry.</p> <code>None</code> <code>discover</code> <p>Attempt to discover unit registry from fields.</p> <code>True</code> Source code in <code>src/scida/fields.py</code> <pre><code>def set_ureg(self, ureg=None, discover=True):\n    \"\"\"\n    Set the unit registry.\n\n    Parameters\n    ----------\n    ureg: pint.UnitRegistry\n        Unit registry.\n    discover: bool\n        Attempt to discover unit registry from fields.\n\n    Returns\n    -------\n\n    \"\"\"\n    if ureg is None and not discover:\n        raise ValueError(\"Need to specify ureg or set discover=True.\")\n    if ureg is None and discover:\n        keys = self.keys(withgroups=False, withrecipes=False, withinternal=True)\n        for k in keys:\n            if hasattr(self[k], \"units\"):\n                if isinstance(self[k].units, pint.Unit):\n                    ureg = self[k].units._REGISTRY\n    self._ureg = ureg\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldContainer.values","title":"<code>values(evaluate=True)</code>","text":"<p>Return fields/recipes the container.</p> <p>Parameters:</p> Name Type Description Default <code>evaluate</code> <p>Whether to evaluate recipes.</p> <code>True</code> <p>Returns:</p> Type Description <code>list</code> Source code in <code>src/scida/fields.py</code> <pre><code>def values(self, evaluate=True):\n    \"\"\"\n    Return fields/recipes the container.\n\n    Parameters\n    ----------\n    evaluate: bool\n        Whether to evaluate recipes.\n\n    Returns\n    -------\n    list\n\n    \"\"\"\n    return (self._getitem(k, evaluate_recipe=evaluate) for k in self.keys())\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldRecipe","title":"<code>FieldRecipe</code>","text":"<p>               Bases: <code>object</code></p> <p>Recipe for a field.</p> Source code in <code>src/scida/fields.py</code> <pre><code>class FieldRecipe(object):\n    \"\"\"\n    Recipe for a field.\n    \"\"\"\n\n    def __init__(\n        self, name, func=None, arr=None, description=\"\", units=None, ftype=FieldType.IO\n    ):\n        \"\"\"\n        Recipes for a field. Either specify a function or an array.\n\n        Parameters\n        ----------\n        name: str\n            Name of the field.\n        func: callable | None\n            Function to construct array of the field.\n        arr: da.Array | None\n            Array to construct the field.\n        description: str\n            Description of the field.\n        units: pint.Unit | str | None\n            Units of the field.\n        ftype: FieldType\n            Type of the field.\n        \"\"\"\n        if func is None and arr is None:\n            raise ValueError(\"Need to specify either func or arr.\")\n        self.type = ftype\n        self.name = name\n        self.description = description\n        self.units = units\n        self.func = func\n        self.arr = arr\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldRecipe.__init__","title":"<code>__init__(name, func=None, arr=None, description='', units=None, ftype=FieldType.IO)</code>","text":"<p>Recipes for a field. Either specify a function or an array.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>Name of the field.</p> required <code>func</code> <p>Function to construct array of the field.</p> <code>None</code> <code>arr</code> <p>Array to construct the field.</p> <code>None</code> <code>description</code> <p>Description of the field.</p> <code>''</code> <code>units</code> <p>Units of the field.</p> <code>None</code> <code>ftype</code> <p>Type of the field.</p> <code>IO</code> Source code in <code>src/scida/fields.py</code> <pre><code>def __init__(\n    self, name, func=None, arr=None, description=\"\", units=None, ftype=FieldType.IO\n):\n    \"\"\"\n    Recipes for a field. Either specify a function or an array.\n\n    Parameters\n    ----------\n    name: str\n        Name of the field.\n    func: callable | None\n        Function to construct array of the field.\n    arr: da.Array | None\n        Array to construct the field.\n    description: str\n        Description of the field.\n    units: pint.Unit | str | None\n        Units of the field.\n    ftype: FieldType\n        Type of the field.\n    \"\"\"\n    if func is None and arr is None:\n        raise ValueError(\"Need to specify either func or arr.\")\n    self.type = ftype\n    self.name = name\n    self.description = description\n    self.units = units\n    self.func = func\n    self.arr = arr\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.FieldType","title":"<code>FieldType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for field types.</p> Source code in <code>src/scida/fields.py</code> <pre><code>class FieldType(Enum):\n    \"\"\"\n    Enum for field types.\n    \"\"\"\n\n    INTERNAL = 1  # for internal use only\n    IO = 2  # from disk\n    DERIVED = 3  # derived from other fields\n</code></pre>"},{"location":"api/moduleindex/#scida.fields.walk_container","title":"<code>walk_container(cntr, path='', handler_field=None, handler_group=None, withrecipes=False)</code>","text":"<p>Recursively walk a container and call handlers on fields and groups.</p> <p>Parameters:</p> Name Type Description Default <code>cntr</code> <p>Container to walk.</p> required <code>path</code> <p>relative path in hierarchy to this container</p> <code>''</code> <code>handler_field</code> <p>Function to call on fields.</p> <code>None</code> <code>handler_group</code> <p>Function to call on subcontainers.</p> <code>None</code> <code>withrecipes</code> <p>Include recipes.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/fields.py</code> <pre><code>def walk_container(\n    cntr, path=\"\", handler_field=None, handler_group=None, withrecipes=False\n):\n    \"\"\"\n    Recursively walk a container and call handlers on fields and groups.\n\n    Parameters\n    ----------\n    cntr: FieldContainer\n        Container to walk.\n    path: str\n        relative path in hierarchy to this container\n    handler_field: callable\n        Function to call on fields.\n    handler_group: callable\n        Function to call on subcontainers.\n    withrecipes: bool\n        Include recipes.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    keykwargs = dict(withgroups=True, withrecipes=withrecipes)\n    for ck in cntr.keys(**keykwargs):\n        # we do not want to instantiate entry from recipe by calling cntr[ck] here\n        entry = cntr[ck]\n        newpath = path + \"/\" + ck\n        if isinstance(entry, FieldContainer):\n            if handler_group is not None:\n                handler_group(entry, newpath)\n            walk_container(\n                entry,\n                newpath,\n                handler_field,\n                handler_group,\n                withrecipes=withrecipes,\n            )\n        else:\n            if handler_field is not None:\n                handler_field(entry, newpath, parent=cntr)\n</code></pre>"},{"location":"api/moduleindex/#scida.helpers_hdf5","title":"<code>helpers_hdf5</code>","text":"<p>Helper functions for hdf5 and zarr file processing.</p>"},{"location":"api/moduleindex/#scida.helpers_hdf5.create_mergedhdf5file","title":"<code>create_mergedhdf5file(fn, files, max_workers=None, virtual=True, groupwise_shape=False, nonvirtual_datasets=None)</code>","text":"<p>Creates a virtual hdf5 file from list of given files. Virtual by default.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <p>file to write to</p> required <code>files</code> <p>files to merge</p> required <code>max_workers</code> <p>parallel workers to process files</p> <code>None</code> <code>virtual</code> <p>whether to create linked (\"virtual\") dataset on disk (otherwise copy)</p> <code>True</code> <code>nonvirtual_datasets</code> <code>list[str] | None</code> <p>list of datasets to copy (not virtual) for more fine-grained control</p> <code>None</code> <code>groupwise_shape</code> <p>whether to require shapes to be the same within a group</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/helpers_hdf5.py</code> <pre><code>def create_mergedhdf5file(\n    fn,\n    files,\n    max_workers=None,\n    virtual=True,\n    groupwise_shape=False,\n    nonvirtual_datasets: list[str] | None = None,\n):\n    \"\"\"\n    Creates a virtual hdf5 file from list of given files. Virtual by default.\n\n    Parameters\n    ----------\n    fn: str\n        file to write to\n    files: list\n        files to merge\n    max_workers: int\n        parallel workers to process files\n    virtual: bool\n        whether to create linked (\"virtual\") dataset on disk (otherwise copy)\n    nonvirtual_datasets: list\n        list of datasets to copy (not virtual) for more fine-grained control\n    groupwise_shape: bool\n        whether to require shapes to be the same within a group\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if nonvirtual_datasets is None:\n        nonvirtual_datasets = []\n    if max_workers is None:\n        # read from config\n        config = get_config()\n        max_workers = config.get(\"nthreads\", 16)\n    # first obtain all datasets and groups\n    trees: list[dict[str, Any]] = [{} for i in range(len(files))]\n\n    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n        result = executor.map(walk_hdf5file, files, trees)\n    result = list(result)\n\n    groups = set([item for r in result for item in r[\"groups\"]])\n    # filter out internal scida metadata groups/datasets from source files\n    groups = {g for g in groups if not g.lstrip(\"/\").startswith(\"_chunks\")}\n    datasets = set([item[0] for r in result for item in r[\"datasets\"]])\n    datasets = {d for d in datasets if not d.lstrip(\"/\").startswith(\"_chunks\")}\n\n    def todct(lst):\n        \"\"\"helper func\"\"\"\n        return {item[0]: (item[1], item[2]) for item in lst[\"datasets\"]}\n\n    dcts = [todct(lst) for lst in result]\n    shapes = OrderedDict((d, {}) for d in datasets)\n\n    def shps(i, k, s):\n        \"\"\"helper func\"\"\"\n        return shapes[k].update({i: s[0]}) if s is not None else None\n\n    [shps(i, k, dct.get(k)) for k in datasets for i, dct in enumerate(dcts)]\n    dtypes = {}\n\n    def dtps(k, s):\n        \"\"\"helper func\"\"\"\n        return dtypes.update({k: s[1]}) if s is not None else None\n\n    [dtps(k, dct.get(k)) for k in datasets for i, dct in enumerate(dcts)]\n\n    # get shapes of the respective chunks\n    chunks = {}\n    for field in sorted(shapes.keys()):\n        chunks[field] = [[k, shapes[field][k][0]] for k in shapes[field]]\n    groupchunks = {}\n\n    # assert that all datasets in a given group have the same chunks.\n    for group in sorted(groups):\n        if group == \"/\":\n            group = \"\"  # this is needed to have consistent levels for 0th level\n        groupfields = [f for f in shapes.keys() if f.startswith(group)]\n        groupfields = [f for f in groupfields if f.count(\"/\") - 1 == group.count(\"/\")]\n        groupfields = sorted(groupfields)\n        if len(groupfields) == 0:\n            continue\n        arr0 = chunks[groupfields[0]]\n        for field in groupfields[1:]:\n            arr = np.array(chunks[field])\n            if groupwise_shape and not np.array_equal(arr0, arr):\n                raise ValueError(\"Requiring same shape (see 'groupwise_shape' flag)\")\n            # then save the chunking information for this group\n            groupchunks[field] = arr0\n\n    # just in case somebody forgot trailing slashes\n    nonvirtual_datasets_stripped = [v.strip(\"/\") for v in nonvirtual_datasets]\n\n    # next fill merger file\n    with h5py.File(fn, \"w\", libver=\"latest\") as hf:\n        # create groups\n        for group in sorted(groups):\n            if group == \"/\":\n                group = \"\"\n            else:\n                hf.create_group(group)\n            groupfields = [\n                field\n                for field in shapes.keys()\n                if field.startswith(group) and field.count(\"/\") - 1 == group.count(\"/\")\n            ]\n            if len(groupfields) == 0:\n                continue\n\n            # create virtual datasets as requested\n            for field in groupfields:\n                nonvirtual_field = field.strip(\"/\") in nonvirtual_datasets_stripped\n                if virtual and not nonvirtual_field:\n                    virtual_concat(field, hf, chunks, shapes, dtypes, files)\n\n            # remaining datasets are created non-virtual\n            groupfields_nonvirtual = list(groupfields)\n            # remove fields already present in hf from fields we copy in the following\n            for field in groupfields:\n                if len(group) &gt; 0:\n                    grp = hf[group]\n                else:\n                    grp = hf\n                if field in grp:\n                    groupfields_nonvirtual.remove(field)\n            for field in groupfields_nonvirtual:\n                totentries = np.array([k[1] for k in chunks[field]]).sum()\n                extrashapes = shapes[field][next(iter(shapes[field]))][1:]\n                newshape = (totentries,) + extrashapes\n                hf.create_dataset(field, shape=newshape, dtype=dtypes[field])\n            counters = {field: 0 for field in groupfields}\n            for k, fl in enumerate(files):\n                with h5py.File(fl) as hf_load:\n                    for field in groupfields_nonvirtual:\n                        n = shapes[field].get(k, [0, 0])[0]\n                        if n == 0:\n                            continue\n                        offset = counters[field]\n                        hf[field][offset : offset + n] = hf_load[field]\n                        counters[field] = offset + n\n\n        # save information regarding chunks\n        grp = hf.create_group(\"_chunks\")\n        for k, v in groupchunks.items():\n            grp.attrs[k] = v\n\n        # write the attributes\n        # find attributes that change across data sets\n        attrs_key_lists = [\n            list(v[\"attrs\"].keys()) for v in result\n        ]  # attribute paths for each file\n        attrspaths_all = set().union(*attrs_key_lists)\n        attrspaths_intersec = set(attrspaths_all).intersection(*attrs_key_lists)\n        attrspath_diff = attrspaths_all.difference(attrspaths_intersec)\n        if attrspaths_all != attrspaths_intersec:\n            # if difference only stems from missing datasets (and their assoc. attrs); thats fine\n            if not attrspath_diff.issubset(datasets):\n                raise NotImplementedError(\n                    \"Some attribute paths not present in each partial data file.\"\n                )\n        # check for common key+values across all files\n        attrs_same: dict[str, Any] = {}\n        attrs_differ: dict[str, Any] = {}\n\n        nfiles = len(files)\n\n        for apath in sorted(attrspaths_all):\n            attrs_same[apath] = {}\n            attrs_differ[apath] = {}\n            attrsnames = set().union(\n                *[\n                    result[i][\"attrs\"][apath]\n                    for i in range(nfiles)\n                    if apath in result[i][\"attrs\"]\n                ]\n            )\n            for k in attrsnames:\n                # we ignore apaths and k existing in some files.\n                attrvallist = [\n                    result[i][\"attrs\"][apath][k]\n                    for i in range(nfiles)\n                    if apath in result[i][\"attrs\"] and k in result[i][\"attrs\"][apath]\n                ]\n                attrval0 = attrvallist[0]\n                if isinstance(attrval0, np.ndarray):\n                    if not (np.all([np.array_equal(attrval0, v) for v in attrvallist])):\n                        log.debug(\"%s: %s has different values.\" % (apath, k))\n                        attrs_differ[apath][k] = np.stack(attrvallist)\n                        continue\n                else:\n                    same = len(set(attrvallist)) == 1\n                    if isinstance(attrval0, np.floating):\n                        # for floats we do not require binary equality\n                        # (we had some incident...)\n                        same = np.allclose(attrval0, attrvallist)\n                    if not same:\n                        log.debug(\"%s: %s has different values.\" % (apath, k))\n                        attrs_differ[apath][k] = np.array(attrvallist)\n                        continue\n                attrs_same[apath][k] = attrval0\n        for apath in attrspaths_all:\n            for k, v in attrs_same.get(apath, {}).items():\n                hf[apath].attrs[k] = v\n            for k, v in attrs_differ.get(apath, {}).items():\n                hf[apath].attrs[k] = v\n</code></pre>"},{"location":"api/moduleindex/#scida.helpers_hdf5.get_dtype","title":"<code>get_dtype(obj)</code>","text":"<p>Get the data type of given h5py.Dataset or zarr.Array object</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>object to get dtype from</p> required <p>Returns:</p> Name Type Description <code>dtype</code> <code>dtype</code> <p>dtype of the object</p> Source code in <code>src/scida/helpers_hdf5.py</code> <pre><code>def get_dtype(obj):\n    \"\"\"\n    Get the data type of given h5py.Dataset or zarr.Array object\n\n    Parameters\n    ----------\n    obj: h5py.Dataset or zarr.Array\n        object to get dtype from\n\n    Returns\n    -------\n    dtype: numpy.dtype\n        dtype of the object\n    \"\"\"\n    if isinstance(obj, h5py.Dataset):\n        try:\n            dtype = obj.dtype\n        except TypeError as e:\n            msg = \"data type '&lt;u6' not understood\"\n            if msg == e.__str__():\n                # MTNG defines 6 byte unsigned integers, which are not supported by h5py\n                # could not figure out how to query type in h5py other than the reporting error.\n                # (any call to .dtype will try to resolve \"&lt;u6\" to a numpy dtype, which fails)\n                # we just handle this as 64 bit unsigned integer\n                dtype = np.uint64\n            else:\n                raise e\n        return dtype\n    elif isinstance(obj, zarr.Array):\n        return obj.dtype\n    else:\n        return None\n</code></pre>"},{"location":"api/moduleindex/#scida.helpers_hdf5.walk_group","title":"<code>walk_group(obj, tree, get_attrs=False, scalar_to_attr=True)</code>","text":"<p>Walks through a h5py.Group or zarr.Group object and fills the tree dictionary with information about the datasets and groups.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>object to walk through</p> required <code>tree</code> <p>dictionary to fill recursively</p> required <code>get_attrs</code> <p>whether to get attributes of each object</p> <code>False</code> <code>scalar_to_attr</code> <p>whether to convert scalar datasets to attributes</p> <code>True</code> Source code in <code>src/scida/helpers_hdf5.py</code> <pre><code>def walk_group(obj, tree, get_attrs=False, scalar_to_attr=True):\n    \"\"\"\n    Walks through a h5py.Group or zarr.Group object and fills the tree dictionary with\n    information about the datasets and groups.\n\n    Parameters\n    ----------\n    obj: h5py.Group or zarr.Group\n        object to walk through\n    tree: dict\n        dictionary to fill recursively\n    get_attrs: bool\n        whether to get attributes of each object\n    scalar_to_attr: bool\n        whether to convert scalar datasets to attributes\n\n    Returns\n    -------\n\n    \"\"\"\n    if len(tree) == 0:\n        tree.update(**dict(attrs={}, groups=[], datasets=[]))\n    if get_attrs and len(obj.attrs) &gt; 0:\n        tree[\"attrs\"][obj.name] = dict(obj.attrs)\n    if isinstance(obj, (h5py.Dataset, zarr.Array)):\n        dtype = get_dtype(obj)\n        tree[\"datasets\"].append([obj.name, obj.shape, dtype])\n        if scalar_to_attr and len(obj.shape) == 0:\n            tree[\"attrs\"][obj.name] = obj[()]\n    elif isinstance(obj, (h5py.Group, zarr.Group)):\n        tree[\"groups\"].append(obj.name)  # continue the walk\n        for o in obj.values():\n            walk_group(o, tree, get_attrs=get_attrs)\n</code></pre>"},{"location":"api/moduleindex/#scida.helpers_hdf5.walk_hdf5file","title":"<code>walk_hdf5file(fn, tree, get_attrs=True)</code>","text":"<p>Walks through a hdf5 file and fills the tree dictionary with information about the datasets and groups.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <p>file path to hdf5 file to walk through</p> required <code>tree</code> <p>dictionary to fill recursively</p> required <code>get_attrs</code> <p>whether to get attributes of each object</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tree</code> <code>dict</code> <p>filled dictionary</p> Source code in <code>src/scida/helpers_hdf5.py</code> <pre><code>def walk_hdf5file(fn, tree, get_attrs=True):\n    \"\"\"\n    Walks through a hdf5 file and fills the tree dictionary with\n    information about the datasets and groups.\n\n    Parameters\n    ----------\n    fn: str\n        file path to hdf5 file to walk through\n    tree: dict\n        dictionary to fill recursively\n    get_attrs: bool\n        whether to get attributes of each object\n\n    Returns\n    -------\n    tree: dict\n        filled dictionary\n    \"\"\"\n    with h5py.File(fn, \"r\") as hf:\n        walk_group(hf, tree, get_attrs=get_attrs)\n    return tree\n</code></pre>"},{"location":"api/moduleindex/#scida.helpers_hdf5.walk_zarrfile","title":"<code>walk_zarrfile(fn, tree, get_attrs=True)</code>","text":"<p>Walks through a zarr file and fills the tree dictionary with information about the datasets and groups.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <p>file path to zarr file to walk through</p> required <code>tree</code> <p>dictionary to fill recursively</p> required <code>get_attrs</code> <p>whether to get attributes of each object</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tree</code> <code>dict</code> <p>filled dictionary</p> Source code in <code>src/scida/helpers_hdf5.py</code> <pre><code>def walk_zarrfile(fn, tree, get_attrs=True):\n    \"\"\"\n    Walks through a zarr file and fills the tree dictionary with\n    information about the datasets and groups.\n\n    Parameters\n    ----------\n    fn: str\n        file path to zarr file to walk through\n    tree: dict\n        dictionary to fill recursively\n    get_attrs: bool\n        whether to get attributes of each object\n\n    Returns\n    -------\n    tree: dict\n        filled dictionary\n    \"\"\"\n    with zarr.open(fn) as z:\n        walk_group(z, tree, get_attrs=get_attrs)\n    return tree\n</code></pre>"},{"location":"api/moduleindex/#scida.helpers_misc","title":"<code>helpers_misc</code>","text":""},{"location":"api/moduleindex/#scida.helpers_misc.RecursiveNamespace","title":"<code>RecursiveNamespace</code>","text":"<p>               Bases: <code>SimpleNamespace</code></p> <p>A SimpleNamespace that can be created recursively from a dict</p> Source code in <code>src/scida/helpers_misc.py</code> <pre><code>class RecursiveNamespace(types.SimpleNamespace):\n    \"\"\"A SimpleNamespace that can be created recursively from a dict\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Create a SimpleNamespace recursively\n        \"\"\"\n        super().__init__(**kwargs)\n        self.__dict__.update({k: self.__elt(v) for k, v in kwargs.items()})\n\n    def __elt(self, elt):\n        \"\"\"\n        Recurse into elt to create leaf namespace objects.\n        \"\"\"\n        if isinstance(elt, dict):\n            return type(self)(**elt)\n        if type(elt) in (list, tuple):\n            return [self.__elt(i) for i in elt]\n        return elt\n</code></pre>"},{"location":"api/moduleindex/#scida.helpers_misc.RecursiveNamespace.__elt","title":"<code>__elt(elt)</code>","text":"<p>Recurse into elt to create leaf namespace objects.</p> Source code in <code>src/scida/helpers_misc.py</code> <pre><code>def __elt(self, elt):\n    \"\"\"\n    Recurse into elt to create leaf namespace objects.\n    \"\"\"\n    if isinstance(elt, dict):\n        return type(self)(**elt)\n    if type(elt) in (list, tuple):\n        return [self.__elt(i) for i in elt]\n    return elt\n</code></pre>"},{"location":"api/moduleindex/#scida.helpers_misc.RecursiveNamespace.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Create a SimpleNamespace recursively</p> Source code in <code>src/scida/helpers_misc.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Create a SimpleNamespace recursively\n    \"\"\"\n    super().__init__(**kwargs)\n    self.__dict__.update({k: self.__elt(v) for k, v in kwargs.items()})\n</code></pre>"},{"location":"api/moduleindex/#scida.helpers_misc.computedecorator","title":"<code>computedecorator(func)</code>","text":"<p>Decorator introducing compute keyword to evalute dask array returns.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <p>Function to decorate</p> required <p>Returns:</p> Type Description <code>callable</code> <p>Decorated function</p> Source code in <code>src/scida/helpers_misc.py</code> <pre><code>def computedecorator(func):\n    \"\"\"\n    Decorator introducing compute keyword to evalute dask array returns.\n\n    Parameters\n    ----------\n    func: callable\n        Function to decorate\n\n    Returns\n    -------\n    callable\n        Decorated function\n    \"\"\"\n\n    def wrapper(*args, compute=False, **kwargs):\n        res = func(*args, **kwargs)\n        if compute:\n            return res.compute()\n        else:\n            return res\n\n    return wrapper\n</code></pre>"},{"location":"api/moduleindex/#scida.helpers_misc.get_args","title":"<code>get_args(func)</code>","text":"<p>Get the positional arguments of a function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> required <p>Returns:</p> Type Description <code>list</code> <p>Positional arguments of the function</p> Source code in <code>src/scida/helpers_misc.py</code> <pre><code>def get_args(func):\n    \"\"\"\n    Get the positional arguments of a function.\n\n    Parameters\n    ----------\n    func: callable\n\n    Returns\n    -------\n    list\n        Positional arguments of the function\n    \"\"\"\n    signature = inspect.signature(func)\n    return [\n        k\n        for k, v in signature.parameters.items()\n        if v.default is inspect.Parameter.empty\n    ]\n</code></pre>"},{"location":"api/moduleindex/#scida.helpers_misc.get_kwargs","title":"<code>get_kwargs(func)</code>","text":"<p>Get the keyword arguments of a function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <p>Function to get keyword arguments from</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Keyword arguments of the function</p> Source code in <code>src/scida/helpers_misc.py</code> <pre><code>def get_kwargs(func):\n    \"\"\"\n    Get the keyword arguments of a function.\n\n    Parameters\n    ----------\n    func: callable\n        Function to get keyword arguments from\n\n    Returns\n    -------\n    dict\n        Keyword arguments of the function\n    \"\"\"\n    signature = inspect.signature(func)\n    return {\n        k: v.default\n        for k, v in signature.parameters.items()\n        if v.default is not inspect.Parameter.empty\n    }\n</code></pre>"},{"location":"api/moduleindex/#scida.helpers_misc.hash_path","title":"<code>hash_path(path)</code>","text":"<p>Hash a path to a fixed length string.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> required <p>Returns:</p> Type Description <code>str</code> Source code in <code>src/scida/helpers_misc.py</code> <pre><code>def hash_path(path):\n    \"\"\"\n    Hash a path to a fixed length string.\n\n    Parameters\n    ----------\n    path: str or pathlib.Path\n\n    Returns\n    -------\n    str\n    \"\"\"\n    sha = hashlib.sha256()\n    sha.update(str(path).strip(\"/ \").encode())\n    return sha.hexdigest()[:16]\n</code></pre>"},{"location":"api/moduleindex/#scida.helpers_misc.make_serializable","title":"<code>make_serializable(v)</code>","text":"<p>Make a value JSON serializable.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <p>Object to make JSON serializable</p> required <p>Returns:</p> Type Description <code>any</code> <p>JSON serializable object</p> Source code in <code>src/scida/helpers_misc.py</code> <pre><code>def make_serializable(v):\n    \"\"\"\n    Make a value JSON serializable.\n\n    Parameters\n    ----------\n    v: any\n        Object to make JSON serializable\n\n    Returns\n    -------\n    any\n        JSON serializable object\n    \"\"\"\n\n    # Attributes need to be JSON serializable. No numpy types allowed.\n    if isinstance(v, np.ndarray):\n        v = v.tolist()\n    if isinstance(v, np.generic):\n        v = v.item()\n    if isinstance(v, bytes):\n        v = v.decode(\"utf-8\")\n    return v\n</code></pre>"},{"location":"api/moduleindex/#scida.helpers_misc.map_blocks","title":"<code>map_blocks(func, *args, name=None, token=None, dtype=None, chunks=None, drop_axis=None, new_axis=None, enforce_ndim=False, meta=None, output_units=None, **kwargs)</code>","text":"<p>map_blocks with units</p> Source code in <code>src/scida/helpers_misc.py</code> <pre><code>def map_blocks(\n    func,\n    *args,\n    name=None,\n    token=None,\n    dtype=None,\n    chunks=None,\n    drop_axis=None,\n    new_axis=None,\n    enforce_ndim=False,\n    meta=None,\n    output_units=None,\n    **kwargs,\n):\n    \"\"\"map_blocks with units\"\"\"\n    da_kwargs = dict(\n        name=name,\n        token=token,\n        dtype=dtype,\n        chunks=chunks,\n        drop_axis=drop_axis,\n        new_axis=new_axis,\n        enforce_ndim=enforce_ndim,\n        meta=meta,\n    )\n    res = da.map_blocks(\n        func,\n        *args,\n        **da_kwargs,\n        **kwargs,\n    )\n    if output_units is not None:\n        if hasattr(res, \"magnitude\"):\n            log.info(\"map_blocks output already has units, overwriting.\")\n            res = res.magnitude * output_units\n        res = res * output_units\n\n    return res\n</code></pre>"},{"location":"api/moduleindex/#scida.helpers_misc.sprint","title":"<code>sprint(*args, end='\\n', **kwargs)</code>","text":"<p>Print to a string.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>Arguments to print</p> <code>()</code> <code>end</code> <p>String to append at the end</p> <code>'\\n'</code> <code>kwargs</code> <p>Keyword arguments to pass to print</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>String to print</p> Source code in <code>src/scida/helpers_misc.py</code> <pre><code>def sprint(*args, end=\"\\n\", **kwargs):\n    \"\"\"\n    Print to a string.\n\n    Parameters\n    ----------\n    args: any\n        Arguments to print\n    end: str\n        String to append at the end\n    kwargs: any\n        Keyword arguments to pass to print\n\n    Returns\n    -------\n    str\n       String to print\n    \"\"\"\n    output = io.StringIO()\n    print(*args, file=output, end=end, **kwargs)\n    contents = output.getvalue()\n    output.close()\n    return contents\n</code></pre>"},{"location":"api/moduleindex/#scida.interface","title":"<code>interface</code>","text":"<p>Base dataset class and its handling.</p>"},{"location":"api/moduleindex/#scida.interface.BaseDataset","title":"<code>BaseDataset</code>","text":"<p>Base class for all datasets.</p> Source code in <code>src/scida/interface.py</code> <pre><code>class BaseDataset(metaclass=MixinMeta):\n    \"\"\"\n    Base class for all datasets.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        chunksize: str | int = \"auto\",\n        virtualcache: bool = True,\n        overwrite_cache: bool = False,\n        fileprefix: str = \"\",\n        hints: dict[str, Any] | None = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize a dataset object.\n\n        Parameters\n        ----------\n        path: str\n            Path to the dataset.\n        chunksize: int or str\n            Chunksize for dask arrays.\n        virtualcache: bool\n            Whether to use virtual caching.\n        overwrite_cache: bool\n            Whether to overwrite existing cache.\n        fileprefix: str\n            Prefix for files to scan for.\n        hints: dict\n            Hints for the dataset.\n        kwargs: dict\n            Additional keyword arguments.\n        \"\"\"\n        super().__init__()\n        self.hints = hints if hints is not None else {}\n        self.path = path\n        self.file = None\n        # need this 'tempfile' reference to keep garbage collection away for the tempfile\n        self.tempfile = None\n        self.location = str(path)\n        self.chunksize = chunksize\n        self.virtualcache = virtualcache\n        self.overwrite_cache = overwrite_cache\n        self.withunits = kwargs.get(\"units\", False)\n\n        # Let's find the data and metadata for the object at 'path'\n        self.metadata: dict[str, Any] = {}\n        self._metadata_raw: dict[str, Any] = {}\n        self.data = FieldContainer(withunits=self.withunits)\n\n        if not os.path.exists(self.path):\n            raise Exception(\"Specified path '%s' does not exist.\" % self.path)\n\n        loadkwargs = dict(\n            overwrite_cache=self.overwrite_cache,\n            fileprefix=fileprefix,\n            virtualcache=virtualcache,\n            nonvirtual_datasets=kwargs.get(\"nonvirtual_datasets\", None),\n            derivedfields_kwargs=dict(snap=self),\n            token=self.__dask_tokenize__(),\n            withunits=self.withunits,\n        )\n        if \"choose_prefix\" in kwargs:\n            loadkwargs[\"choose_prefix\"] = kwargs[\"choose_prefix\"]\n\n        res = scida.io.load(path, **loadkwargs)\n        self.data = res[0]\n        self._metadata_raw = res[1]\n        self.file = res[2]\n        self.tempfile = res[3]\n        self._cached = False\n\n        # any identifying metadata?\n        if \"dsname\" not in self.hints:\n            candidates = check_config_for_dataset(self._metadata_raw, path=self.path)\n            if len(candidates) &gt; 0:\n                dsname = candidates[0]\n                log.debug(\"Dataset is identified as '%s'.\" % dsname)\n                self.hints[\"dsname\"] = dsname\n\n    def _info_custom(self):\n        \"\"\"\n        Custom information to be printed by info() method.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        return None\n\n    def info(self, listfields: bool = False):\n        \"\"\"\n        Print information about the dataset.\n\n        Parameters\n        ----------\n        listfields: bool\n            If True, list all fields in the dataset.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        rep = \"\"\n        rep += \"class: \" + sprint(self.__class__.__name__)\n        props = self._repr_dict()\n        for k, v in props.items():\n            rep += sprint(\"%s: %s\" % (k, v))\n        if self._info_custom() is not None:\n            rep += self._info_custom()\n        rep += sprint(\"=== data ===\")\n        rep += self.data.info(name=\"root\")\n        rep += sprint(\"============\")\n        print(rep)\n\n    def _repr_dict(self) -&gt; dict[str, str]:\n        \"\"\"\n        Return a dictionary of properties to be printed by __repr__ method.\n\n        Returns\n        -------\n        dict\n        \"\"\"\n        props = dict()\n        props[\"source\"] = self.path\n        return props\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Return a string representation of the object.\n\n        Returns\n        -------\n        str\n        \"\"\"\n        props = self._repr_dict()\n        clsname = self.__class__.__name__\n        result = clsname + \"[\"\n        for k, v in props.items():\n            result += \"%s=%s, \" % (k, v)\n        result = result[:-2] + \"]\"\n        return result\n\n    def _repr_pretty_(self, p, cycle):\n        \"\"\"\n        Pretty print representation for IPython.\n        Parameters\n        ----------\n        p\n        cycle\n\n        Returns\n        -------\n        None\n        \"\"\"\n        rpr = self.__repr__()\n        p.text(rpr)\n\n    def __init_subclass__(cls, *args, **kwargs):\n        \"\"\"\n        Register subclasses in the dataset type registry.\n        Parameters\n        ----------\n        args: list\n        kwargs: dict\n\n        Returns\n        -------\n        None\n        \"\"\"\n        super().__init_subclass__(*args, **kwargs)\n        if cls.__name__ == \"Delay\":\n            return  # nothing to register for Delay objects\n        if \"Mixin\" in cls.__name__:\n            return  # do not register classes with Mixins\n        dataset_type_registry[cls.__name__] = cls\n\n    @classmethod\n    @abc.abstractmethod\n    def validate_path(cls, path, *args, **kwargs):\n        \"\"\"\n        Validate whether the given path is a valid path for this dataset.\n        Parameters\n        ----------\n        path\n        args\n        kwargs\n\n        Returns\n        -------\n        bool\n\n        \"\"\"\n        return False\n\n    def __hash__(self) -&gt; int:\n        \"\"\"\n        Hash for Dataset instance to be derived from the file location.\n\n        Returns\n        -------\n        int\n        \"\"\"\n        # deterministic hash; note that hash() on a string is no longer deterministic in python3.\n        hash_value = (\n            int(hashlib.sha256(self.location.encode(\"utf-8\")).hexdigest(), 16) % 10**10\n        )\n        return hash_value\n\n    def __getitem__(self, item):\n        return self.data[item]\n\n    def __dask_tokenize__(self) -&gt; int:\n        \"\"\"\n        Token for dask to be derived -- naively from the file location.\n\n        Returns\n        -------\n        int\n        \"\"\"\n        return self.__hash__()\n\n    def return_data(self) -&gt; FieldContainer:\n        \"\"\"\n        Return the data container.\n\n        Returns\n        -------\n        FieldContainer\n        \"\"\"\n        return self.data\n\n    def save(\n        self,\n        fname: str,\n        fields: (\n            str | dict[str, list[str] | dict[str, da.Array]] | FieldContainer\n        ) = \"all\",\n        overwrite: bool = True,\n        zarr_kwargs: dict[str, Any] | None = None,\n        cast_uints: bool = False,\n        extra_attrs: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Save the dataset to a file using the 'zarr' format.\n        Parameters\n        ----------\n        extra_attrs: dict\n            additional attributes to save in the root group\n        fname: str\n            Filename to save to.\n        fields: str or dict\n            dictionary of dask arrays to save. If equal to 'all', save all fields in current dataset.\n        overwrite\n            overwrite existing file\n        zarr_kwargs\n            optional arguments to pass to zarr\n        cast_uints\n            need to potentially cast uints to ints for some compressions; TODO: clean this up\n\n        Returns\n        -------\n        None\n        \"\"\"\n        # We use zarr, as this way we have support to directly write into the file by the workers\n        # (rather than passing back the data chunk over the scheduler to the interface)\n        # Also, this way we can leverage new features, such as a large variety of compression methods.\n        # cast_uints: if true, we cast uints to ints; needed for some compressions (particularly zfp)\n        if zarr_kwargs is None:\n            zarr_kwargs = {}\n        if overwrite and os.path.exists(fname):\n            if os.path.isdir(fname) and len(os.listdir(fname)) &gt; 0:\n                # check if it is a zarr group/array\n                is_zarr = os.path.exists(os.path.join(fname, \".zgroup\"))\n                is_zarr |= os.path.exists(os.path.join(fname, \".zarray\"))\n                if not is_zarr:\n                    raise ValueError(\n                        f\"Directory '{fname}' exists and is not a zarr group. \"\n                        \"Refusing to overwrite for safety.\"\n                    )\n        store = zarr.DirectoryStore(fname, **zarr_kwargs)\n        root = zarr.group(store, overwrite=overwrite)\n\n        # Metadata\n        defaultattributes = [\"Config\", \"Header\", \"Parameters\"]\n        for dctname in defaultattributes:\n            if dctname.lower() in self.__dict__:\n                grp = root.create_group(dctname)\n                dct = self.__dict__[dctname.lower()]\n                for k, v in dct.items():\n                    v = make_serializable(v)\n                    grp.attrs[k] = v\n        if extra_attrs is not None:\n            for k, v in extra_attrs.items():\n                root.attrs[k] = v\n        # Data\n        tasks = []\n        units_to_save = {}\n        ptypes = self.data.keys()\n        if isinstance(fields, dict):\n            ptypes = fields.keys()\n        elif isinstance(fields, str):\n            if not fields == \"all\":\n                raise ValueError(\"Invalid field specifier.\")\n        else:\n            raise ValueError(\"Invalid type for fields.\")\n        for p in ptypes:\n            root.create_group(p)\n            if fields == \"all\":\n                fieldkeys = self.data[p]\n            else:\n                if isinstance(fields[p], dict):\n                    fieldkeys = fields[p].keys()\n                else:\n                    fieldkeys = fields[p]\n            for k in fieldkeys:\n                if not isinstance(fields, str) and isinstance(fields[p], dict):\n                    arr = fields[p][k]\n                else:\n                    arr = self.data[p][k]\n                if hasattr(arr, \"magnitude\"):  # if we have units, remove those here\n                    units_to_save[(p, k)] = str(arr.units)\n                    arr = arr.magnitude\n                if np.any(np.isnan(arr.shape)):\n                    arr.compute_chunk_sizes()  # very inefficient (have to do it separately for every array)\n                    arr = arr.rechunk(chunks=\"auto\")\n                if cast_uints:\n                    if arr.dtype == np.uint64:\n                        arr = arr.astype(np.int64)\n                    elif arr.dtype == np.uint32:\n                        arr = arr.astype(np.int32)\n                task = da.to_zarr(\n                    arr, os.path.join(fname, p, k), overwrite=True, compute=False\n                )\n                tasks.append(task)\n        dask.compute(tasks)\n        for (p, k), unit_str in units_to_save.items():\n            root[p][k].attrs[\"units\"] = make_serializable(unit_str)\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.BaseDataset.__dask_tokenize__","title":"<code>__dask_tokenize__()</code>","text":"<p>Token for dask to be derived -- naively from the file location.</p> <p>Returns:</p> Type Description <code>int</code> Source code in <code>src/scida/interface.py</code> <pre><code>def __dask_tokenize__(self) -&gt; int:\n    \"\"\"\n    Token for dask to be derived -- naively from the file location.\n\n    Returns\n    -------\n    int\n    \"\"\"\n    return self.__hash__()\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.BaseDataset.__hash__","title":"<code>__hash__()</code>","text":"<p>Hash for Dataset instance to be derived from the file location.</p> <p>Returns:</p> Type Description <code>int</code> Source code in <code>src/scida/interface.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"\n    Hash for Dataset instance to be derived from the file location.\n\n    Returns\n    -------\n    int\n    \"\"\"\n    # deterministic hash; note that hash() on a string is no longer deterministic in python3.\n    hash_value = (\n        int(hashlib.sha256(self.location.encode(\"utf-8\")).hexdigest(), 16) % 10**10\n    )\n    return hash_value\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.BaseDataset.__init__","title":"<code>__init__(path, chunksize='auto', virtualcache=True, overwrite_cache=False, fileprefix='', hints=None, **kwargs)</code>","text":"<p>Initialize a dataset object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset.</p> required <code>chunksize</code> <code>str | int</code> <p>Chunksize for dask arrays.</p> <code>'auto'</code> <code>virtualcache</code> <code>bool</code> <p>Whether to use virtual caching.</p> <code>True</code> <code>overwrite_cache</code> <code>bool</code> <p>Whether to overwrite existing cache.</p> <code>False</code> <code>fileprefix</code> <code>str</code> <p>Prefix for files to scan for.</p> <code>''</code> <code>hints</code> <code>dict[str, Any] | None</code> <p>Hints for the dataset.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>src/scida/interface.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    chunksize: str | int = \"auto\",\n    virtualcache: bool = True,\n    overwrite_cache: bool = False,\n    fileprefix: str = \"\",\n    hints: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize a dataset object.\n\n    Parameters\n    ----------\n    path: str\n        Path to the dataset.\n    chunksize: int or str\n        Chunksize for dask arrays.\n    virtualcache: bool\n        Whether to use virtual caching.\n    overwrite_cache: bool\n        Whether to overwrite existing cache.\n    fileprefix: str\n        Prefix for files to scan for.\n    hints: dict\n        Hints for the dataset.\n    kwargs: dict\n        Additional keyword arguments.\n    \"\"\"\n    super().__init__()\n    self.hints = hints if hints is not None else {}\n    self.path = path\n    self.file = None\n    # need this 'tempfile' reference to keep garbage collection away for the tempfile\n    self.tempfile = None\n    self.location = str(path)\n    self.chunksize = chunksize\n    self.virtualcache = virtualcache\n    self.overwrite_cache = overwrite_cache\n    self.withunits = kwargs.get(\"units\", False)\n\n    # Let's find the data and metadata for the object at 'path'\n    self.metadata: dict[str, Any] = {}\n    self._metadata_raw: dict[str, Any] = {}\n    self.data = FieldContainer(withunits=self.withunits)\n\n    if not os.path.exists(self.path):\n        raise Exception(\"Specified path '%s' does not exist.\" % self.path)\n\n    loadkwargs = dict(\n        overwrite_cache=self.overwrite_cache,\n        fileprefix=fileprefix,\n        virtualcache=virtualcache,\n        nonvirtual_datasets=kwargs.get(\"nonvirtual_datasets\", None),\n        derivedfields_kwargs=dict(snap=self),\n        token=self.__dask_tokenize__(),\n        withunits=self.withunits,\n    )\n    if \"choose_prefix\" in kwargs:\n        loadkwargs[\"choose_prefix\"] = kwargs[\"choose_prefix\"]\n\n    res = scida.io.load(path, **loadkwargs)\n    self.data = res[0]\n    self._metadata_raw = res[1]\n    self.file = res[2]\n    self.tempfile = res[3]\n    self._cached = False\n\n    # any identifying metadata?\n    if \"dsname\" not in self.hints:\n        candidates = check_config_for_dataset(self._metadata_raw, path=self.path)\n        if len(candidates) &gt; 0:\n            dsname = candidates[0]\n            log.debug(\"Dataset is identified as '%s'.\" % dsname)\n            self.hints[\"dsname\"] = dsname\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.BaseDataset.__init_subclass__","title":"<code>__init_subclass__(*args, **kwargs)</code>","text":"<p>Register subclasses in the dataset type registry.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/interface.py</code> <pre><code>def __init_subclass__(cls, *args, **kwargs):\n    \"\"\"\n    Register subclasses in the dataset type registry.\n    Parameters\n    ----------\n    args: list\n    kwargs: dict\n\n    Returns\n    -------\n    None\n    \"\"\"\n    super().__init_subclass__(*args, **kwargs)\n    if cls.__name__ == \"Delay\":\n        return  # nothing to register for Delay objects\n    if \"Mixin\" in cls.__name__:\n        return  # do not register classes with Mixins\n    dataset_type_registry[cls.__name__] = cls\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.BaseDataset.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the object.</p> <p>Returns:</p> Type Description <code>str</code> Source code in <code>src/scida/interface.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Return a string representation of the object.\n\n    Returns\n    -------\n    str\n    \"\"\"\n    props = self._repr_dict()\n    clsname = self.__class__.__name__\n    result = clsname + \"[\"\n    for k, v in props.items():\n        result += \"%s=%s, \" % (k, v)\n    result = result[:-2] + \"]\"\n    return result\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.BaseDataset.info","title":"<code>info(listfields=False)</code>","text":"<p>Print information about the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>listfields</code> <code>bool</code> <p>If True, list all fields in the dataset.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/interface.py</code> <pre><code>def info(self, listfields: bool = False):\n    \"\"\"\n    Print information about the dataset.\n\n    Parameters\n    ----------\n    listfields: bool\n        If True, list all fields in the dataset.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    rep = \"\"\n    rep += \"class: \" + sprint(self.__class__.__name__)\n    props = self._repr_dict()\n    for k, v in props.items():\n        rep += sprint(\"%s: %s\" % (k, v))\n    if self._info_custom() is not None:\n        rep += self._info_custom()\n    rep += sprint(\"=== data ===\")\n    rep += self.data.info(name=\"root\")\n    rep += sprint(\"============\")\n    print(rep)\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.BaseDataset.return_data","title":"<code>return_data()</code>","text":"<p>Return the data container.</p> <p>Returns:</p> Type Description <code>FieldContainer</code> Source code in <code>src/scida/interface.py</code> <pre><code>def return_data(self) -&gt; FieldContainer:\n    \"\"\"\n    Return the data container.\n\n    Returns\n    -------\n    FieldContainer\n    \"\"\"\n    return self.data\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.BaseDataset.save","title":"<code>save(fname, fields='all', overwrite=True, zarr_kwargs=None, cast_uints=False, extra_attrs=None)</code>","text":"<p>Save the dataset to a file using the 'zarr' format.</p> <p>Parameters:</p> Name Type Description Default <code>extra_attrs</code> <code>dict[str, Any] | None</code> <p>additional attributes to save in the root group</p> <code>None</code> <code>fname</code> <code>str</code> <p>Filename to save to.</p> required <code>fields</code> <code>str | dict[str, list[str] | dict[str, Array]] | FieldContainer</code> <p>dictionary of dask arrays to save. If equal to 'all', save all fields in current dataset.</p> <code>'all'</code> <code>overwrite</code> <code>bool</code> <p>overwrite existing file</p> <code>True</code> <code>zarr_kwargs</code> <code>dict[str, Any] | None</code> <p>optional arguments to pass to zarr</p> <code>None</code> <code>cast_uints</code> <code>bool</code> <p>need to potentially cast uints to ints for some compressions; TODO: clean this up</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/interface.py</code> <pre><code>def save(\n    self,\n    fname: str,\n    fields: (\n        str | dict[str, list[str] | dict[str, da.Array]] | FieldContainer\n    ) = \"all\",\n    overwrite: bool = True,\n    zarr_kwargs: dict[str, Any] | None = None,\n    cast_uints: bool = False,\n    extra_attrs: dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"\n    Save the dataset to a file using the 'zarr' format.\n    Parameters\n    ----------\n    extra_attrs: dict\n        additional attributes to save in the root group\n    fname: str\n        Filename to save to.\n    fields: str or dict\n        dictionary of dask arrays to save. If equal to 'all', save all fields in current dataset.\n    overwrite\n        overwrite existing file\n    zarr_kwargs\n        optional arguments to pass to zarr\n    cast_uints\n        need to potentially cast uints to ints for some compressions; TODO: clean this up\n\n    Returns\n    -------\n    None\n    \"\"\"\n    # We use zarr, as this way we have support to directly write into the file by the workers\n    # (rather than passing back the data chunk over the scheduler to the interface)\n    # Also, this way we can leverage new features, such as a large variety of compression methods.\n    # cast_uints: if true, we cast uints to ints; needed for some compressions (particularly zfp)\n    if zarr_kwargs is None:\n        zarr_kwargs = {}\n    if overwrite and os.path.exists(fname):\n        if os.path.isdir(fname) and len(os.listdir(fname)) &gt; 0:\n            # check if it is a zarr group/array\n            is_zarr = os.path.exists(os.path.join(fname, \".zgroup\"))\n            is_zarr |= os.path.exists(os.path.join(fname, \".zarray\"))\n            if not is_zarr:\n                raise ValueError(\n                    f\"Directory '{fname}' exists and is not a zarr group. \"\n                    \"Refusing to overwrite for safety.\"\n                )\n    store = zarr.DirectoryStore(fname, **zarr_kwargs)\n    root = zarr.group(store, overwrite=overwrite)\n\n    # Metadata\n    defaultattributes = [\"Config\", \"Header\", \"Parameters\"]\n    for dctname in defaultattributes:\n        if dctname.lower() in self.__dict__:\n            grp = root.create_group(dctname)\n            dct = self.__dict__[dctname.lower()]\n            for k, v in dct.items():\n                v = make_serializable(v)\n                grp.attrs[k] = v\n    if extra_attrs is not None:\n        for k, v in extra_attrs.items():\n            root.attrs[k] = v\n    # Data\n    tasks = []\n    units_to_save = {}\n    ptypes = self.data.keys()\n    if isinstance(fields, dict):\n        ptypes = fields.keys()\n    elif isinstance(fields, str):\n        if not fields == \"all\":\n            raise ValueError(\"Invalid field specifier.\")\n    else:\n        raise ValueError(\"Invalid type for fields.\")\n    for p in ptypes:\n        root.create_group(p)\n        if fields == \"all\":\n            fieldkeys = self.data[p]\n        else:\n            if isinstance(fields[p], dict):\n                fieldkeys = fields[p].keys()\n            else:\n                fieldkeys = fields[p]\n        for k in fieldkeys:\n            if not isinstance(fields, str) and isinstance(fields[p], dict):\n                arr = fields[p][k]\n            else:\n                arr = self.data[p][k]\n            if hasattr(arr, \"magnitude\"):  # if we have units, remove those here\n                units_to_save[(p, k)] = str(arr.units)\n                arr = arr.magnitude\n            if np.any(np.isnan(arr.shape)):\n                arr.compute_chunk_sizes()  # very inefficient (have to do it separately for every array)\n                arr = arr.rechunk(chunks=\"auto\")\n            if cast_uints:\n                if arr.dtype == np.uint64:\n                    arr = arr.astype(np.int64)\n                elif arr.dtype == np.uint32:\n                    arr = arr.astype(np.int32)\n            task = da.to_zarr(\n                arr, os.path.join(fname, p, k), overwrite=True, compute=False\n            )\n            tasks.append(task)\n    dask.compute(tasks)\n    for (p, k), unit_str in units_to_save.items():\n        root[p][k].attrs[\"units\"] = make_serializable(unit_str)\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.BaseDataset.validate_path","title":"<code>validate_path(path, *args, **kwargs)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Validate whether the given path is a valid path for this dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> required <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> Source code in <code>src/scida/interface.py</code> <pre><code>@classmethod\n@abc.abstractmethod\ndef validate_path(cls, path, *args, **kwargs):\n    \"\"\"\n    Validate whether the given path is a valid path for this dataset.\n    Parameters\n    ----------\n    path\n    args\n    kwargs\n\n    Returns\n    -------\n    bool\n\n    \"\"\"\n    return False\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Base class for datasets with some functions to be overwritten by subclass.</p> Source code in <code>src/scida/interface.py</code> <pre><code>class Dataset(BaseDataset):\n    \"\"\"\n    Base class for datasets with some functions to be overwritten by subclass.\n    \"\"\"\n\n    @classmethod\n    def validate_path(cls, path, *args, **kwargs):\n        \"\"\"\n        Validate whether the given path is a valid path for this dataset.\n\n        Parameters\n        ----------\n        path: str\n            Path to the dataset.\n        args: list\n        kwargs: dict\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return True\n\n    @classmethod\n    def _clean_metadata_from_raw(cls, rawmetadata):\n        \"\"\"Clean metadata from raw metadata\"\"\"\n        return {}\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.Dataset.validate_path","title":"<code>validate_path(path, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Validate whether the given path is a valid path for this dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>Path to the dataset.</p> required <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> Source code in <code>src/scida/interface.py</code> <pre><code>@classmethod\ndef validate_path(cls, path, *args, **kwargs):\n    \"\"\"\n    Validate whether the given path is a valid path for this dataset.\n\n    Parameters\n    ----------\n    path: str\n        Path to the dataset.\n    args: list\n    kwargs: dict\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    return True\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.DatasetWithUnitMixin","title":"<code>DatasetWithUnitMixin</code>","text":"<p>               Bases: <code>UnitMixin</code>, <code>Dataset</code></p> <p>Dataset with units.</p> Source code in <code>src/scida/interface.py</code> <pre><code>class DatasetWithUnitMixin(UnitMixin, Dataset):\n    \"\"\"\n    Dataset with units.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize dataset with units.\"\"\"\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.DatasetWithUnitMixin.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize dataset with units.</p> Source code in <code>src/scida/interface.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Initialize dataset with units.\"\"\"\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.MixinMeta","title":"<code>MixinMeta</code>","text":"<p>               Bases: <code>type</code></p> <p>Metaclass for Mixin classes.</p> Source code in <code>src/scida/interface.py</code> <pre><code>class MixinMeta(type):\n    \"\"\"\n    Metaclass for Mixin classes.\n    \"\"\"\n\n    def __call__(cls, *args, **kwargs):\n        mixins = kwargs.pop(\"mixins\", None)\n        newcls = create_datasetclass_with_mixins(cls, mixins)\n        return type.__call__(newcls, *args, **kwargs)\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.Selector","title":"<code>Selector</code>","text":"<p>               Bases: <code>object</code></p> <p>Base Class for data selection decorator factory</p> Source code in <code>src/scida/interface.py</code> <pre><code>class Selector(object):\n    \"\"\"Base Class for data selection decorator factory\"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the selector.\n        \"\"\"\n        self.keys = None  # the keys we check for.\n        # holds a copy of the species' fields\n        self.data_backup = FieldContainer()\n        # holds the species' fields we operate on\n        self.data: FieldContainer = FieldContainer()\n\n    def __call__(self, fn, *args, **kwargs):\n        \"\"\"\n        Call the selector.\n\n        Parameters\n        ----------\n        fn: function\n            Function to be decorated.\n        args: list\n        kwargs: dict\n\n        Returns\n        -------\n        function\n            Decorated function.\n\n        \"\"\"\n\n        def newfn(*args, **kwargs):\n            # TODO: Add graceful exit/restore after exception in self.prepare\n            self.data_backup = args[0].data\n            self.data = args[0].data.copy_skeleton()\n            # deepdictkeycopy(self.data_backup, self.data)\n\n            self.prepare(*args, **kwargs)\n            if self.keys is None:\n                raise NotImplementedError(\n                    \"Subclass implementation needed for self.keys!\"\n                )\n            if kwargs.pop(\"dropkeys\", True):\n                for k in self.keys:\n                    kwargs.pop(k, None)\n            try:\n                result = fn(*args, **kwargs)\n                return result\n            finally:\n                self.finalize(*args, **kwargs)\n\n        return newfn\n\n    def prepare(self, *args, **kwargs) -&gt; None:\n        \"\"\"\n        Prepare the data for selection. To be implemented in subclasses.\n\n        Parameters\n        ----------\n        args\n        kwargs\n\n        Returns\n        -------\n\n        \"\"\"\n        raise NotImplementedError(\"Subclass implementation needed!\")\n\n    def finalize(self, *args, **kwargs) -&gt; None:\n        \"\"\"\n        Finalize the data after selection. To be implemented in subclasses.\n\n        Parameters\n        ----------\n        args\n        kwargs\n\n        Returns\n        -------\n\n        \"\"\"\n        args[0].data = self.data_backup\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.Selector.__call__","title":"<code>__call__(fn, *args, **kwargs)</code>","text":"<p>Call the selector.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <p>Function to be decorated.</p> required <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>function</code> <p>Decorated function.</p> Source code in <code>src/scida/interface.py</code> <pre><code>def __call__(self, fn, *args, **kwargs):\n    \"\"\"\n    Call the selector.\n\n    Parameters\n    ----------\n    fn: function\n        Function to be decorated.\n    args: list\n    kwargs: dict\n\n    Returns\n    -------\n    function\n        Decorated function.\n\n    \"\"\"\n\n    def newfn(*args, **kwargs):\n        # TODO: Add graceful exit/restore after exception in self.prepare\n        self.data_backup = args[0].data\n        self.data = args[0].data.copy_skeleton()\n        # deepdictkeycopy(self.data_backup, self.data)\n\n        self.prepare(*args, **kwargs)\n        if self.keys is None:\n            raise NotImplementedError(\n                \"Subclass implementation needed for self.keys!\"\n            )\n        if kwargs.pop(\"dropkeys\", True):\n            for k in self.keys:\n                kwargs.pop(k, None)\n        try:\n            result = fn(*args, **kwargs)\n            return result\n        finally:\n            self.finalize(*args, **kwargs)\n\n    return newfn\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.Selector.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the selector.</p> Source code in <code>src/scida/interface.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the selector.\n    \"\"\"\n    self.keys = None  # the keys we check for.\n    # holds a copy of the species' fields\n    self.data_backup = FieldContainer()\n    # holds the species' fields we operate on\n    self.data: FieldContainer = FieldContainer()\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.Selector.finalize","title":"<code>finalize(*args, **kwargs)</code>","text":"<p>Finalize the data after selection. To be implemented in subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> Source code in <code>src/scida/interface.py</code> <pre><code>def finalize(self, *args, **kwargs) -&gt; None:\n    \"\"\"\n    Finalize the data after selection. To be implemented in subclasses.\n\n    Parameters\n    ----------\n    args\n    kwargs\n\n    Returns\n    -------\n\n    \"\"\"\n    args[0].data = self.data_backup\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.Selector.prepare","title":"<code>prepare(*args, **kwargs)</code>","text":"<p>Prepare the data for selection. To be implemented in subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> Source code in <code>src/scida/interface.py</code> <pre><code>def prepare(self, *args, **kwargs) -&gt; None:\n    \"\"\"\n    Prepare the data for selection. To be implemented in subclasses.\n\n    Parameters\n    ----------\n    args\n    kwargs\n\n    Returns\n    -------\n\n    \"\"\"\n    raise NotImplementedError(\"Subclass implementation needed!\")\n</code></pre>"},{"location":"api/moduleindex/#scida.interface.create_datasetclass_with_mixins","title":"<code>create_datasetclass_with_mixins(cls, mixins)</code>","text":"<p>Create a new class from a given class and a list of mixins.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type</code> <p>dataset class to be extended</p> required <code>mixins</code> <code>list | None</code> <p>list of mixin classes to be added</p> required <p>Returns:</p> Type Description <code>Type[BaseDataset]</code> <p>new class with mixins</p> Source code in <code>src/scida/interface.py</code> <pre><code>def create_datasetclass_with_mixins(cls: type, mixins: list | None) -&gt; type:\n    \"\"\"\n    Create a new class from a given class and a list of mixins.\n\n    Parameters\n    ----------\n    cls:\n        dataset class to be extended\n    mixins:\n        list of mixin classes to be added\n\n    Returns\n    -------\n    Type[BaseDataset]\n        new class with mixins\n    \"\"\"\n    newcls = cls\n    if isinstance(mixins, list) and len(mixins) &gt; 0:\n        # check for duplicates within the mixins list itself\n        seen = set()\n        for m in mixins:\n            if m in seen:\n                raise ValueError(\n                    \"Mixin '%s' appears multiple times in the mixins list. \"\n                    \"Please remove duplicates.\" % getattr(m, \"__name__\", str(m))\n                )\n            seen.add(m)\n\n        # check whether any mixin already in cls hierarchy recursively\n        def has_mixin_in_hierarchy(cls, m):\n            if m in cls.__bases__:\n                return True\n            for b in cls.__bases__:\n                if has_mixin_in_hierarchy(b, m):\n                    return True\n            return False\n\n        for m in mixins:\n            if has_mixin_in_hierarchy(cls, m):\n                raise ValueError(\n                    \"Mixin '%s' is already present in the class hierarchy of '%s'. \"\n                    \"Please remove it from the mixins list to avoid duplication.\"\n                    % (getattr(m, \"__name__\", str(m)), cls.__name__)\n                )\n        name = cls.__name__ + \"With\" + \"And\".join([m.__name__ for m in mixins])\n        # adjust entry point if __init__ available in some mixin\n        nms = dict(cls.__dict__)\n        # need to make sure first mixin init is called over cls init\n        nms[\"__init__\"] = mixins[0].__init__\n        newcls = type(name, (*mixins, cls), nms)\n    return newcls\n</code></pre>"},{"location":"api/moduleindex/#scida.io","title":"<code>io</code>","text":"<p>scida.io</p>"},{"location":"api/moduleindex/#scida.io.fits","title":"<code>fits</code>","text":"<p>FITS file reader for scida</p>"},{"location":"api/moduleindex/#scida.io.fits.fitsrecords_to_daskarrays","title":"<code>fitsrecords_to_daskarrays(fitsrecords)</code>","text":"<p>Convert a FITS record array to a dictionary of dask arrays.</p> <p>Parameters:</p> Name Type Description Default <code>fitsrecords</code> <p>FITS record array</p> required <p>Returns:</p> Type Description <code>dict</code> <p>dictionary of dask arrays</p> Source code in <code>src/scida/io/fits.py</code> <pre><code>def fitsrecords_to_daskarrays(fitsrecords):\n    \"\"\"\n    Convert a FITS record array to a dictionary of dask arrays.\n    Parameters\n    ----------\n    fitsrecords: np.ndarray\n        FITS record array\n\n    Returns\n    -------\n    dict\n        dictionary of dask arrays\n\n    \"\"\"\n    load_arr = delayed(lambda slc, field: fitsrecords[slc][field])\n    shape = fitsrecords.shape\n    darrdict = {}\n    csize = dask.config.get(\"array.chunk-size\")\n\n    csize = parse_size(csize)  # need int\n\n    nbytes_dtype_max = 1\n    for fieldname in fitsrecords.dtype.names:\n        nbytes_dtype = fitsrecords.dtype[fieldname].itemsize\n        nbytes_dtype_max = max(nbytes_dtype_max, nbytes_dtype)\n    chunksize = csize // nbytes_dtype_max\n\n    for fieldname in fitsrecords.dtype.names:\n        chunks = []\n        for index in range(0, shape[-1], chunksize):\n            dtype = fitsrecords.dtype[fieldname]\n            chunk_size = min(chunksize, shape[-1] - index)\n            slc = slice(index, index + chunk_size)\n            shp = (chunk_size,)\n            if dtype.subdtype is not None:\n                # for now, we expect this to be void type\n                assert dtype.type is np.void\n                break  # do not handle void type for now =&gt; skip field\n                # shp = shp + dtype.subdtype[0].shape\n                # dtype = dtype.subdtype[0].base\n            chunk = da.from_delayed(load_arr(slc, fieldname), shape=shp, dtype=dtype)\n            chunks.append(chunk)\n        if len(chunks) &gt; 0:\n            darrdict[fieldname] = da.concatenate(chunks, axis=0)\n    return darrdict\n</code></pre>"},{"location":"api/moduleindex/#scida.misc","title":"<code>misc</code>","text":"<p>Miscellaneous helper functions.</p>"},{"location":"api/moduleindex/#scida.misc.check_config_for_dataset","title":"<code>check_config_for_dataset(metadata, path=None, unique=True)</code>","text":"<p>Check whether the given dataset can be identified to be a certain simulation (type) by its metadata.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <p>metadata of the dataset used for identification</p> required <code>path</code> <code>str | None</code> <p>path to the dataset, sometimes helpful for identification</p> <code>None</code> <code>unique</code> <code>bool</code> <p>whether to expect return to be unique</p> <code>True</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>Strings of Candidate identifiers</p> Source code in <code>src/scida/misc.py</code> <pre><code>def check_config_for_dataset(metadata, path: str | None = None, unique: bool = True):\n    \"\"\"\n    Check whether the given dataset can be identified to be a certain simulation (type) by its metadata.\n\n    Parameters\n    ----------\n    metadata: dict\n        metadata of the dataset used for identification\n    path: str\n        path to the dataset, sometimes helpful for identification\n    unique: bool\n        whether to expect return to be unique\n\n    Returns\n    -------\n    List[str]\n        Strings of Candidate identifiers\n\n    \"\"\"\n    c = get_simulationconfig()\n\n    candidates: list[str] = []\n    if \"data\" not in c:\n        return candidates\n    simdct = c[\"data\"]\n    if simdct is None:\n        simdct = {}\n    for k, vals in simdct.items():\n        if vals is None:\n            continue\n        possible_candidate = True\n        if \"identifiers\" in vals:\n            idtfrs = vals[\"identifiers\"]\n            # special key not specifying identifying metadata\n            specialkeys = [\"name_contains\"]\n            allkeys = idtfrs.keys()\n            keys = list([k for k in allkeys if k not in specialkeys])\n            if \"name_contains\" in idtfrs and path is not None:\n                p = pathlib.Path(path)\n                # we only check the last three path elements\n                dirnames = [p.name, p.parents[0].name, p.parents[1].name]\n                substring = idtfrs[\"name_contains\"]\n                if not any([substring.lower() in d.lower() for d in dirnames]):\n                    possible_candidate = False\n            if len(allkeys) == 0:\n                possible_candidate = False\n            for grp in keys:\n                v = idtfrs[grp]\n                h5path = \"/\" + grp\n                if h5path not in metadata:\n                    possible_candidate = False\n                    break\n                attrs = metadata[h5path]\n                for ikey, ival in v.items():\n                    if ikey not in attrs:\n                        possible_candidate = False\n                        break\n                    av = attrs[ikey]\n                    matchtype = None\n                    if isinstance(ival, dict):\n                        matchtype = ival.get(\"match\", matchtype)  # default means equal\n                        ival = ival[\"content\"]\n\n                    if isinstance(av, bytes):\n                        av = av.decode(\"UTF-8\")\n                    if matchtype is None:\n                        if is_scalar(av):\n                            if not np.isclose(av, ival):\n                                possible_candidate = False\n                                break\n                        elif isinstance(av, (list, np.ndarray)):\n                            if not np.all(av == ival):\n                                possible_candidate = False\n                                break\n                        else:\n                            if av != ival:\n                                possible_candidate = False\n                                break\n                    elif matchtype == \"substring\":\n                        if ival not in av:\n                            possible_candidate = False\n                            break\n        else:\n            possible_candidate = False\n        if possible_candidate:\n            candidates.append(k)\n    if unique and len(candidates) &gt; 1:\n        raise ValueError(\"Multiple dataset candidates (set unique=False?):\", candidates)\n    return candidates\n</code></pre>"},{"location":"api/moduleindex/#scida.misc.deepdictkeycopy","title":"<code>deepdictkeycopy(olddict, newdict)</code>","text":"<p>Recursively walk nested dictionary, only creating empty dictionaries for entries that are dictionaries themselves.</p> <p>Parameters:</p> Name Type Description Default <code>olddict</code> <code>object</code> required <code>newdict</code> <code>object</code> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/misc.py</code> <pre><code>def deepdictkeycopy(olddict: object, newdict: object) -&gt; None:\n    \"\"\"\n    Recursively walk nested dictionary, only creating empty dictionaries for entries that are dictionaries themselves.\n    Parameters\n    ----------\n    olddict\n    newdict\n\n    Returns\n    -------\n    None\n    \"\"\"\n    cls = olddict.__class__\n    for k, v in olddict.items():\n        if isinstance(v, MutableMapping):\n            newdict[k] = cls()\n            deepdictkeycopy(v, newdict[k])\n</code></pre>"},{"location":"api/moduleindex/#scida.misc.get_container_from_path","title":"<code>get_container_from_path(element, container=None, create_missing=False)</code>","text":"<p>Get a container from a path.</p> <p>Parameters:</p> Name Type Description Default <code>element</code> <code>str</code> required <code>container</code> <code>FieldContainer | None</code> <code>None</code> <code>create_missing</code> <code>bool</code> <code>False</code> <p>Returns:</p> Name Type Description <code>FieldContainer</code> <code>FieldContainer</code> <p>container specified by path</p> Source code in <code>src/scida/misc.py</code> <pre><code>def get_container_from_path(\n    element: str, container: FieldContainer | None = None, create_missing: bool = False\n) -&gt; FieldContainer:\n    \"\"\"\n    Get a container from a path.\n    Parameters\n    ----------\n    element: str\n    container: FieldContainer\n    create_missing: bool\n\n    Returns\n    -------\n    FieldContainer:\n        container specified by path\n\n    \"\"\"\n    keys = element.split(\"/\")\n    rv = container\n    for key in keys:\n        if key == \"\":\n            continue\n        if key not in rv._containers:\n            if not create_missing:\n                raise ValueError(\"Container '%s' not found in '%s'\" % (key, rv))\n            rv.add_container(key, name=key)\n        rv = rv._containers[key]\n    return rv\n</code></pre>"},{"location":"api/moduleindex/#scida.misc.map_interface_args","title":"<code>map_interface_args(paths, *args, **kwargs)</code>","text":"<p>Map arguments for interface if they are not lists.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list</code> required <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>generator</code> <p>yields path, args, kwargs</p> Source code in <code>src/scida/misc.py</code> <pre><code>def map_interface_args(paths: list, *args, **kwargs):\n    \"\"\"\n    Map arguments for interface if they are not lists.\n    Parameters\n    ----------\n    paths\n    args\n    kwargs\n\n    Returns\n    -------\n    generator\n        yields path, args, kwargs\n\n    \"\"\"\n    n = len(paths)\n    for i, path in enumerate(paths):\n        targs = []\n        for arg in args:\n            if not (isinstance(arg, list)) or len(arg) != n:\n                targs.append(arg)\n            else:\n                targs.append(arg[i])\n        tkwargs = {}\n        for k, v in kwargs.items():\n            if not (isinstance(v, list)) or len(v) != n:\n                tkwargs[k] = v\n            else:\n                tkwargs[k] = v[i]\n        yield path, targs, tkwargs\n</code></pre>"},{"location":"api/moduleindex/#scida.misc.parse_size","title":"<code>parse_size(size)</code>","text":"<p>Parse a human-readable size string to a number in bytes.</p> <p>Supports both SI units (KB, MB, GB, TB) and binary units (KiB, MiB, GiB, TiB).</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>str</code> <p>Human-readable size string, e.g. \"128MiB\", \"1.5 GiB\", \"500MB\".</p> required <p>Returns:</p> Type Description <code>int</code> <p>Size in bytes.</p> Source code in <code>src/scida/misc.py</code> <pre><code>def parse_size(size):\n    \"\"\"\n    Parse a human-readable size string to a number in bytes.\n\n    Supports both SI units (KB, MB, GB, TB) and binary units (KiB, MiB, GiB, TiB).\n\n    Parameters\n    ----------\n    size : str\n        Human-readable size string, e.g. \"128MiB\", \"1.5 GiB\", \"500MB\".\n\n    Returns\n    -------\n    int\n        Size in bytes.\n    \"\"\"\n    size = size.upper()\n    if not size.startswith(\" \"):\n        size = _RE_UNIT_SIZE.sub(r\" \\1\", size)\n    number, unit = [string.strip() for string in size.split()]\n    return int(float(number) * _sizeunits[unit])\n</code></pre>"},{"location":"api/moduleindex/#scida.misc.path_hdf5cachefile_exists","title":"<code>path_hdf5cachefile_exists(path, **kwargs)</code>","text":"<p>Checks whether a cache file exists for given path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>path to the dataset</p> required <code>kwargs</code> <p>passed to return_hdf5cachepath</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> Source code in <code>src/scida/misc.py</code> <pre><code>def path_hdf5cachefile_exists(path, **kwargs) -&gt; bool:\n    \"\"\"\n    Checks whether a cache file exists for given path.\n    Parameters\n    ----------\n    path:\n        path to the dataset\n    kwargs:\n        passed to return_hdf5cachepath\n    Returns\n    -------\n    bool\n\n    \"\"\"\n    fp = return_hdf5cachepath(path, **kwargs)\n    if os.path.isfile(fp):\n        return True\n    return False\n</code></pre>"},{"location":"api/moduleindex/#scida.misc.rectangular_cutout_mask","title":"<code>rectangular_cutout_mask(center, width, coords, pbc=True, boxsize=None, backend='dask', chunksize='auto')</code>","text":"<p>Create a rectangular mask for a given set of coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>center</code> <p>center of the rectangle</p> required <code>width</code> <p>widths of the rectangle</p> required <code>coords</code> <p>coordinates to mask</p> required <code>pbc</code> <p>whether to apply PBC</p> <code>True</code> <code>boxsize</code> <p>boxsize for PBC</p> <code>None</code> <code>backend</code> <p>backend to use (dask or numpy)</p> <code>'dask'</code> <code>chunksize</code> <p>chunksize for dask</p> <code>'auto'</code> <p>Returns:</p> Name Type Description <code>ndarray</code> <p>mask</p> Source code in <code>src/scida/misc.py</code> <pre><code>def rectangular_cutout_mask(\n    center, width, coords, pbc=True, boxsize=None, backend=\"dask\", chunksize=\"auto\"\n):\n    \"\"\"\n    Create a rectangular mask for a given set of coordinates.\n    Parameters\n    ----------\n    center: list\n        center of the rectangle\n    width: list\n        widths of the rectangle\n    coords: ndarray\n        coordinates to mask\n    pbc: bool\n        whether to apply PBC\n    boxsize:\n        boxsize for PBC\n    backend: str\n        backend to use (dask or numpy)\n    chunksize: str\n        chunksize for dask\n\n    Returns\n    -------\n    ndarray:\n        mask\n\n    \"\"\"\n    center = np.array(center)\n    width = np.array(width)\n    if backend == \"dask\":\n        be = da\n    else:\n        be = np\n\n    dists = coords - center\n    dists = be.fabs(dists)\n    if pbc:\n        if boxsize is None:\n            raise ValueError(\"Need to specify for boxsize for PBC.\")\n        dists = be.where(dists &gt; 0.5 * boxsize, be.fabs(boxsize - dists), dists)\n\n    kwargs = {}\n    if backend == \"dask\":\n        kwargs[\"chunks\"] = chunksize\n    mask = be.ones(coords.shape[0], dtype=bool, **kwargs)\n    for i in range(3):\n        mask &amp;= dists[:, i] &lt; (\n            width[i] / 2.0\n        )  # TODO: This interval is not closed on the left side.\n    return mask\n</code></pre>"},{"location":"api/moduleindex/#scida.misc.return_cachefile_path","title":"<code>return_cachefile_path(fname)</code>","text":"<p>Return the path to the cache file, return None if path cannot be generated.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>filename of cache file</p> required <p>Returns:</p> Type Description <code>str or None</code> Source code in <code>src/scida/misc.py</code> <pre><code>def return_cachefile_path(fname: str) -&gt; str | None:\n    \"\"\"\n    Return the path to the cache file, return None if path cannot be generated.\n\n    Parameters\n    ----------\n    fname: str\n        filename of cache file\n\n    Returns\n    -------\n    str or None\n\n    \"\"\"\n    config = get_config()\n    if \"cache_path\" not in config:\n        return None\n    cp = config[\"cache_path\"]\n    cp = os.path.expanduser(cp)\n    path = pathlib.Path(cp)\n    path.mkdir(parents=True, exist_ok=True)\n    fp = os.path.join(cp, fname)\n    fp = os.path.expanduser(fp)\n    bp = os.path.dirname(fp)\n    if not os.path.exists(bp):\n        try:\n            os.mkdir(bp)\n        except FileExistsError:\n            pass  # can happen due to parallel access\n    return fp\n</code></pre>"},{"location":"api/moduleindex/#scida.misc.return_hdf5cachepath","title":"<code>return_hdf5cachepath(path, fileprefix=None)</code>","text":"<p>Returns the path to the cache file for a given path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>path to the dataset</p> required <code>fileprefix</code> <code>str | None</code> <p>Can be used to specify the fileprefix used for the dataset.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> Source code in <code>src/scida/misc.py</code> <pre><code>def return_hdf5cachepath(path, fileprefix: str | None = None) -&gt; str:\n    \"\"\"\n    Returns the path to the cache file for a given path.\n\n    Parameters\n    ----------\n    path: str\n        path to the dataset\n    fileprefix: str | None\n        Can be used to specify the fileprefix used for the dataset.\n\n    Returns\n    -------\n    str\n\n    \"\"\"\n    if fileprefix is not None:\n        path = os.path.join(path, fileprefix)\n    hsh = hash_path(path)\n    fp = return_cachefile_path(os.path.join(hsh, \"data.hdf5\"))\n    return fp\n</code></pre>"},{"location":"api/moduleindex/#scida.misc.str_is_float","title":"<code>str_is_float(element)</code>","text":"<p>Check whether a string can be converted to a float.</p> <p>Parameters:</p> Name Type Description Default <code>element</code> <code>str</code> <p>string to check</p> required <p>Returns:</p> Type Description <code>bool</code> Source code in <code>src/scida/misc.py</code> <pre><code>def str_is_float(element: str) -&gt; bool:\n    \"\"\"\n    Check whether a string can be converted to a float.\n    Parameters\n    ----------\n    element: str\n        string to check\n\n    Returns\n    -------\n    bool\n\n    \"\"\"\n    try:\n        float(element)\n        return True\n    except ValueError:\n        return False\n</code></pre>"},{"location":"api/moduleindex/#scida.registries","title":"<code>registries</code>","text":"<p>This module contains registries for dataset and dataseries subclasses. Subclasses are automatically registered through init_subclass</p>"},{"location":"api/moduleindex/#scida.series","title":"<code>series</code>","text":"<p>This module contains the base class for DataSeries, which is a container for collections of dataset instances.</p>"},{"location":"api/moduleindex/#scida.series.DatasetSeries","title":"<code>DatasetSeries</code>","text":"<p>               Bases: <code>object</code></p> <p>A container for collections of dataset instances</p> <p>Attributes:</p> Name Type Description <code>datasets</code> <code>list</code> <p>list of dataset instances</p> <code>paths</code> <code>list</code> <p>list of paths to data</p> <code>names</code> <code>list</code> <p>list of names for datasets</p> <code>hash</code> <code>str</code> <p>hash of the object, constructed from dataset paths.</p> Source code in <code>src/scida/series.py</code> <pre><code>class DatasetSeries(object):\n    \"\"\"A container for collections of dataset instances\n\n    Attributes\n    ----------\n    datasets: list\n        list of dataset instances\n    paths: list\n        list of paths to data\n    names: list\n        list of names for datasets\n    hash: str\n        hash of the object, constructed from dataset paths.\n    \"\"\"\n\n    def __init__(\n        self,\n        paths: list[str] | list[Path],\n        *interface_args,\n        datasetclass=None,\n        overwrite_cache=False,\n        lazy=True,  # lazy will only initialize data sets on demand.\n        names=None,\n        **interface_kwargs,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        paths: list\n            list of paths to data\n        interface_args:\n            arguments to pass to interface class\n        datasetclass:\n            class to use for dataset instances\n        overwrite_cache:\n            whether to overwrite existing cache\n        lazy:\n            whether to initialize datasets lazily\n        names:\n            names for datasets\n        interface_kwargs:\n            keyword arguments to pass to interface class\n        \"\"\"\n        self.paths = paths\n        self.names = names\n        self.hash = hash_path(\"\".join([str(p) for p in paths]))\n        self._metadata = None\n        self._metadatafile = return_cachefile_path(os.path.join(self.hash, \"data.json\"))\n        self.lazy = lazy\n        if overwrite_cache and os.path.exists(self._metadatafile):\n            os.remove(self._metadatafile)\n        for p in paths:\n            if not (isinstance(p, Path)):\n                p = Path(p)\n            if not (p.exists()):\n                raise ValueError(\"Specified path '%s' does not exist.\" % p)\n        dec = delay_init  # lazy loading\n\n        # Catch Mixins and create type:\n        ikw = dict(overwrite_cache=overwrite_cache)\n        ikw.update(**interface_kwargs)\n        mixins = ikw.pop(\"mixins\", [])\n        datasetclass = create_datasetclass_with_mixins(datasetclass, mixins)\n        self._dataset_cls = datasetclass\n\n        gen = map_interface_args(paths, *interface_args, **ikw)\n        self.datasets = [dec(datasetclass)(p, *a, **kw) for p, a, kw in gen]\n\n        if self.metadata is None:\n            print(\"Have not cached this data series. Can take a while.\")\n            dct = {}\n            for i, (path, d) in enumerate(\n                tqdm(zip(self.paths, self.datasets), total=len(self.paths))\n            ):\n                rawmeta = load_metadata(\n                    path, choose_prefix=True, use_cachefile=not (overwrite_cache)\n                )\n                # class method does not initiate obj.\n                dct[i] = d._clean_metadata_from_raw(rawmeta)\n            self.metadata = dct\n\n    def __init_subclass__(cls, *args, **kwargs):\n        \"\"\"\n        Register datasetseries subclass in registry.\n        Parameters\n        ----------\n        args:\n            (unused)\n        kwargs:\n            (unused)\n        Returns\n        -------\n        None\n        \"\"\"\n        super().__init_subclass__(*args, **kwargs)\n        dataseries_type_registry[cls.__name__] = cls\n\n    def __len__(self):\n        \"\"\"Return number of datasets in series.\n\n        Returns\n        -------\n        int\n        \"\"\"\n        return len(self.datasets)\n\n    def __getitem__(self, key):\n        \"\"\"\n        Return dataset by index.\n        Parameters\n        ----------\n        key\n\n        Returns\n        -------\n        Dataset\n\n        \"\"\"\n        return self.datasets[key]\n\n    def info(self):\n        \"\"\"\n        Print information about this datasetseries.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        rep = \"\"\n        rep += \"class: \" + sprint(self.__class__.__name__)\n        props = self._repr_dict()\n        for k, v in props.items():\n            rep += sprint(\"%s: %s\" % (k, v))\n        if self.metadata is not None:\n            rep += sprint(\"=== metadata ===\")\n            # we print the range of each metadata attribute\n            minmax_dct = {}\n            for mdct in self.metadata.values():\n                for k, v in mdct.items():\n                    if k not in minmax_dct:\n                        minmax_dct[k] = [v, v]\n                    else:\n                        if not np.isscalar(v):\n                            continue  # cannot compare arrays\n                        minmax_dct[k][0] = min(minmax_dct[k][0], v)\n                        minmax_dct[k][1] = max(minmax_dct[k][1], v)\n            for k in minmax_dct:\n                reprval1, reprval2 = minmax_dct[k][0], minmax_dct[k][1]\n                if isinstance(reprval1, float):\n                    reprval1 = \"%.2f\" % reprval1\n                    reprval2 = \"%.2f\" % reprval2\n                m1 = minmax_dct[k][0]\n                m2 = minmax_dct[k][1]\n                if (not np.isscalar(m1)) or (np.isscalar(m1) and m1 == m2):\n                    rep += sprint(\"%s: %s\" % (k, minmax_dct[k][0]))\n                else:\n                    rep += sprint(\n                        \"%s: %s -- %s\" % (k, minmax_dct[k][0], minmax_dct[k][1])\n                    )\n            rep += sprint(\"============\")\n        print(rep)\n\n    @property\n    def data(self) -&gt; None:\n        \"\"\"\n        Dummy property to make user aware this is not a Dataset instance.\n        Returns\n        -------\n        None\n\n        \"\"\"\n        raise AttributeError(\n            \"Series do not have 'data' attribute. Load a dataset from series.get_dataset().\"\n        )\n\n    def _repr_dict(self) -&gt; dict[str, str]:\n        \"\"\"\n        Return a dictionary of properties to be printed by __repr__ method.\n\n        Returns\n        -------\n        dict\n        \"\"\"\n        props = dict()\n        sources = [str(p) for p in self.paths]\n        props[\"source(id=0)\"] = sources[0]\n        props[\"Ndatasets\"] = len(self.datasets)\n        return props\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Return a string representation of the datasetseries object.\n\n        Returns\n        -------\n        str\n        \"\"\"\n        props = self._repr_dict()\n        clsname = self.__class__.__name__\n        result = clsname + \"[\"\n        for k, v in props.items():\n            result += \"%s=%s, \" % (k, v)\n        result = result[:-2] + \"]\"\n        return result\n\n    @classmethod\n    def validate_path(cls, path, *args, **kwargs) -&gt; CandidateStatus:\n        \"\"\"\n        Check whether a given path is a valid path for this dataseries class.\n        Parameters\n        ----------\n        path: str\n            path to check\n        args:\n            (unused)\n        kwargs:\n            (unused)\n\n        Returns\n        -------\n        CandidateStatus\n        \"\"\"\n        return CandidateStatus.NO  # base class dummy\n\n    @classmethod\n    def from_directory(\n        cls, path, *interface_args, datasetclass=None, pattern=None, **interface_kwargs\n    ) -&gt; \"DatasetSeries\":\n        \"\"\"\n        Create a datasetseries instance from a directory.\n        Parameters\n        ----------\n        path: str\n            path to directory\n        interface_args:\n            arguments to pass to interface class\n        datasetclass: Dataset | None\n            force class to use for dataset instances\n        pattern:\n            pattern to match files in directory\n        interface_kwargs:\n            keyword arguments to pass to interface class\n        Returns\n        -------\n        DatasetSeries\n\n        \"\"\"\n        p = Path(path)\n        if not (p.exists()):\n            raise ValueError(\"Specified path does not exist.\")\n        if pattern is None:\n            pattern = \"*\"\n        paths = [f for f in p.glob(pattern)]\n        return cls(\n            paths, *interface_args, datasetclass=datasetclass, **interface_kwargs\n        )\n\n    def get_dataset(\n        self,\n        index: int | None = None,\n        name: str | None = None,\n        reltol=1e-2,\n        **kwargs,\n    ):\n        \"\"\"\n        Get dataset by some metadata property. In the base class, we go by list index.\n\n        Parameters\n        ----------\n        index: int\n            index of dataset to get\n        name: str\n            name of dataset to get\n        reltol:\n            relative tolerance for metadata comparison\n        kwargs:\n            metadata properties to compare for selection\n\n        Returns\n        -------\n        Dataset\n\n        \"\"\"\n        if index is None and name is None and len(kwargs) == 0:\n            raise ValueError(\"Specify index/name or some parameter to select for.\")\n        # aliases for index:\n        aliases = [\"snap\", \"snapshot\"]\n        aliases_given = [k for k in aliases if k in kwargs]\n        if index is not None:\n            aliases_given += [index]\n        if len(aliases_given) &gt; 1:\n            raise ValueError(\"Multiple aliases for index specified.\")\n        for a in aliases_given:\n            if kwargs.get(a) is not None:\n                index = kwargs.pop(a)\n        if index is not None:\n            return self.datasets[index]\n\n        if name is not None:\n            if self.names is None:\n                raise ValueError(\"No names specified for members of this series.\")\n            if name not in self.names:\n                raise ValueError(\"Name %s not found in this series.\" % name)\n            return self.datasets[self.names.index(name)]\n        if len(kwargs) &gt; 0 and self.metadata is None:\n            if self.lazy:\n                raise ValueError(\n                    \"Cannot select by given keys before dataset evaluation.\"\n                )\n            raise ValueError(\"Unknown error.\")  # should not happen?\n\n        # find candidates from metadata\n        candidates = []\n        candidates_props: dict[str, Any] = {}\n        props_compare = set()  # save names of fields we want to compare\n        for k, v in kwargs.items():\n            candidates_props[k] = []\n        for i, (j, dm) in enumerate(self.metadata.items()):\n            assert int(i) == int(j)\n            is_candidate = True\n            for k, v in kwargs.items():\n                if k not in dm:\n                    is_candidate = False\n                    continue\n                if isinstance(v, (int, float, np.integer, np.floating)):\n                    candidates_props[k].append(dm[k])\n                    props_compare.add(k)\n                elif v != dm[k]:\n                    is_candidate = False\n            if is_candidate:\n                candidates.append(i)\n            else:  # unroll changes\n                for lst in candidates_props.values():\n                    if len(lst) &gt; len(candidates):\n                        lst.pop()\n\n        # find candidate closest to request\n        if len(candidates) == 0:\n            raise ValueError(\"No candidate found for given metadata.\")\n        idxlist = []\n        for k in props_compare:\n            idx = np.argmin(np.abs(np.array(candidates_props[k]) - kwargs[k]))\n            idxlist.append(idx)\n        if len(set(idxlist)) &gt; 1:\n            raise ValueError(\"Ambiguous selection request\")\n        elif len(idxlist) == 0:\n            raise ValueError(\"No candidate found.\")\n        index = candidates[idxlist[0]]\n        # tolerance check\n        for k in props_compare:\n            if not np.isclose(kwargs[k], self.metadata[index][k], rtol=reltol):\n                msg = (\n                    \"Candidate does not match tolerance for %s (%s vs %s requested)\"\n                    % (\n                        k,\n                        self.metadata[index][k],\n                        kwargs[k],\n                    )\n                )\n                raise ValueError(msg)\n        return self.get_dataset(index=index)\n\n    @property\n    def metadata(self):\n        \"\"\"\n        Return metadata dictionary for this series.\n\n        Returns\n        -------\n        Optional[dict]\n            metadata dictionary\n        \"\"\"\n        if self._metadata is not None:\n            return self._metadata\n        fp = self._metadatafile\n        if os.path.exists(fp):\n            md = json.load(open(fp, \"r\"))\n            ikeys = sorted([int(k) for k in md.keys()])\n            mdnew = {}\n            for ik in ikeys:\n                mdnew[ik] = md[str(ik)]\n            self._metadata = mdnew\n            return self._metadata\n        return None\n\n    @metadata.setter\n    def metadata(self, dct):\n        \"\"\"\n        Set metadata dictionary for this series, and save to disk.\n        Parameters\n        ----------\n        dct: dict\n            metadata dictionary\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n\n        class ComplexEncoder(json.JSONEncoder):\n            \"\"\"\n            JSON encoder that can handle numpy arrays and bytes.\n            \"\"\"\n\n            def default(self, obj):\n                \"\"\"\n                Default recipe for encoding objects.\n                Parameters\n                ----------\n                obj: object\n                    object to encode\n\n                Returns\n                -------\n                object\n\n                \"\"\"\n                if isinstance(obj, np.int64):\n                    return int(obj)\n                if isinstance(obj, np.int32):\n                    return int(obj)\n                if isinstance(obj, np.uint32):\n                    return int(obj)\n                if isinstance(obj, bytes):\n                    return obj.decode(\"utf-8\")\n                if isinstance(obj, np.ndarray):\n                    assert len(obj) &lt; 1000  # do not want large obs here...\n                    return list(obj)\n                try:\n                    return json.JSONEncoder.default(self, obj)\n                except TypeError as e:\n                    print(\"obj failing json encoding:\", obj)\n                    raise e\n\n        self._metadata = dct\n        fp = self._metadatafile\n        if not os.path.exists(fp):\n            json.dump(dct, open(fp, \"w\"), cls=ComplexEncoder)\n</code></pre>"},{"location":"api/moduleindex/#scida.series.DatasetSeries.data","title":"<code>data</code>  <code>property</code>","text":"<p>Dummy property to make user aware this is not a Dataset instance.</p> <p>Returns:</p> Type Description <code>None</code>"},{"location":"api/moduleindex/#scida.series.DatasetSeries.metadata","title":"<code>metadata</code>  <code>property</code> <code>writable</code>","text":"<p>Return metadata dictionary for this series.</p> <p>Returns:</p> Type Description <code>Optional[dict]</code> <p>metadata dictionary</p>"},{"location":"api/moduleindex/#scida.series.DatasetSeries.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Return dataset by index.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> required <p>Returns:</p> Type Description <code>Dataset</code> Source code in <code>src/scida/series.py</code> <pre><code>def __getitem__(self, key):\n    \"\"\"\n    Return dataset by index.\n    Parameters\n    ----------\n    key\n\n    Returns\n    -------\n    Dataset\n\n    \"\"\"\n    return self.datasets[key]\n</code></pre>"},{"location":"api/moduleindex/#scida.series.DatasetSeries.__init__","title":"<code>__init__(paths, *interface_args, datasetclass=None, overwrite_cache=False, lazy=True, names=None, **interface_kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[str] | list[Path]</code> <p>list of paths to data</p> required <code>interface_args</code> <p>arguments to pass to interface class</p> <code>()</code> <code>datasetclass</code> <p>class to use for dataset instances</p> <code>None</code> <code>overwrite_cache</code> <p>whether to overwrite existing cache</p> <code>False</code> <code>lazy</code> <p>whether to initialize datasets lazily</p> <code>True</code> <code>names</code> <p>names for datasets</p> <code>None</code> <code>interface_kwargs</code> <p>keyword arguments to pass to interface class</p> <code>{}</code> Source code in <code>src/scida/series.py</code> <pre><code>def __init__(\n    self,\n    paths: list[str] | list[Path],\n    *interface_args,\n    datasetclass=None,\n    overwrite_cache=False,\n    lazy=True,  # lazy will only initialize data sets on demand.\n    names=None,\n    **interface_kwargs,\n):\n    \"\"\"\n\n    Parameters\n    ----------\n    paths: list\n        list of paths to data\n    interface_args:\n        arguments to pass to interface class\n    datasetclass:\n        class to use for dataset instances\n    overwrite_cache:\n        whether to overwrite existing cache\n    lazy:\n        whether to initialize datasets lazily\n    names:\n        names for datasets\n    interface_kwargs:\n        keyword arguments to pass to interface class\n    \"\"\"\n    self.paths = paths\n    self.names = names\n    self.hash = hash_path(\"\".join([str(p) for p in paths]))\n    self._metadata = None\n    self._metadatafile = return_cachefile_path(os.path.join(self.hash, \"data.json\"))\n    self.lazy = lazy\n    if overwrite_cache and os.path.exists(self._metadatafile):\n        os.remove(self._metadatafile)\n    for p in paths:\n        if not (isinstance(p, Path)):\n            p = Path(p)\n        if not (p.exists()):\n            raise ValueError(\"Specified path '%s' does not exist.\" % p)\n    dec = delay_init  # lazy loading\n\n    # Catch Mixins and create type:\n    ikw = dict(overwrite_cache=overwrite_cache)\n    ikw.update(**interface_kwargs)\n    mixins = ikw.pop(\"mixins\", [])\n    datasetclass = create_datasetclass_with_mixins(datasetclass, mixins)\n    self._dataset_cls = datasetclass\n\n    gen = map_interface_args(paths, *interface_args, **ikw)\n    self.datasets = [dec(datasetclass)(p, *a, **kw) for p, a, kw in gen]\n\n    if self.metadata is None:\n        print(\"Have not cached this data series. Can take a while.\")\n        dct = {}\n        for i, (path, d) in enumerate(\n            tqdm(zip(self.paths, self.datasets), total=len(self.paths))\n        ):\n            rawmeta = load_metadata(\n                path, choose_prefix=True, use_cachefile=not (overwrite_cache)\n            )\n            # class method does not initiate obj.\n            dct[i] = d._clean_metadata_from_raw(rawmeta)\n        self.metadata = dct\n</code></pre>"},{"location":"api/moduleindex/#scida.series.DatasetSeries.__init_subclass__","title":"<code>__init_subclass__(*args, **kwargs)</code>","text":"<p>Register datasetseries subclass in registry.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>(unused)</p> <code>()</code> <code>kwargs</code> <p>(unused)</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/series.py</code> <pre><code>def __init_subclass__(cls, *args, **kwargs):\n    \"\"\"\n    Register datasetseries subclass in registry.\n    Parameters\n    ----------\n    args:\n        (unused)\n    kwargs:\n        (unused)\n    Returns\n    -------\n    None\n    \"\"\"\n    super().__init_subclass__(*args, **kwargs)\n    dataseries_type_registry[cls.__name__] = cls\n</code></pre>"},{"location":"api/moduleindex/#scida.series.DatasetSeries.__len__","title":"<code>__len__()</code>","text":"<p>Return number of datasets in series.</p> <p>Returns:</p> Type Description <code>int</code> Source code in <code>src/scida/series.py</code> <pre><code>def __len__(self):\n    \"\"\"Return number of datasets in series.\n\n    Returns\n    -------\n    int\n    \"\"\"\n    return len(self.datasets)\n</code></pre>"},{"location":"api/moduleindex/#scida.series.DatasetSeries.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the datasetseries object.</p> <p>Returns:</p> Type Description <code>str</code> Source code in <code>src/scida/series.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Return a string representation of the datasetseries object.\n\n    Returns\n    -------\n    str\n    \"\"\"\n    props = self._repr_dict()\n    clsname = self.__class__.__name__\n    result = clsname + \"[\"\n    for k, v in props.items():\n        result += \"%s=%s, \" % (k, v)\n    result = result[:-2] + \"]\"\n    return result\n</code></pre>"},{"location":"api/moduleindex/#scida.series.DatasetSeries.from_directory","title":"<code>from_directory(path, *interface_args, datasetclass=None, pattern=None, **interface_kwargs)</code>  <code>classmethod</code>","text":"<p>Create a datasetseries instance from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>path to directory</p> required <code>interface_args</code> <p>arguments to pass to interface class</p> <code>()</code> <code>datasetclass</code> <p>force class to use for dataset instances</p> <code>None</code> <code>pattern</code> <p>pattern to match files in directory</p> <code>None</code> <code>interface_kwargs</code> <p>keyword arguments to pass to interface class</p> <code>{}</code> <p>Returns:</p> Type Description <code>DatasetSeries</code> Source code in <code>src/scida/series.py</code> <pre><code>@classmethod\ndef from_directory(\n    cls, path, *interface_args, datasetclass=None, pattern=None, **interface_kwargs\n) -&gt; \"DatasetSeries\":\n    \"\"\"\n    Create a datasetseries instance from a directory.\n    Parameters\n    ----------\n    path: str\n        path to directory\n    interface_args:\n        arguments to pass to interface class\n    datasetclass: Dataset | None\n        force class to use for dataset instances\n    pattern:\n        pattern to match files in directory\n    interface_kwargs:\n        keyword arguments to pass to interface class\n    Returns\n    -------\n    DatasetSeries\n\n    \"\"\"\n    p = Path(path)\n    if not (p.exists()):\n        raise ValueError(\"Specified path does not exist.\")\n    if pattern is None:\n        pattern = \"*\"\n    paths = [f for f in p.glob(pattern)]\n    return cls(\n        paths, *interface_args, datasetclass=datasetclass, **interface_kwargs\n    )\n</code></pre>"},{"location":"api/moduleindex/#scida.series.DatasetSeries.get_dataset","title":"<code>get_dataset(index=None, name=None, reltol=0.01, **kwargs)</code>","text":"<p>Get dataset by some metadata property. In the base class, we go by list index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | None</code> <p>index of dataset to get</p> <code>None</code> <code>name</code> <code>str | None</code> <p>name of dataset to get</p> <code>None</code> <code>reltol</code> <p>relative tolerance for metadata comparison</p> <code>0.01</code> <code>kwargs</code> <p>metadata properties to compare for selection</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dataset</code> Source code in <code>src/scida/series.py</code> <pre><code>def get_dataset(\n    self,\n    index: int | None = None,\n    name: str | None = None,\n    reltol=1e-2,\n    **kwargs,\n):\n    \"\"\"\n    Get dataset by some metadata property. In the base class, we go by list index.\n\n    Parameters\n    ----------\n    index: int\n        index of dataset to get\n    name: str\n        name of dataset to get\n    reltol:\n        relative tolerance for metadata comparison\n    kwargs:\n        metadata properties to compare for selection\n\n    Returns\n    -------\n    Dataset\n\n    \"\"\"\n    if index is None and name is None and len(kwargs) == 0:\n        raise ValueError(\"Specify index/name or some parameter to select for.\")\n    # aliases for index:\n    aliases = [\"snap\", \"snapshot\"]\n    aliases_given = [k for k in aliases if k in kwargs]\n    if index is not None:\n        aliases_given += [index]\n    if len(aliases_given) &gt; 1:\n        raise ValueError(\"Multiple aliases for index specified.\")\n    for a in aliases_given:\n        if kwargs.get(a) is not None:\n            index = kwargs.pop(a)\n    if index is not None:\n        return self.datasets[index]\n\n    if name is not None:\n        if self.names is None:\n            raise ValueError(\"No names specified for members of this series.\")\n        if name not in self.names:\n            raise ValueError(\"Name %s not found in this series.\" % name)\n        return self.datasets[self.names.index(name)]\n    if len(kwargs) &gt; 0 and self.metadata is None:\n        if self.lazy:\n            raise ValueError(\n                \"Cannot select by given keys before dataset evaluation.\"\n            )\n        raise ValueError(\"Unknown error.\")  # should not happen?\n\n    # find candidates from metadata\n    candidates = []\n    candidates_props: dict[str, Any] = {}\n    props_compare = set()  # save names of fields we want to compare\n    for k, v in kwargs.items():\n        candidates_props[k] = []\n    for i, (j, dm) in enumerate(self.metadata.items()):\n        assert int(i) == int(j)\n        is_candidate = True\n        for k, v in kwargs.items():\n            if k not in dm:\n                is_candidate = False\n                continue\n            if isinstance(v, (int, float, np.integer, np.floating)):\n                candidates_props[k].append(dm[k])\n                props_compare.add(k)\n            elif v != dm[k]:\n                is_candidate = False\n        if is_candidate:\n            candidates.append(i)\n        else:  # unroll changes\n            for lst in candidates_props.values():\n                if len(lst) &gt; len(candidates):\n                    lst.pop()\n\n    # find candidate closest to request\n    if len(candidates) == 0:\n        raise ValueError(\"No candidate found for given metadata.\")\n    idxlist = []\n    for k in props_compare:\n        idx = np.argmin(np.abs(np.array(candidates_props[k]) - kwargs[k]))\n        idxlist.append(idx)\n    if len(set(idxlist)) &gt; 1:\n        raise ValueError(\"Ambiguous selection request\")\n    elif len(idxlist) == 0:\n        raise ValueError(\"No candidate found.\")\n    index = candidates[idxlist[0]]\n    # tolerance check\n    for k in props_compare:\n        if not np.isclose(kwargs[k], self.metadata[index][k], rtol=reltol):\n            msg = (\n                \"Candidate does not match tolerance for %s (%s vs %s requested)\"\n                % (\n                    k,\n                    self.metadata[index][k],\n                    kwargs[k],\n                )\n            )\n            raise ValueError(msg)\n    return self.get_dataset(index=index)\n</code></pre>"},{"location":"api/moduleindex/#scida.series.DatasetSeries.info","title":"<code>info()</code>","text":"<p>Print information about this datasetseries.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/series.py</code> <pre><code>def info(self):\n    \"\"\"\n    Print information about this datasetseries.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    rep = \"\"\n    rep += \"class: \" + sprint(self.__class__.__name__)\n    props = self._repr_dict()\n    for k, v in props.items():\n        rep += sprint(\"%s: %s\" % (k, v))\n    if self.metadata is not None:\n        rep += sprint(\"=== metadata ===\")\n        # we print the range of each metadata attribute\n        minmax_dct = {}\n        for mdct in self.metadata.values():\n            for k, v in mdct.items():\n                if k not in minmax_dct:\n                    minmax_dct[k] = [v, v]\n                else:\n                    if not np.isscalar(v):\n                        continue  # cannot compare arrays\n                    minmax_dct[k][0] = min(minmax_dct[k][0], v)\n                    minmax_dct[k][1] = max(minmax_dct[k][1], v)\n        for k in minmax_dct:\n            reprval1, reprval2 = minmax_dct[k][0], minmax_dct[k][1]\n            if isinstance(reprval1, float):\n                reprval1 = \"%.2f\" % reprval1\n                reprval2 = \"%.2f\" % reprval2\n            m1 = minmax_dct[k][0]\n            m2 = minmax_dct[k][1]\n            if (not np.isscalar(m1)) or (np.isscalar(m1) and m1 == m2):\n                rep += sprint(\"%s: %s\" % (k, minmax_dct[k][0]))\n            else:\n                rep += sprint(\n                    \"%s: %s -- %s\" % (k, minmax_dct[k][0], minmax_dct[k][1])\n                )\n        rep += sprint(\"============\")\n    print(rep)\n</code></pre>"},{"location":"api/moduleindex/#scida.series.DatasetSeries.validate_path","title":"<code>validate_path(path, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Check whether a given path is a valid path for this dataseries class.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>path to check</p> required <code>args</code> <p>(unused)</p> <code>()</code> <code>kwargs</code> <p>(unused)</p> <code>{}</code> <p>Returns:</p> Type Description <code>CandidateStatus</code> Source code in <code>src/scida/series.py</code> <pre><code>@classmethod\ndef validate_path(cls, path, *args, **kwargs) -&gt; CandidateStatus:\n    \"\"\"\n    Check whether a given path is a valid path for this dataseries class.\n    Parameters\n    ----------\n    path: str\n        path to check\n    args:\n        (unused)\n    kwargs:\n        (unused)\n\n    Returns\n    -------\n    CandidateStatus\n    \"\"\"\n    return CandidateStatus.NO  # base class dummy\n</code></pre>"},{"location":"api/moduleindex/#scida.series.DirectoryCatalog","title":"<code>DirectoryCatalog</code>","text":"<p>               Bases: <code>object</code></p> <p>A catalog consisting of interface instances contained in a directory.</p> Source code in <code>src/scida/series.py</code> <pre><code>class DirectoryCatalog(object):\n    \"\"\"A catalog consisting of interface instances contained in a directory.\"\"\"\n\n    def __init__(self, path):\n        \"\"\"\n        Initialize a directory catalog.\n\n        Parameters\n        ----------\n        path: str\n            path to directory\n        \"\"\"\n        self.path = path\n</code></pre>"},{"location":"api/moduleindex/#scida.series.DirectoryCatalog.__init__","title":"<code>__init__(path)</code>","text":"<p>Initialize a directory catalog.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>path to directory</p> required Source code in <code>src/scida/series.py</code> <pre><code>def __init__(self, path):\n    \"\"\"\n    Initialize a directory catalog.\n\n    Parameters\n    ----------\n    path: str\n        path to directory\n    \"\"\"\n    self.path = path\n</code></pre>"},{"location":"api/moduleindex/#scida.series.HomogeneousSeries","title":"<code>HomogeneousSeries</code>","text":"<p>               Bases: <code>DatasetSeries</code></p> <p>Series consisting of same-type data sets.</p> Source code in <code>src/scida/series.py</code> <pre><code>class HomogeneousSeries(DatasetSeries):\n    \"\"\"Series consisting of same-type data sets.\"\"\"\n\n    def __init__(self, path, **interface_kwargs):\n        \"\"\"\n        Initialize a homogeneous series.\n        Parameters\n        ----------\n        path:\n            path to data\n        interface_kwargs:\n            keyword arguments to pass to interface class\n        \"\"\"\n        super().__init__()\n</code></pre>"},{"location":"api/moduleindex/#scida.series.HomogeneousSeries.__init__","title":"<code>__init__(path, **interface_kwargs)</code>","text":"<p>Initialize a homogeneous series.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>path to data</p> required <code>interface_kwargs</code> <p>keyword arguments to pass to interface class</p> <code>{}</code> Source code in <code>src/scida/series.py</code> <pre><code>def __init__(self, path, **interface_kwargs):\n    \"\"\"\n    Initialize a homogeneous series.\n    Parameters\n    ----------\n    path:\n        path to data\n    interface_kwargs:\n        keyword arguments to pass to interface class\n    \"\"\"\n    super().__init__()\n</code></pre>"},{"location":"api/moduleindex/#scida.series.delay_init","title":"<code>delay_init(cls)</code>","text":"<p>Decorate class to delay initialization until an attribute is requested.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>class to decorate</p> required <p>Returns:</p> Type Description <code>Delay</code> Source code in <code>src/scida/series.py</code> <pre><code>def delay_init(cls):\n    \"\"\"\n    Decorate class to delay initialization until an attribute is requested.\n    Parameters\n    ----------\n    cls:\n        class to decorate\n\n    Returns\n    -------\n    Delay\n    \"\"\"\n\n    class Delay(cls):\n        \"\"\"\n        Delayed initialization of a class. The class is replaced by the actual class\n        when an attribute is requested.\n        \"\"\"\n\n        def __init__(self, *args, **kwargs):\n            \"\"\"Store arguments for later initialization.\"\"\"\n            self._args = args\n            self._kwargs = kwargs\n\n        def __getattribute__(self, name):\n            \"\"\"Replace the class with the actual class and initialize it if needed.\"\"\"\n            # a few special calls do not trigger initialization:\n            specialattrs = [\n                \"__repr__\",\n                \"__str__\",\n                \"__dir__\",\n                \"__class__\",\n                \"_args\",\n                \"_kwargs\",\n                # https://github.com/jupyter/notebook/issues/2014\n                \"_ipython_canary_method_should_not_exist_\",\n            ]\n            if name in specialattrs or name.startswith(\"_repr\"):\n                return object.__getattribute__(self, name)\n            elif hasattr(cls, name) and inspect.ismethod(getattr(cls, name)):\n                # do not need to initialize for class methods\n                return getattr(cls, name)\n            arg = self._args\n            kwarg = self._kwargs\n            self.__class__ = cls\n            del self._args\n            del self._kwargs\n            self.__init__(*arg, **kwarg)\n            if name == \"evaluate_lazy\":\n                return getattr(self, \"__repr__\")  # some dummy\n            return getattr(self, name)\n\n        def __repr__(self):\n            \"\"\"Return a string representation of the lazy class.\"\"\"\n            return \"&lt;Lazy %s&gt;\" % cls.__name__\n\n    return Delay\n</code></pre>"},{"location":"api/moduleindex/#scida.utilities","title":"<code>utilities</code>","text":"<p>Some utility functions</p>"},{"location":"api/moduleindex/#scida.utilities.copy_to_zarr","title":"<code>copy_to_zarr(fp_in, fp_out, compressor=None)</code>","text":"<p>Reads and converts a scida Dataset to a zarr object on disk</p> <p>Parameters:</p> Name Type Description Default <code>fp_in</code> <p>object path to convert</p> required <code>fp_out</code> <p>output path</p> required <code>compressor</code> <p>zarr compressor to use, see https://zarr.readthedocs.io/en/stable/tutorial.html#compressors</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>src/scida/utilities.py</code> <pre><code>def copy_to_zarr(fp_in, fp_out, compressor=None):\n    \"\"\"\n    Reads and converts a scida Dataset to a zarr object on disk\n\n    Parameters\n    ----------\n    fp_in: str\n        object path to convert\n    fp_out: str\n        output path\n    compressor:\n        zarr compressor to use, see https://zarr.readthedocs.io/en/stable/tutorial.html#compressors\n\n    Returns\n    -------\n    None\n    \"\"\"\n    ds = Dataset(fp_in)\n    compressor_dflt = zarr.storage.default_compressor\n    zarr.storage.default_compressor = compressor\n    ds.save(fp_out, cast_uints=True)\n    zarr.storage.default_compressor = compressor_dflt\n</code></pre>"},{"location":"cookbooks/","title":"Cookbooks","text":""},{"location":"cookbooks/#cookbooks","title":"Cookbooks","text":"<p>Contains cookbooks for various tasks. Each cookbook is a Jupyter notebook that demonstrates specific tasks using scida, giving a more in-depth hands-on compared to the tutorial. More material will be added over time. You can download the underlying notebooks from the scida repository. Many notebooks require access to various data sets. You can learn more about most datasets supported data here. For astrophysical simulations, your easiest way to work with the data is to use the TNGLab online.</p>"},{"location":"cookbooks/#working-with-the-tng-cluster-simulation","title":"Working with the TNG-Cluster simulation","text":""},{"location":"cookbooks/TNG-Cluster/basic/","title":"TNG-Cluster Basics","text":"In\u00a0[1]: Copied! <pre># imports\nfrom dask.distributed import Client\nfrom dask_jobqueue import SLURMCluster\nfrom scida import load\nimport numpy as np\nimport dask\nimport dask.array as da\nimport logging\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LogNorm\n# logging.root.setLevel(logging.INFO)\n\n# more sensible choice for chunk size here\ndask.config.set({'array.chunk-size': '1024MiB'})\n\n# get some SLURM nodes\ncluster = SLURMCluster(queue='p.large', cores=72, memory=\"500 GB\",\n                       processes=36, # mostly want processes; threads do not release GIL for I/O etc\n                       scheduler_options={\"dashboard_address\": \":8844\"})\ncluster.scale(jobs=4)  # ask for 4 jobs\nclient = Client(cluster)\n</pre> # imports from dask.distributed import Client from dask_jobqueue import SLURMCluster from scida import load import numpy as np import dask import dask.array as da import logging import matplotlib.pyplot as plt from matplotlib.colors import LogNorm # logging.root.setLevel(logging.INFO)  # more sensible choice for chunk size here dask.config.set({'array.chunk-size': '1024MiB'})  # get some SLURM nodes cluster = SLURMCluster(queue='p.large', cores=72, memory=\"500 GB\",                        processes=36, # mostly want processes; threads do not release GIL for I/O etc                        scheduler_options={\"dashboard_address\": \":8844\"}) cluster.scale(jobs=4)  # ask for 4 jobs client = Client(cluster) <pre>Warning! Using default configuration. Please adjust/replace in '/u/byrohlc/.config/scida/config.yaml'.\n</pre> In\u00a0[2]: Copied! <pre>series = load(\"TNG-Cluster\")\nds = series.get_dataset(redshift=0.0)\ndata_all = ds.data\ncoords_all = data_all[\"PartType1\"][\"Coordinates\"].magnitude\n</pre> series = load(\"TNG-Cluster\") ds = series.get_dataset(redshift=0.0) data_all = ds.data coords_all = data_all[\"PartType1\"][\"Coordinates\"].magnitude In\u00a0[3]: Copied! <pre>nbins = 1024\nranges = [[0.0, ds.boxsize[0]], [0.0, ds.boxsize[0]]]\nbins = np.linspace(0.0, ds.boxsize[0], nbins)\nhist = da.histogramdd(coords_all[:,:2], bins=[nbins, nbins], range=ranges)[0].compute()\n</pre> nbins = 1024 ranges = [[0.0, ds.boxsize[0]], [0.0, ds.boxsize[0]]] bins = np.linspace(0.0, ds.boxsize[0], nbins) hist = da.histogramdd(coords_all[:,:2], bins=[nbins, nbins], range=ranges)[0].compute() In\u00a0[4]: Copied! <pre># white spaces as we only consider high-res particles (\"PartType1\"), not low-res (\"PartType2\")\nplt.imshow(hist.T, origin=\"lower\", norm=LogNorm(), interpolation=\"none\")\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> # white spaces as we only consider high-res particles (\"PartType1\"), not low-res (\"PartType2\") plt.imshow(hist.T, origin=\"lower\", norm=LogNorm(), interpolation=\"none\") plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[5]: Copied! <pre># let us select zoomID object 5, including its \"fluff\"\ndata_zoom = ds.return_data(zoomID=5, withfuzz=True)\ncoords_zoom = data_zoom[\"PartType1\"][\"Coordinates\"]\n</pre> # let us select zoomID object 5, including its \"fluff\" data_zoom = ds.return_data(zoomID=5, withfuzz=True) coords_zoom = data_zoom[\"PartType1\"][\"Coordinates\"] In\u00a0[6]: Copied! <pre>idx = da.where(data_zoom[\"Group\"][\"GroupPrimaryZoomTarget\"]==1)[0].compute()[0]\ncenter = data_zoom[\"Group\"][\"GroupPos\"][idx].compute().magnitude\nradius = data_zoom[\"Group\"][\"Group_R_Crit200\"][idx].compute().magnitude\n</pre> idx = da.where(data_zoom[\"Group\"][\"GroupPrimaryZoomTarget\"]==1)[0].compute()[0] center = data_zoom[\"Group\"][\"GroupPos\"][idx].compute().magnitude radius = data_zoom[\"Group\"][\"Group_R_Crit200\"][idx].compute().magnitude In\u00a0[7]: Copied! <pre>nbins = 1024\nbins = np.linspace(0.0, ds.boxsize[0], nbins)\nranges = [[center[0]-radius, center[0]+radius], [center[1]-radius, center[1]+radius]]\nhist = da.histogramdd(coords_zoom[:,:2], bins=[nbins, nbins], range=ranges)[0].compute()\n</pre> nbins = 1024 bins = np.linspace(0.0, ds.boxsize[0], nbins) ranges = [[center[0]-radius, center[0]+radius], [center[1]-radius, center[1]+radius]] hist = da.histogramdd(coords_zoom[:,:2], bins=[nbins, nbins], range=ranges)[0].compute() In\u00a0[8]: Copied! <pre>plt.imshow(hist.T, origin=\"lower\", norm=LogNorm(), interpolation=\"none\")\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> plt.imshow(hist.T, origin=\"lower\", norm=LogNorm(), interpolation=\"none\") plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[9]: Copied! <pre>cluster.close()\n</pre> cluster.close()"},{"location":"cookbooks/TNG-Cluster/basic/#tng-cluster-basics","title":"TNG-Cluster Basics\u00b6","text":"<p>In this notebook, you will combine using a SLURM cluster and load the TNG-Cluster simulation. You will learn about the custom selectors allowing efficient access to different clusters given the data layout.</p> <p>If you do not have access to a SLURM HPC resource, but nevertheless access to the simulation, you can comment out all lines defining and initializing \"cluster\" and \"client\".</p>"},{"location":"cookbooks/TNG-Cluster/basic/#setup","title":"Setup\u00b6","text":""},{"location":"cookbooks/TNG-Cluster/basic/#load-data","title":"Load data\u00b6","text":""},{"location":"cookbooks/TNG-Cluster/basic/#global-particles","title":"Global particles\u00b6","text":""},{"location":"cookbooks/TNG-Cluster/basic/#simple-projection","title":"Simple projection\u00b6","text":""},{"location":"cookbooks/TNG-Cluster/basic/#local-particles","title":"Local particles\u00b6","text":"<p>Efficiently select a certain zoom-in target given TNG-Cluster layout</p>"},{"location":"tutorial/","title":"Tutorial","text":"<p>           Simulations         </p> <p>Tutorial on a simulation dataset.</p> <p>           Observations         </p> <p>Tutorial on an observational dataset.</p>"},{"location":"tutorial/observations/","title":"Observations","text":""},{"location":"tutorial/observations/#tutorial-observational-data-set","title":"Tutorial (observational data set)","text":"<p>This package is designed to aid in the efficient analysis of large datasets, such as GAIA DR3.</p> <p>Tutorial dataset</p> <p>In the following, we will subset from the GAIA data release 3. The reduced dataset contains 100000 randomly selected entries only. The reduced dataset can be downloaded here. Check Supported Datasets for an incomplete list of supported datasets and requirements for support of new datasets. A tutorial for a cosmological simulation can be found here.</p> <p>It uses the dask library to perform computations, which has several key advantages:</p> <ol> <li>very large datasets which cannot normally fit into memory can be analyzed,</li> <li>calculations can be automatically distributed onto parallel 'workers', across one or more nodes, to speed them up,</li> <li>we can create abstract graphs (\"recipes\", such as for derived quantities) and only evaluate on actual demand.</li> </ol>"},{"location":"tutorial/observations/#loading-an-individual-dataset","title":"Loading an individual dataset","text":"<p>Here we use the GAIA data release 3 as an example. In particular, we support the single HDF5 version of DR3.</p> <p>The dataset is obtained in HDF5 format as used at ITA Heidelberg. We intentionally select a small subset of the data to work with. Choosing a subset means that the data size is small and easy to work with. We demonstrate how to work with larger data sets at a later stage.</p> <p>First, we load the dataset using the convenience function <code>load()</code> that will determine the appropriate dataset class for us:</p> <p>Missing units</p> <p>Below snippet will report missing units for some fields. This is expected. Those fields that cannot have their units determined automatically at this point and carry the unit unknown. See Units for more information.</p> Loading a dataset<pre><code>&gt;&gt;&gt; from scida import load\n&gt;&gt;&gt; ds = load(\"gaia_dr3_subset100000.hdf5\", units=True) #(1)!\n&gt;&gt;&gt; ds.info() #(2)!\nclass: DatasetWithUnitMixin\nsource: /home/cbyrohl/data/testdata-scida/gaia_dr3_subset100000.hdf5\n=== Unit-aware Dataset ===\n==========================\n=== data ===\n+ root (fields: 27, entries: 100000)\n============\n</code></pre> <ol> <li>The <code>units=True</code> argument will attach code units to all fields (this is the default). Alternative choices are False to go without units and cgs for cgs units.</li> <li>Call to receive some information about the loaded dataset.</li> </ol> <p>The dataset is now loaded, and we can inspect its contents, specifically its container and fields loaded. We can access the data in the dataset by using the <code>data</code> attribute, which is a dictionary of containers and fields.</p> <p>We have a total of 27 fields available, which are:</p> Available fields<pre><code>&gt;&gt;&gt; ds.data.keys()\n['pmdec',\n 'distance_gspphot',\n...\n 'distance_gspphot_upper',\n 'pmra_error']\n</code></pre> <p>Let's take a look at some field in this container:</p> Inspecting a field<pre><code>&gt;&gt;&gt; ds.data[\"dec\"]\ndask.array&lt;mul, shape=(100000,), dtype=float64, chunksize=(100000,), chunktype=numpy.ndarray&gt; &lt;Unit('degree')&gt;\n</code></pre> <p>The field is a dask array, which is a lazy array that will only be evaluated when needed. How these lazy arrays and their units work and are to be used will be explored in the next section.</p>"},{"location":"tutorial/observations/#dask-arrays-and-units","title":"Dask arrays and units","text":""},{"location":"tutorial/observations/#dask-arrays","title":"Dask arrays","text":"<p>Dask arrays are virtual entities that are only evaluated when needed. If you are unfamiliar with dask arrays, consider taking a look at this 3-minute introduction.</p> <p>They are not numpy arrays, but they can be converted to them, and have most of their functionality. Within dask, an internal task graph is created that holds the recipes how to construct the array from the underlying data.</p> <p>In general, fields can be also be stored in flat or more nested structures, depending on the dataset.</p> <p>We can trigger the evaluation of the dask array by calling <code>compute()</code> on it:</p> Evaluating a dask array<pre><code>&gt;&gt;&gt; ds.data[\"dec\"].compute()\narray([ 0.86655069,  1.15477218,  2.14207063, ..., -1.5291509 ,\n       -1.30061261, -0.88984633]) &lt;Unit('degree')&gt;\n</code></pre> <p>However, directly evaluating dask arrays is strongly discouraged for large datasets, as it will load the entire dataset into memory. Instead, we will reduce the datasize by running desired analysis/reduction within dask before calling compute(), which we present in the next section.</p>"},{"location":"tutorial/observations/#units","title":"Units","text":"<p>If passing <code>units=True</code> (default) to <code>load()</code>, the dataset will be loaded with code units attached to all fields. These units are attached to each field / dask array. Units are provided via the pint package. See the pint documentation for more information. Also check out this page for more unit-related examples.</p> <p>In short, each field, that is represented by a modified dask array, has a magnitude (the dask array without any units attached) and a unit. These can be accessed via the <code>magnitude</code> and <code>units</code> attributes, respectively.</p> Accessing the magnitude and units of a field<pre><code>&gt;&gt;&gt; ds.data[\"dec\"].magnitude.compute(), ds.data[\"dec\"].units\n(dask.array&lt;mul, shape=(100000,), dtype=float64, chunksize=(100000,), chunktype=numpy.ndarray&gt;,\n &lt;Unit('degree')&gt;)\n</code></pre> <p>When defining derived fields from dask arrays, the correct units are automatically propagated to the new field, and dimensionality checks are performed. Importantly, the unit calculation is done immediately, thus allowing to directly see the resulting units and any dimensionality mismatches.</p>"},{"location":"tutorial/observations/#analyzing-the-data","title":"Analyzing the data","text":""},{"location":"tutorial/observations/#computing-a-simple-statistic-on-all-objects","title":"Computing a simple statistic on (all) objects","text":"<p>The fields in our data object behave similar to actual numpy arrays.</p> <p>As a first simple example, let's calculate the mean declination of the stars. Just as in numpy we can write</p> Calculating the mean declination<pre><code>&gt;&gt;&gt; dec = ds.data[\"dec\"]\n&gt;&gt;&gt; task = dec.mean()\n&gt;&gt;&gt; task\ndask.array&lt;mean_agg-aggregate, shape=(), dtype=float64, chunksize=(), chunktype=numpy.ndarray&gt; &lt;Unit('degree')&gt;\n</code></pre> <p>Note that all objects remain 'virtual': they are not calculated or loaded from disk, but are merely the required instructions, encoded into tasks.</p> <p>We can request a calculation of the actual operation(s) by applying the <code>.compute()</code> method to the task.</p> <pre><code>&gt;&gt;&gt; meandec = task.compute()\n&gt;&gt;&gt; meandec\n-18.433358575323904 &lt;Unit('degree')&gt;\n</code></pre> <p>As an example of calculating something more complicated than just <code>sum()</code>, let's do the usual \"poor man's projection\" via a 2D histogram.</p> <p>To do so, we use da.histogram2d() of dask, which is analogous to numpy.histogram2d(), except that it operates on a dask array. We discuss more advanced and interactive visualization methods here.</p> <pre><code>&gt;&gt;&gt; import dask.array as da\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; x = ds.data[\"l\"]\n&gt;&gt;&gt; y = ds.data[\"b\"]\n&gt;&gt;&gt; nbins = (360, 180)\n&gt;&gt;&gt; extent = [0.0, 360.0, -90.0, 90.0]\n&gt;&gt;&gt; xbins = np.linspace(*extent[:2], nbins[0] + 1)\n&gt;&gt;&gt; ybins = np.linspace(*extent[-2:], nbins[1] + 1)\n&gt;&gt;&gt; hist, xbins, ybins = da.histogram2d(x, y, bins=[xbins, ybins])\n&gt;&gt;&gt; im2d = hist.compute() #(1)!\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from matplotlib.colors import LogNorm\n&gt;&gt;&gt; plt.imshow(im2d.T, origin=\"lower\", norm=LogNorm(), extent=extent, interpolation=\"none\")\n&gt;&gt;&gt; plt.xlabel(\"l [deg]\")\n&gt;&gt;&gt; plt.ylabel(\"b [deg]\")\n&gt;&gt;&gt; plt.show()\n</code></pre> <ol> <li>The compute() on <code>im2d</code> results in a two-dimensional array which we can display.</li> </ol> <p></p> <p>Info</p> <p>Above image shows the histogram obtained for the full data set.</p>"},{"location":"tutorial/observations/#fits-files","title":"FITS files","text":"<p>Observations are often stored in FITS files. Support in scida is work-in-progress and requires the astropy package.  Here we show use of the SDSS DR16.</p> <p>SDSS DR16</p> <p>The SDSS DR16 redshift and classification file \"specObj-dr16.fits\" can be found here.</p> <pre><code>&gt;&gt;&gt; from scida import load\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; path = \"/virgotng/mpia/obs/SDSS/specObj-dr16.fits\"\n&gt;&gt;&gt; ds = load(path)\n&gt;&gt;&gt;\n&gt;&gt;&gt; cx = ds.data[\"CX\"].compute()\n&gt;&gt;&gt; cy = ds.data[\"CY\"].compute()\n&gt;&gt;&gt; cz = ds.data[\"CZ\"].compute()\n&gt;&gt;&gt; z = ds.data[\"Z\"].compute()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # order by redshift for scatter plotting\n&gt;&gt;&gt; idx = np.argsort(z)\n&gt;&gt;&gt;\n&gt;&gt;&gt; theta = np.arccos(cz.magnitude / np.sqrt(cx**2 + cy**2 + cz**2).magnitude)\n&gt;&gt;&gt; phi = np.arctan2(cy.magnitude,cx.magnitude)\n&gt;&gt;&gt;\n&gt;&gt;&gt; fig = plt.figure(figsize=(10,5))\n&gt;&gt;&gt; ax = fig.add_subplot(111, projection=\"aitoff\")\n&gt;&gt;&gt; ra = phi[idx]\n&gt;&gt;&gt; dec = -(theta-np.pi/2.0)\n&gt;&gt;&gt; sc = ax.scatter(ra, dec[idx], s=0.05, c=z[idx], rasterized=True)\n&gt;&gt;&gt; fig.colorbar(sc, label=\"redshift\")\n&gt;&gt;&gt; ax.set_xticklabels(['14h','16h','18h','20h','22h','0h','2h','4h','6h','8h','10h'])\n&gt;&gt;&gt; ax.set_xlabel(\"RA\")\n&gt;&gt;&gt; ax.set_ylabel(\"DEC\")\n&gt;&gt;&gt; ax.grid(True)\n&gt;&gt;&gt; plt.savefig(\"sdss_dr16.png\", dpi=150)\n&gt;&gt;&gt; plt.show()\n</code></pre> <p></p>"},{"location":"tutorial/simulations/","title":"Simulations","text":""},{"location":"tutorial/simulations/#getting-started","title":"Getting started","text":"<p>Tutorial dataset</p> <p>In the following, we will use a small test dataset from the TNG50 simulation. This is a cosmological galaxy formation simulation. This dataset is still a gigabyte in size and can be downloaded here. On the command line, you can download and extract the dataset with:</p> <p></p><pre><code>wget \"https://heibox.uni-heidelberg.de/f/dc65a8c75220477eb62d/?dl=1\" -O snapshot.tar.gz\ntar -xvf snapshot.tar.gz\n</code></pre> The snapshot will be extracted into a folder called snapdir_030.<p></p> <p>Note that analysis is not limited to simulations, but also observational data. Check Supported Datasets for an incomplete list of supported datasets and requirements for support of new datasets. A shorter tutorial for an observational dataset can be found here.</p> <p>This package is designed to aid in the efficient analysis of large simulations, such as cosmological (hydrodynamical) simulations of large-scale structure. Datasets are expected to be spatially unstructured, e.g. tessellation or particle based. Much of the functionality can also be used to process observational datasets.</p> <p>It uses the dask library to perform computations, which has several key advantages:</p> <ol> <li>very large datasets which cannot normally fit into memory can be analyzed,</li> <li>calculations can be automatically distributed onto parallel 'workers', across one or more nodes, to speed them up,</li> <li>we can create abstract graphs (\"recipes\", such as for derived quantities) and only evaluate on actual demand.</li> </ol>"},{"location":"tutorial/simulations/#loading-an-individual-dataset","title":"Loading an individual dataset","text":"<p>The first step is to choose an existing snapshot of a simulation. To start, we will intentionally select the output of TNG50-4 at redshift z=2.32, which is the lowest resolution version of TNG50, a suite for galaxy formation simulations in cosmological volumes. Choosing TNG50-4 means that the data size in the snapshot is small and easy to work with. We demonstrate how to work with larger data sets at a later stage.</p> <p>First, we load the dataset using the convenience function <code>load()</code> that will determine the appropriate dataset class for us:</p> Loading a dataset<pre><code>&gt;&gt;&gt; from scida import load\n&gt;&gt;&gt; ds = load(\"./snapdir_030\")\n&gt;&gt;&gt; ds.info() #(1)!\nclass: ArepoSnapshotWithUnitMixinAndCosmologyMixin\nsource: /vera/u/byrohlc/Downloads/snapdir_030\n=== Cosmological Simulation ===\nz = 2.32\ncosmology = FlatLambdaCDM(H0=67.74 km / (Mpc s), Om0=0.3089, Tcmb0=0.0 K, Neff=3.04, m_nu=None, Ob0=0.0486)\n===============================\n=== Unit-aware Dataset ===\n==========================\n=== data ===\n+ root (containers: 5)\n++ PartType0 (fields: 10, entries: 19124236)\n++ PartType1 (fields: 3, entries: 19683000)\n++ PartType3 (fields: 2, entries: 19683000)\n++ PartType4 (fields: 9, entries: 161401)\n++ PartType5 (fields: 20, entries: 2673)\n============\n</code></pre> <ol> <li>Call to receive some information about the loaded dataset.</li> </ol> <p>Specifying the load path</p> <p>Here, we passed the directory \"snapdir_030\" as the path to <code>load()</code>. In detail, it will depend on the desired dataset type to be loaded which path to pass to <code>load()</code>, but generally this is the base folder containing all relevant data. In above example, you could also use path=\"snapdir_030/snap_030.0.hdf5\", however this would only load this file's content, a fraction of all data.</p> <p>The dataset is now loaded, and we can inspect its contents, specifically its container and fields loaded. We can access the data in the dataset by using the <code>data</code> attribute, which is a dictionary of containers and fields.</p> <p>From above output we see that the dataset contains seven containers, each of which contains a number of fields. For this snapshot, the containers are PartType0, PartType1, PartType3, PartType4, PartType5, Group, and Subhalo. The containers represent gas cells, dark matter particles, stars, black holes, groups and subhalos, respectively. Each container contains fields that are specific to the container, such as the mass of the gas cells, the velocity of the dark matter particles, etc. The fields are described in the TNG50 documentation.</p> <p>The available fields of the container \"PartType0\" are:</p> Available fields of a given container<pre><code>&gt;&gt;&gt; ds.data[\"PartType0\"].keys() #(1)!\n['Coordinates',\n 'Density',\n 'ElectronAbundance',\n 'GFM_Metallicity',\n 'InternalEnergy',\n 'Masses',\n 'ParticleIDs',\n 'StarFormationRate',\n 'Temperature',\n 'Velocities']\n</code></pre> <ol> <li>For many simulation types aliases exist, such that \"gas\" can be used instead of \"PartType0\", \"stars\" instead of \"PartType4\", etc.</li> </ol> <p>Let's take a look at some field in this container:</p> Inspecting a field<pre><code>&gt;&gt;&gt; gas = ds.data[\"PartType0\"]\n&gt;&gt;&gt; gas[\"StarFormationRate\"]\n'dask.array&lt;mul, shape=(19124236,), dtype=float32, chunksize=(19124236,), chunktype=numpy.ndarray&gt; Msun / year'\n</code></pre> <p>The field is a dask array, which is a lazy array that will only be evaluated when needed. How these lazy arrays and their units work and are to be used will be explored in the next section.</p>"},{"location":"tutorial/simulations/#dask-arrays-and-units","title":"Dask arrays and units","text":""},{"location":"tutorial/simulations/#dask-arrays","title":"Dask arrays","text":"<p>Dask arrays are virtual entities that are only evaluated when needed. If you are unfamiliar with dask arrays, consider taking a look at this 3-minute introduction.</p> <p>They are not numpy arrays, but they can be converted to them, and have most of their functionality. Within dask, an internal task graph is created that holds the recipes how to construct the array from the underlying data.</p> <p>In our case, in fact, the snapshot and its fields are split across multiple files on disk (as for most large datasets), and virtually combined into a single dask array. The user does not need to worry about this as these operations take place in the background.</p> <p>For the TNG50-4 datasets, the first level of <code>ds.data</code> maps the different particle types (such as gas and dark matter), and the second level holds the different physical field arrays (such as density and ionization).</p> <p>In general, fields can be also be stored in flat or more nested structures, depending on the dataset.</p> <p>We can trigger the evaluation of the dask array by calling <code>compute()</code> on it:</p> Evaluating a dask array<pre><code>&gt;&gt;&gt; gas[\"StarFormationRate\"].compute()\n'[0.017970681190490723 0.0 0.016353357583284378 ... 0.0 0.0 0.0] Msun / year'\n</code></pre> <p>However, directly evaluating dask arrays is strongly discouraged for large datasets, as it will load the entire dataset into memory. Instead, we will reduce the datasize by running desired analysis/reduction within dask before calling compute(), which we present in the next section.</p>"},{"location":"tutorial/simulations/#units","title":"Units","text":"<p>If passing <code>units=True</code> (default) to <code>load()</code>, the dataset will be loaded with code units attached to all fields. These units are attached to each field / dask array. Units are provided via the pint package. See the pint documentation for more information. Also check out this page for more unit-related examples.</p> <p>In short, each field, that is represented by a modified dask array, has a magnitude (the dask array without any units attached) and a unit. These can be accessed via the <code>magnitude</code> and <code>units</code> attributes, respectively.</p> Accessing the magnitude and units of a field<pre><code>&gt;&gt;&gt; gas[\"Coordinates\"].magnitude\n'dask.array&lt;mul, shape=(19124236, 3), dtype=float32, chunksize=(11184810, 3), chunktype=numpy.ndarray&gt;'\n&gt;&gt;&gt; gas[\"Coordinates\"].units\n'code_length'\n</code></pre> <p>When defining derived fields from dask arrays, the correct units are automatically propagated to the new field, and dimensionality checks are performed. Importantly, the unit calculation is done immediately, thus allowing to directly see the resulting units and any dimensionality mismatches.</p>"},{"location":"tutorial/simulations/#analyzing-snapshot-data","title":"Analyzing snapshot data","text":""},{"location":"tutorial/simulations/#computing-a-simple-statistic-on-all-particles","title":"Computing a simple statistic on (all) particles","text":"<p>The fields in our snapshot object behave similar to actual numpy arrays.</p> <p>As a first simple example, let's calculate the total mass of gas cells in the entire simulation. Just as in numpy we can write</p> Calculating the total mass of gas cells<pre><code>&gt;&gt;&gt; masses = ds.data[\"PartType0\"][\"Masses\"]\n&gt;&gt;&gt; task = masses.sum()\n&gt;&gt;&gt; task\n'dask.array&lt;sum-aggregate, shape=(), dtype=float32, chunksize=(), chunktype=numpy.ndarray&gt; code_mass'\n</code></pre> <p>Note that all objects remain 'virtual': they are not calculated or loaded from disk, but are merely the required instructions, encoded into tasks.</p> <p>We can request a calculation of the actual operation(s) by applying the <code>.compute()</code> method to the task.</p> <pre><code>&gt;&gt;&gt; totmass = task.compute()\n&gt;&gt;&gt; totmass\n'57384.59375 code_mass'\n</code></pre> Converting units <p>We can easily convert the code units to something physically meaningful by using the <code>.to()</code> method of the task or its computed result:</p> Converting units<pre><code>&gt;&gt;&gt; totmass.to(\"Msun\")\n'8.3e15 Msun'\n</code></pre> <p>As an example of calculating something more complicated than just <code>sum()</code>, let's do the usual \"poor man's projection\" via a 2D histogram.</p> <p>To do so, we use da.histogram2d() of dask, which is analogous to numpy.histogram2d(), except that it operates on a dask array. We discuss more advanced and interactive visualization methods here.</p> <pre><code>&gt;&gt;&gt; import dask.array as da\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; coords = ds.data[\"PartType0\"][\"Coordinates\"]\n&gt;&gt;&gt; x = coords[:,0]\n&gt;&gt;&gt; y = coords[:,1]\n&gt;&gt;&gt; nbins = 512\n&gt;&gt;&gt; bins1d = np.linspace(0, ds.header[\"BoxSize\"], nbins+1)\n&gt;&gt;&gt; hist, xbins, ybins = da.histogram2d(x,y,bins=[bins1d,bins1d])\n&gt;&gt;&gt; im2d = hist.compute() #(1)!\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from matplotlib.colors import LogNorm\n&gt;&gt;&gt; fig = plt.figure(figsize=(6, 6))\n&gt;&gt;&gt; plt.imshow(im2d.T, norm=LogNorm(vmin=25, vmax=500), extent=[0, ds.header[\"BoxSize\"], 0, ds.header[\"BoxSize\"]], cmap=\"viridis\")\n&gt;&gt;&gt; plt.xlabel(\"x (ckpc/h)\")\n&gt;&gt;&gt; plt.ylabel(\"y (ckpc/h)\")\n&gt;&gt;&gt; plt.show()\n</code></pre> <ol> <li>The compute() on <code>im2d</code> results in a two-dimensional array which we can display.</li> </ol> <p></p>"},{"location":"tutorial/simulations/#catalogs","title":"Catalogs","text":"<p>Many cosmological simulations have a catalog of halos, subhalos, galaxies, etc.</p> <p>For AREPO/Gadget based simulations, we support use of this information. Find more find more information on how to use catalogs here.</p>"}]}