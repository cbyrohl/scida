{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Scida","text":"<p>scida is an out-of-the-box analysis tool for large astrophysical datasets, particularly cosmological and galaxy formation simulations using particles or unstructured meshes. This tool uses dask, allowing analysis to scale up from your personal computer to HPC resources and the cloud. Find some visual impressions here.</p> <p>           Quick start         </p> <p>Short tutorials introducing primary features for galaxy formation simulations and observational data.</p> <p>           API         </p> <p>Contains the reference of the API.</p> <p>           Developer guide         </p> <p>How to modify and contribute to the package.</p>"},{"location":"api_docs/","title":"API documentation","text":""},{"location":"api_docs/#api-documentation","title":"API documentation","text":""},{"location":"api_docs/#scida.config","title":"<code>config</code>","text":""},{"location":"api_docs/#scida.config.copy_defaultconfig","title":"<code>copy_defaultconfig(overwrite=False)</code>","text":"<p>Copy the configuration example to the user's home directory.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <p>Overwrite existing configuration file.</p> <code>False</code> Source code in <code>src/scida/config.py</code> <pre><code>def copy_defaultconfig(overwrite=False) -&gt; None:\n    \"\"\"\n    Copy the configuration example to the user's home directory.\n    Parameters\n    ----------\n    overwrite: bool\n        Overwrite existing configuration file.\n\n    Returns\n    -------\n\n    \"\"\"\n\n    path_user = os.path.expanduser(\"~\")\n    path_confdir = os.path.join(path_user, \".config/scida\")\n    if not os.path.exists(path_confdir):\n        os.makedirs(path_confdir, exist_ok=True)\n    path_conf = os.path.join(path_confdir, \"config.yaml\")\n    if os.path.exists(path_conf) and not overwrite:\n        raise ValueError(\"Configuration file already exists at '%s'\" % path_conf)\n    with importlib.resources.path(\"scida.configfiles\", \"config.yaml\") as fp:\n        with open(fp, \"r\") as file:\n            content = file.read()\n            with open(path_conf, \"w\") as newfile:\n                newfile.write(content)\n</code></pre>"},{"location":"api_docs/#scida.config.get_config","title":"<code>get_config(reload=False, update_global=True)</code>","text":"<p>Load the configuration from the default path.</p> <p>Parameters:</p> Name Type Description Default <code>reload</code> <code>bool</code> <p>Reload the configuration, even if it has already been loaded.</p> <code>False</code> <code>update_global</code> <p>Update the global configuration dictionary.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>The configuration dictionary.</p> Source code in <code>src/scida/config.py</code> <pre><code>def get_config(reload: bool = False, update_global=True) -&gt; dict:\n    \"\"\"\n    Load the configuration from the default path.\n    Parameters\n    ----------\n    reload: bool\n        Reload the configuration, even if it has already been loaded.\n    update_global: bool\n        Update the global configuration dictionary.\n\n    Returns\n    -------\n    dict\n        The configuration dictionary.\n\n    \"\"\"\n    global _conf\n    prefix = \"SCIDA_\"\n    envconf = {\n        k.replace(prefix, \"\").lower(): v\n        for k, v in os.environ.items()\n        if k.startswith(prefix)\n    }\n\n    # in any case, we make sure that there is some config in the default path.\n    path_confdir = _access_confdir()\n    path_conf = os.path.join(path_confdir, \"config.yaml\")\n\n    # next, we load the config from the default path, unless explicitly overridden.\n    path = envconf.pop(\"config_path\", None)\n    if path is None:\n        path = path_conf\n    if not reload and len(_conf) &gt; 0:\n        return _conf\n    config = get_config_fromfile(path)\n    if config.get(\"copied_default\", False):\n        print(\n            \"Warning! Using default configuration. Please adjust/replace in '%s'.\"\n            % path\n        )\n\n    config.update(**envconf)\n    if update_global:\n        _conf = config\n    return config\n</code></pre>"},{"location":"api_docs/#scida.config.get_config_fromfile","title":"<code>get_config_fromfile(resource)</code>","text":"<p>Load config from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>str</code> <p>The name of the resource or file path.</p> required Source code in <code>src/scida/config.py</code> <pre><code>def get_config_fromfile(resource: str) -&gt; Dict:\n    \"\"\"\n    Load config from a YAML file.\n    Parameters\n    ----------\n    resource\n        The name of the resource or file path.\n\n    Returns\n    -------\n\n    \"\"\"\n    if resource == \"\":\n        raise ValueError(\"Config name cannot be empty.\")\n    # order (in descending order of priority):\n    # 1. absolute path?\n    path = os.path.expanduser(resource)\n    if os.path.isabs(path):\n        with open(path, \"r\") as file:\n            conf = yaml.safe_load(file)\n        return conf\n    # 2. non-absolute path?\n    # 2.1. check ~/.config/scida/\n    bpath = os.path.expanduser(\"~/.config/scida\")\n    path = os.path.join(bpath, resource)\n    if os.path.isfile(path):\n        with open(path, \"r\") as file:\n            conf = yaml.safe_load(file)\n        return conf\n    # 2.2 check scida package resources\n    resource_path = \"scida.configfiles\"\n    resource_elements = resource.split(\"/\")\n    rname = resource_elements[-1]\n    if len(resource_elements) &gt; 1:\n        resource_path += \".\" + \".\".join(resource_elements[:-1])\n    with importlib.resources.path(resource_path, rname) as fp:\n        with open(fp, \"r\") as file:\n            conf = yaml.safe_load(file)\n    return conf\n</code></pre>"},{"location":"api_docs/#scida.config.get_config_fromfiles","title":"<code>get_config_fromfiles(paths, subconf_keys=None)</code>","text":"<p>Load and merge multiple YAML config files</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>List[str]</code> <p>Paths to the config files.</p> required <code>subconf_keys</code> <code>Optional[List[str]]</code> <p>The keys to the correct sub configuration within each config.</p> <code>None</code> Source code in <code>src/scida/config.py</code> <pre><code>def get_config_fromfiles(paths: List[str], subconf_keys: Optional[List[str]] = None):\n    \"\"\"\n    Load and merge multiple YAML config files\n    Parameters\n    ----------\n    paths\n        Paths to the config files.\n    subconf_keys\n        The keys to the correct sub configuration within each config.\n\n    Returns\n    -------\n\n    \"\"\"\n    confs = []\n    for path in paths:\n        confs.append(get_config_fromfile(path))\n    conf = {}\n    for confdict in confs:\n        conf = merge_dicts_recursively(conf, confdict)\n    return conf\n</code></pre>"},{"location":"api_docs/#scida.config.merge_dicts_recursively","title":"<code>merge_dicts_recursively(dict_a, dict_b, path=None, mergefunc_keys=None, mergefunc_values=None)</code>","text":"<p>Merge two dictionaries recursively.</p> <p>Parameters:</p> Name Type Description Default <code>dict_a</code> <code>Dict</code> <p>The first dictionary.</p> required <code>dict_b</code> <code>Dict</code> <p>The second dictionary.</p> required <code>path</code> <code>Optional[List]</code> <p>The path to the current node.</p> <code>None</code> <code>mergefunc_keys</code> <code>Optional[callable]</code> <p>The function to use for merging. If None, we recursively enter the dictionary.</p> <code>None</code> <code>mergefunc_values</code> <code>Optional[callable]</code> <p>The function to use for merging. If None, collisions will raise an exception.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> Source code in <code>src/scida/config.py</code> <pre><code>def merge_dicts_recursively(\n    dict_a: Dict,\n    dict_b: Dict,\n    path: Optional[List] = None,\n    mergefunc_keys: Optional[callable] = None,\n    mergefunc_values: Optional[callable] = None,\n) -&gt; Dict:\n    \"\"\"\n    Merge two dictionaries recursively.\n    Parameters\n    ----------\n    dict_a\n        The first dictionary.\n    dict_b\n        The second dictionary.\n    path\n        The path to the current node.\n    mergefunc_keys: callable\n        The function to use for merging. If None, we recursively enter the dictionary.\n    mergefunc_values: callable\n        The function to use for merging. If None, collisions will raise an exception.\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    if path is None:\n        path = []\n    for key in dict_b:\n        if key in dict_a:\n            if mergefunc_keys is not None:\n                dict_a[key] = mergefunc_keys(dict_a[key], dict_b[key])\n            elif isinstance(dict_a[key], dict) and isinstance(dict_b[key], dict):\n                merge_dicts_recursively(\n                    dict_a[key],\n                    dict_b[key],\n                    path + [str(key)],\n                    mergefunc_keys=mergefunc_keys,\n                    mergefunc_values=mergefunc_values,\n                )\n            elif dict_a[key] == dict_b[key]:\n                pass  # same leaf value\n            else:\n                if mergefunc_values is not None:\n                    dict_a[key] = mergefunc_values(dict_a[key], dict_b[key])\n                else:\n                    raise Exception(\"Conflict at %s\" % \".\".join(path + [str(key)]))\n        else:\n            dict_a[key] = dict_b[key]\n    return dict_a\n</code></pre>"},{"location":"api_docs/#scida.convenience","title":"<code>convenience</code>","text":""},{"location":"api_docs/#scida.convenience.download_and_extract","title":"<code>download_and_extract(url, path, progressbar=True, overwrite=False)</code>","text":"<p>Download and extract a file from a given url.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The url to download from.</p> required <code>path</code> <code>Path</code> <p>The path to download to.</p> required <code>progressbar</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite an existing file.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the downloaded and extracted file(s).</p> Source code in <code>src/scida/convenience.py</code> <pre><code>def download_and_extract(\n    url: str, path: pathlib.Path, progressbar: bool = True, overwrite: bool = False\n):\n    \"\"\"\n    Download and extract a file from a given url.\n    Parameters\n    ----------\n    url: str\n        The url to download from.\n    path: pathlib.Path\n        The path to download to.\n    progressbar: bool\n        Whether to show a progress bar.\n    overwrite: bool\n        Whether to overwrite an existing file.\n    Returns\n    -------\n    str\n        The path to the downloaded and extracted file(s).\n    \"\"\"\n    if path.exists() and not overwrite:\n        raise ValueError(\"Target path '%s' already exists.\" % path)\n    with requests.get(url, stream=True) as r:\n        r.raise_for_status()\n        totlength = int(r.headers.get(\"content-length\", 0))\n        lread = 0\n        t1 = time.time()\n        with open(path, \"wb\") as f:\n            for chunk in r.iter_content(chunk_size=2**22):  # chunks of 4MB\n                t2 = time.time()\n                f.write(chunk)\n                lread += len(chunk)\n                if progressbar:\n                    rate = (lread / 2**20) / (t2 - t1)\n                    sys.stdout.write(\n                        \"\\rDownloaded %.2f/%.2f Megabytes (%.2f%%, %.2f MB/s)\"\n                        % (\n                            lread / 2**20,\n                            totlength / 2**20,\n                            100.0 * lread / totlength,\n                            rate,\n                        )\n                    )\n                    sys.stdout.flush()\n        sys.stdout.write(\"\\n\")\n    tar = tarfile.open(path, \"r:gz\")\n    for t in tar:\n        if t.isdir():\n            t.mode = int(\"0755\", base=8)\n        else:\n            t.mode = int(\"0644\", base=8)\n    tar.extractall(path.parents[0])\n    foldername = tar.getmembers()[0].name  # parent folder of extracted tar.gz\n    tar.close()\n    os.remove(path)\n    return os.path.join(path.parents[0], foldername)\n</code></pre>"},{"location":"api_docs/#scida.convenience.find_path","title":"<code>find_path(path, overwrite=False)</code>","text":"<p>Find path to dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> required <code>overwrite</code> <p>Only for remote datasets. Whether to overwrite an existing download.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> Source code in <code>src/scida/convenience.py</code> <pre><code>def find_path(path, overwrite=False) -&gt; str:\n    \"\"\"\n    Find path to dataset.\n    Parameters\n    ----------\n    path: str\n    overwrite: bool\n        Only for remote datasets. Whether to overwrite an existing download.\n\n    Returns\n    -------\n    str\n\n    \"\"\"\n    config = get_config()\n    path = os.path.expanduser(path)\n    if os.path.exists(path):\n        # datasets on disk\n        pass\n    elif len(path.split(\":\")) &gt; 1:\n        # check different alternative backends\n        databackend = path.split(\"://\")[0]\n        dataname = path.split(\"://\")[1]\n        if databackend in [\"http\", \"https\"]:\n            # dataset on the internet\n            savepath = config.get(\"download_path\", None)\n            if savepath is None:\n                print(\n                    \"Have not specified 'download_path' in config. Using 'cache_path' instead.\"\n                )\n                savepath = config.get(\"cache_path\")\n            savepath = os.path.expanduser(savepath)\n            savepath = pathlib.Path(savepath)\n            savepath.mkdir(parents=True, exist_ok=True)\n            urlhash = str(\n                int(hashlib.sha256(path.encode(\"utf-8\")).hexdigest(), 16) % 10**8\n            )\n            savepath = savepath / (\"download\" + urlhash)\n            filename = \"archive.tar.gz\"\n            if not savepath.exists():\n                os.makedirs(savepath, exist_ok=True)\n            elif overwrite:\n                # delete all files in folder\n                for f in os.listdir(savepath):\n                    fp = os.path.join(savepath, f)\n                    if os.path.isfile(fp):\n                        os.unlink(fp)\n                    else:\n                        shutil.rmtree(fp)\n            foldercontent = [f for f in savepath.glob(\"*\")]\n            if len(foldercontent) == 0:\n                savepath = savepath / filename\n                extractpath = download_and_extract(\n                    path, savepath, progressbar=True, overwrite=overwrite\n                )\n            else:\n                extractpath = savepath\n            extractpath = pathlib.Path(extractpath)\n\n            # count folders in savepath\n            nfolders = len([f for f in extractpath.glob(\"*\") if f.is_dir()])\n            nobjects = len([f for f in extractpath.glob(\"*\") if f.is_dir()])\n            if nobjects == 1 and nfolders == 1:\n                extractpath = (\n                    extractpath / [f for f in extractpath.glob(\"*\") if f.is_dir()][0]\n                )\n            path = extractpath\n        elif databackend == \"testdata\":\n            path = get_testdata(dataname)\n        else:\n            # potentially custom dataset.\n            resources = config.get(\"resources\", {})\n            if databackend not in resources:\n                raise ValueError(\"Unknown resource '%s'\" % databackend)\n            r = resources[databackend]\n            if dataname not in r:\n                raise ValueError(\n                    \"Unknown dataset '%s' in resource '%s'\" % (dataname, databackend)\n                )\n            path = os.path.expanduser(r[dataname][\"path\"])\n    else:\n        found = False\n        if \"datafolders\" in config:\n            for folder in config[\"datafolders\"]:\n                folder = os.path.expanduser(folder)\n                if os.path.exists(os.path.join(folder, path)):\n                    path = os.path.join(folder, path)\n                    found = True\n                    break\n        if not found:\n            raise ValueError(\"Specified path '%s' unknown.\" % path)\n    return path\n</code></pre>"},{"location":"api_docs/#scida.convenience.get_dataset_by_name","title":"<code>get_dataset_by_name(name)</code>","text":"<p>Get dataset name from alias or name found in the configuration files.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name or alias of dataset.</p> required <p>Returns:</p> Type Description <code>str</code> Source code in <code>src/scida/convenience.py</code> <pre><code>def get_dataset_by_name(name: str) -&gt; Optional[str]:\n    \"\"\"\n    Get dataset name from alias or name found in the configuration files.\n    Parameters\n    ----------\n    name: str\n        Name or alias of dataset.\n\n    Returns\n    -------\n    str\n    \"\"\"\n    dname = None\n    c = get_config()\n    if \"datasets\" not in c:\n        return dname\n    datasets = copy.deepcopy(c[\"datasets\"])\n    if name in datasets:\n        dname = name\n    else:\n        # could still be an alias\n        for k, v in datasets.items():\n            if \"aliases\" not in v:\n                continue\n            if name in v[\"aliases\"]:\n                dname = k\n                break\n    return dname\n</code></pre>"},{"location":"api_docs/#scida.convenience.get_testdata","title":"<code>get_testdata(name)</code>","text":"<p>Get path to test data identifier.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of test data.</p> required <p>Returns:</p> Type Description <code>str</code> Source code in <code>src/scida/convenience.py</code> <pre><code>def get_testdata(name: str) -&gt; str:\n    \"\"\"\n    Get path to test data identifier.\n    Parameters\n    ----------\n    name: str\n        Name of test data.\n    Returns\n    -------\n    str\n    \"\"\"\n    config = get_config()\n    tdpath = config.get(\"testdata_path\", None)\n    if tdpath is None:\n        raise ValueError(\"Test data directory not specified in configuration\")\n    if not os.path.isdir(tdpath):\n        raise ValueError(\"Invalid test data path\")\n    res = {f: os.path.join(tdpath, f) for f in os.listdir(tdpath)}\n    if name not in res.keys():\n        raise ValueError(\"Specified test data not available.\")\n    return res[name]\n</code></pre>"},{"location":"api_docs/#scida.customs","title":"<code>customs</code>","text":""},{"location":"api_docs/#scida.customs.arepo","title":"<code>arepo</code>","text":""},{"location":"api_docs/#scida.customs.arepo.MTNG","title":"<code>MTNG</code>","text":""},{"location":"api_docs/#scida.customs.arepo.MTNG.dataset","title":"<code>dataset</code>","text":""},{"location":"api_docs/#scida.customs.arepo.TNGcluster","title":"<code>TNGcluster</code>","text":""},{"location":"api_docs/#scida.customs.arepo.TNGcluster.dataset","title":"<code>dataset</code>","text":""},{"location":"api_docs/#scida.customs.arepo.dataset","title":"<code>dataset</code>","text":""},{"location":"api_docs/#scida.customs.arepo.dataset.ArepoSnapshot","title":"<code>ArepoSnapshot</code>","text":"<p>             Bases: <code>SpatialCartesian3DMixin</code>, <code>GadgetStyleSnapshot</code></p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>class ArepoSnapshot(SpatialCartesian3DMixin, GadgetStyleSnapshot):\n    _fileprefix_catalog = \"groups\"\n\n    def __init__(self, path, chunksize=\"auto\", catalog=None, **kwargs) -&gt; None:\n        self.iscatalog = kwargs.pop(\"iscatalog\", False)\n        self.header = {}\n        self.config = {}\n        self._defaultunitfiles: List[str] = [\"units/gadget_cosmological.yaml\"]\n        self.parameters = {}\n        self._grouplengths = {}\n        self._subhalolengths = {}\n        # not needed for group catalogs as entries are back-to-back there, we will provide a property for this\n        self._subhalooffsets = {}\n        self.misc = {}  # for storing misc info\n        prfx = kwargs.pop(\"fileprefix\", None)\n        if prfx is None:\n            prfx = self._get_fileprefix(path)\n        super().__init__(path, chunksize=chunksize, fileprefix=prfx, **kwargs)\n\n        self.catalog = catalog\n        if not self.iscatalog:\n            if self.catalog is None:\n                self.discover_catalog()\n                # try to discover group catalog in parent directories.\n            if self.catalog == \"none\":\n                pass  # this string can be set to explicitly disable catalog\n            elif self.catalog:\n                catalog_cls = kwargs.get(\"catalog_cls\", None)\n                self.load_catalog(kwargs, catalog_cls=catalog_cls)\n\n        # add aliases\n        aliases = dict(\n            PartType0=[\"gas\", \"baryons\"],\n            PartType1=[\"dm\", \"dark matter\"],\n            PartType2=[\"lowres\", \"lowres dm\"],\n            PartType3=[\"tracer\", \"tracers\"],\n            PartType4=[\"stars\"],\n            PartType5=[\"bh\", \"black holes\"],\n        )\n        for k, lst in aliases.items():\n            if k not in self.data:\n                continue\n            for v in lst:\n                self.data.add_alias(v, k)\n\n        # set metadata\n        self._set_metadata()\n\n        # add some default fields\n        self.data.merge(fielddefs)\n\n    def load_catalog(self, kwargs, catalog_cls=None):\n        virtualcache = False  # copy catalog for better performance\n        catalog_kwargs = kwargs.get(\"catalog_kwargs\", {})\n        catalog_kwargs[\"overwritecache\"] = kwargs.get(\"overwritecache\", False)\n        # fileprefix = catalog_kwargs.get(\"fileprefix\", self._fileprefix_catalog)\n        prfx = self._get_fileprefix(self.catalog)\n\n        # explicitly need to create unitaware class for catalog as needed\n        # TODO: should just be determined from mixins of parent?\n        if catalog_cls is None:\n            cls = ArepoCatalog\n        else:\n            cls = catalog_cls\n        withunits = kwargs.get(\"units\", False)\n        mixins = []\n        if withunits:\n            mixins += [UnitMixin]\n\n        other_mixins = _determine_mixins(path=self.path)\n        mixins += other_mixins\n        cls = create_MixinDataset(cls, mixins)\n\n        ureg = None\n        if hasattr(self, \"ureg\"):\n            ureg = self.ureg\n\n        self.catalog = cls(\n            self.catalog,\n            virtualcache=virtualcache,\n            fileprefix=prfx,\n            units=self.withunits,\n            ureg=ureg,\n        )\n        if \"Redshift\" in self.catalog.header and \"Redshift\" in self.header:\n            z_catalog = self.catalog.header[\"Redshift\"]\n            z_snap = self.header[\"Redshift\"]\n            if not np.isclose(z_catalog, z_snap):\n                raise ValueError(\n                    \"Redshift mismatch between snapshot and catalog: \"\n                    f\"{z_snap:.2f} vs {z_catalog:.2f}\"\n                )\n\n        # merge data\n        self.merge_data(self.catalog)\n\n        # first snapshots often do not have groups\n        if \"Group\" in self.catalog.data:\n            ngkeys = self.catalog.data[\"Group\"].keys()\n            if len(ngkeys) &gt; 0:\n                self.add_catalogIDs()\n\n        # merge hints from snap and catalog\n        self.merge_hints(self.catalog)\n\n    @classmethod\n    def validate_path(\n        cls, path: Union[str, os.PathLike], *args, **kwargs\n    ) -&gt; CandidateStatus:\n        valid = super().validate_path(path, *args, **kwargs)\n        if valid.value &gt; CandidateStatus.MAYBE.value:\n            valid = CandidateStatus.MAYBE\n        else:\n            return valid\n        # Arepo has no dedicated attribute to identify such runs.\n        # lets just query a bunch of attributes that are present for arepo runs\n        metadata_raw = load_metadata(path, **kwargs)\n        matchingattrs = True\n        matchingattrs &amp;= \"Git_commit\" in metadata_raw[\"/Header\"]\n        # not existent for any arepo run?\n        matchingattrs &amp;= \"Compactify_Version\" not in metadata_raw[\"/Header\"]\n\n        if matchingattrs:\n            valid = CandidateStatus.MAYBE\n\n        return valid\n\n    @ArepoSelector()\n    def return_data(self):\n        \"\"\"\n        Return data.\n        Returns\n        -------\n\n        \"\"\"\n        return super().return_data()\n\n    def discover_catalog(self):\n        \"\"\"\n        Discover the group catalog given the current path\n        Returns\n        -------\n\n        \"\"\"\n        p = str(self.path)\n        # order of candidates matters. For Illustris \"groups\" must precede \"fof_subhalo_tab\"\n        candidates = [\n            p.replace(\"snapshot\", \"group\"),\n            p.replace(\"snapshot\", \"groups\"),\n            p.replace(\"snapdir\", \"groups\").replace(\"snap\", \"groups\"),\n            p.replace(\"snapdir\", \"groups\").replace(\"snap\", \"fof_subhalo_tab\"),\n        ]\n        for candidate in candidates:\n            if not os.path.exists(candidate):\n                continue\n            if candidate == self.path:\n                continue\n            self.catalog = candidate\n            break\n\n    def register_field(self, parttype: str, name: str = None, construct: bool = False):\n        \"\"\"\n        Register a field.\n        Parameters\n        ----------\n        parttype: str\n            name of particle type\n        name: str\n            name of field\n        construct: bool\n            construct field immediately\n\n        Returns\n        -------\n\n        \"\"\"\n        num = part_type_num(parttype)\n        if construct:  # TODO: introduce (immediate) construct option later\n            raise NotImplementedError\n        if num == -1:  # TODO: all particle species\n            key = \"all\"\n            raise NotImplementedError\n        elif isinstance(num, int):\n            key = \"PartType\" + str(num)\n        else:\n            key = parttype\n        return super().register_field(key, name=name)\n\n    def add_catalogIDs(self) -&gt; None:\n        \"\"\"\n        Add field for halo and subgroup IDs for all particle types.\n        Returns\n        -------\n\n        \"\"\"\n        # TODO: make these delayed objects and properly pass into (delayed?) numba functions:\n        # https://docs.dask.org/en/stable/delayed-best-practices.html#avoid-repeatedly-putting-large-inputs-into-delayed-calls\n\n        maxint = np.iinfo(np.int64).max\n        self.misc[\"unboundID\"] = maxint\n\n        # Group ID\n        if \"Group\" not in self.data:  # can happen for empty catalogs\n            for key in self.data:\n                if not (key.startswith(\"PartType\")):\n                    continue\n                uid = self.data[key][\"uid\"]\n                self.data[key][\"GroupID\"] = self.misc[\"unboundID\"] * da.ones_like(\n                    uid, dtype=np.int64\n                )\n                self.data[key][\"SubhaloID\"] = self.misc[\"unboundID\"] * da.ones_like(\n                    uid, dtype=np.int64\n                )\n            return\n\n        glen = self.data[\"Group\"][\"GroupLenType\"]\n        ngrp = glen.shape[0]\n        da_halocelloffsets = da.concatenate(\n            [\n                np.zeros((1, 6), dtype=np.int64),\n                da.cumsum(glen, axis=0, dtype=np.int64),\n            ]\n        )\n        # remove last entry to match shapematch shape\n        self.data[\"Group\"][\"GroupOffsetsType\"] = da_halocelloffsets[:-1].rechunk(\n            glen.chunks\n        )\n        halocelloffsets = da_halocelloffsets.rechunk(-1)\n\n        index_unbound = self.misc[\"unboundID\"]\n\n        for key in self.data:\n            if not (key.startswith(\"PartType\")):\n                continue\n            num = int(key[-1])\n            if \"uid\" not in self.data[key]:\n                continue  # can happen for empty containers\n            gidx = self.data[key][\"uid\"]\n            hidx = compute_haloindex(\n                gidx, halocelloffsets[:, num], index_unbound=index_unbound\n            )\n            self.data[key][\"GroupID\"] = hidx\n\n        # Subhalo ID\n        if \"Subhalo\" not in self.data:  # can happen for empty catalogs\n            for key in self.data:\n                if not (key.startswith(\"PartType\")):\n                    continue\n                self.data[key][\"SubhaloID\"] = -1 * da.ones_like(\n                    da[key][\"uid\"], dtype=np.int64\n                )\n            return\n\n        shnr_attr = \"SubhaloGrNr\"\n        if shnr_attr not in self.data[\"Subhalo\"]:\n            shnr_attr = \"SubhaloGroupNr\"  # what MTNG does\n        if shnr_attr not in self.data[\"Subhalo\"]:\n            raise ValueError(\n                f\"Could not find 'SubhaloGrNr' or 'SubhaloGroupNr' in {self.catalog}\"\n            )\n\n        subhalogrnr = self.data[\"Subhalo\"][shnr_attr]\n        subhalocellcounts = self.data[\"Subhalo\"][\"SubhaloLenType\"]\n\n        # remove \"units\" for numba funcs\n        if hasattr(subhalogrnr, \"magnitude\"):\n            subhalogrnr = subhalogrnr.magnitude\n        if hasattr(subhalocellcounts, \"magnitude\"):\n            subhalocellcounts = subhalocellcounts.magnitude\n\n        grp = self.data[\"Group\"]\n        if \"GroupFirstSub\" not in grp or \"GroupNsubs\" not in grp:\n            # if not provided, we calculate:\n            # \"GroupFirstSub\": First subhalo index for each halo\n            # \"GroupNsubs\": Number of subhalos for each halo\n            dlyd = delayed(get_shcounts_shcells)(subhalogrnr, ngrp)\n            grp[\"GroupFirstSub\"] = dask.compute(dlyd[1])[0]\n            grp[\"GroupNsubs\"] = dask.compute(dlyd[0])[0]\n\n        # remove \"units\" for numba funcs\n        grpfirstsub = grp[\"GroupFirstSub\"]\n        if hasattr(grpfirstsub, \"magnitude\"):\n            grpfirstsub = grpfirstsub.magnitude\n        grpnsubs = grp[\"GroupNsubs\"]\n        if hasattr(grpnsubs, \"magnitude\"):\n            grpnsubs = grpnsubs.magnitude\n\n        for key in self.data:\n            if not (key.startswith(\"PartType\")):\n                continue\n            num = int(key[-1])\n            pdata = self.data[key]\n            if \"uid\" not in self.data[key]:\n                continue  # can happen for empty containers\n            gidx = pdata[\"uid\"]\n\n            # we need to make other dask arrays delayed,\n            # map_block does not incorrectly infer output shape from these\n            halocelloffsets_dlyd = delayed(halocelloffsets[:, num])\n            grpfirstsub_dlyd = delayed(grpfirstsub)\n            grpnsubs_dlyd = delayed(grpnsubs)\n            subhalocellcounts_dlyd = delayed(subhalocellcounts[:, num])\n\n            sidx = compute_localsubhaloindex(\n                gidx,\n                halocelloffsets_dlyd,\n                grpfirstsub_dlyd,\n                grpnsubs_dlyd,\n                subhalocellcounts_dlyd,\n                index_unbound=index_unbound,\n            )\n\n            pdata[\"LocalSubhaloID\"] = sidx\n\n            # reconstruct SubhaloID from Group's GroupFirstSub and LocalSubhaloID\n            # should be easier to do it directly, but quicker to write down like this:\n\n            # calculate first subhalo of each halo that a particle belongs to\n            self.add_groupquantity_to_particles(\"GroupFirstSub\", parttype=key)\n            pdata[\"SubhaloID\"] = pdata[\"GroupFirstSub\"] + pdata[\"LocalSubhaloID\"]\n            pdata[\"SubhaloID\"] = da.where(\n                pdata[\"SubhaloID\"] == index_unbound, index_unbound, pdata[\"SubhaloID\"]\n            )\n\n    @computedecorator\n    def map_group_operation(\n        self,\n        func,\n        cpucost_halo=1e4,\n        nchunks_min=None,\n        chunksize_bytes=None,\n        nmax=None,\n        idxlist=None,\n        objtype=\"halo\",\n    ):\n        \"\"\"\n        Apply a function to each halo in the catalog.\n\n        Parameters\n        ----------\n        objtype: str\n            Type of object to process. Can be \"halo\" or \"subhalo\". Default: \"halo\"\n        idxlist: Optional[np.ndarray]\n            List of halo indices to process. If not provided, all halos are processed.\n        func: function\n            Function to apply to each halo. Must take a dictionary of arrays as input.\n        cpucost_halo:\n            \"CPU cost\" of processing a single halo. This is a relative value to the processing time per input particle\n            used for calculating the dask chunks. Default: 1e4\n        nchunks_min: Optional[int]\n            Minimum number of particles in a halo to process it. Default: None\n        chunksize_bytes: Optional[int]\n        nmax: Optional[int]\n            Only process the first nmax halos.\n        Returns\n        -------\n\n        \"\"\"\n        dfltkwargs = get_kwargs(func)\n        fieldnames = dfltkwargs.get(\"fieldnames\", None)\n        if fieldnames is None:\n            fieldnames = get_args(func)\n        parttype = dfltkwargs.get(\"parttype\", \"PartType0\")\n        entry_nbytes_in = np.sum([self.data[parttype][f][0].nbytes for f in fieldnames])\n        objtype = grp_type_str(objtype)\n        if objtype == \"halo\":\n            lengths = self.get_grouplengths(parttype=parttype)\n            offsets = self.get_groupoffsets(parttype=parttype)\n        elif objtype == \"subhalo\":\n            lengths = self.get_subhalolengths(parttype=parttype)\n            offsets = self.get_subhalooffsets(parttype=parttype)\n        else:\n            raise ValueError(f\"objtype must be 'halo' or 'subhalo', not {objtype}\")\n        arrdict = self.data[parttype]\n        return map_group_operation(\n            func,\n            offsets,\n            lengths,\n            arrdict,\n            cpucost_halo=cpucost_halo,\n            nchunks_min=nchunks_min,\n            chunksize_bytes=chunksize_bytes,\n            entry_nbytes_in=entry_nbytes_in,\n            nmax=nmax,\n            idxlist=idxlist,\n        )\n\n    def add_groupquantity_to_particles(self, name, parttype=\"PartType0\"):\n        pdata = self.data[parttype]\n        assert (\n            name not in pdata\n        )  # we simply map the name from Group to Particle for now. Should work (?)\n        glen = self.data[\"Group\"][\"GroupLenType\"]\n        da_halocelloffsets = da.concatenate(\n            [np.zeros((1, 6), dtype=np.int64), da.cumsum(glen, axis=0)]\n        )\n        if \"GroupOffsetsType\" not in self.data[\"Group\"]:\n            self.data[\"Group\"][\"GroupOffsetsType\"] = da_halocelloffsets[:-1].rechunk(\n                glen.chunks\n            )  # remove last entry to match shape\n        halocelloffsets = da_halocelloffsets.compute()\n\n        gidx = pdata[\"uid\"]\n        num = int(parttype[-1])\n        hquantity = compute_haloquantity(\n            gidx, halocelloffsets[:, num], self.data[\"Group\"][name]\n        )\n        pdata[name] = hquantity\n\n    def get_grouplengths(self, parttype=\"PartType0\"):\n        \"\"\"Get the total number of particles of a given type in all halos.\"\"\"\n        pnum = part_type_num(parttype)\n        ptype = \"PartType%i\" % pnum\n        if ptype not in self._grouplengths:\n            lengths = self.data[\"Group\"][\"GroupLenType\"][:, pnum].compute()\n            if isinstance(lengths, pint.Quantity):\n                lengths = lengths.magnitude\n            self._grouplengths[ptype] = lengths\n        return self._grouplengths[ptype]\n\n    def get_groupoffsets(self, parttype=\"PartType0\"):\n        if parttype not in self._grouplengths:\n            # need to calculate group lengths first\n            self.get_grouplengths(parttype=parttype)\n        return self._groupoffsets[parttype]\n\n    @property\n    def _groupoffsets(self):\n        lengths = self._grouplengths\n        offsets = {\n            k: np.concatenate([[0], np.cumsum(v)[:-1]]) for k, v in lengths.items()\n        }\n        return offsets\n\n    def get_subhalolengths(self, parttype=\"PartType0\"):\n        \"\"\"Get the total number of particles of a given type in all halos.\"\"\"\n        pnum = part_type_num(parttype)\n        ptype = \"PartType%i\" % pnum\n        if ptype in self._subhalolengths:\n            return self._subhalolengths[ptype]\n        lengths = self.data[\"Subhalo\"][\"SubhaloLenType\"][:, pnum].compute()\n        if isinstance(lengths, pint.Quantity):\n            lengths = lengths.magnitude\n        self._subhalolengths[ptype] = lengths\n        return self._subhalolengths[ptype]\n\n    def get_subhalooffsets(self, parttype=\"PartType0\"):\n        pnum = part_type_num(parttype)\n        ptype = \"PartType%i\" % pnum\n        if ptype in self._subhalooffsets:\n            return self._subhalooffsets[ptype]  # use cached result\n        goffsets = self.get_groupoffsets(ptype)\n        shgrnr = self.data[\"Subhalo\"][\"SubhaloGrNr\"]\n        # calculate the index of the first particle for the central subhalo of each subhalos's parent halo\n        shoffset_central = goffsets[shgrnr]\n\n        grpfirstsub = self.data[\"Group\"][\"GroupFirstSub\"]\n        shlens = self.get_subhalolengths(ptype)\n        shoffsets = np.concatenate([[0], np.cumsum(shlens)[:-1]])\n\n        # particle offset for the first subhalo of each group that a subhalo belongs to\n        shfirstshoffset = shoffsets[grpfirstsub[shgrnr]]\n\n        # \"LocalSubhaloOffset\": particle offset of each subhalo in the parent group\n        shoffset_local = shoffsets - shfirstshoffset\n\n        # \"SubhaloOffset\": particle offset of each subhalo in the simulation\n        offsets = shoffset_central + shoffset_local\n\n        self._subhalooffsets[ptype] = offsets\n\n        return offsets\n\n    def grouped(\n        self,\n        fields: Union[str, da.Array, List[str], Dict[str, da.Array]] = \"\",\n        parttype=\"PartType0\",\n        objtype=\"halo\",\n    ):\n        inputfields = None\n        if isinstance(fields, str):\n            if fields == \"\":  # if nothing is specified, we pass all we have.\n                arrdict = self.data[parttype]\n            else:\n                arrdict = dict(field=self.data[parttype][fields])\n                inputfields = [fields]\n        elif isinstance(fields, da.Array) or isinstance(fields, pint.Quantity):\n            arrdict = dict(daskarr=fields)\n            inputfields = [fields.name]\n        elif isinstance(fields, list):\n            arrdict = {k: self.data[parttype][k] for k in fields}\n            inputfields = fields\n        elif isinstance(fields, dict):\n            arrdict = {}\n            arrdict.update(**fields)\n            inputfields = list(arrdict.keys())\n        else:\n            raise ValueError(\"Unknown input type '%s'.\" % type(fields))\n        objtype = grp_type_str(objtype)\n        if objtype == \"halo\":\n            offsets = self.get_groupoffsets(parttype=parttype)\n            lengths = self.get_grouplengths(parttype=parttype)\n        elif objtype == \"subhalo\":\n            offsets = self.get_subhalooffsets(parttype=parttype)\n            lengths = self.get_subhalolengths(parttype=parttype)\n        else:\n            raise ValueError(\"Unknown object type '%s'.\" % objtype)\n\n        gop = GroupAwareOperation(\n            offsets,\n            lengths,\n            arrdict,\n            inputfields=inputfields,\n        )\n        return gop\n</code></pre>"},{"location":"api_docs/#scida.customs.arepo.dataset.ArepoSnapshot.add_catalogIDs","title":"<code>add_catalogIDs()</code>","text":"<p>Add field for halo and subgroup IDs for all particle types.</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def add_catalogIDs(self) -&gt; None:\n    \"\"\"\n    Add field for halo and subgroup IDs for all particle types.\n    Returns\n    -------\n\n    \"\"\"\n    # TODO: make these delayed objects and properly pass into (delayed?) numba functions:\n    # https://docs.dask.org/en/stable/delayed-best-practices.html#avoid-repeatedly-putting-large-inputs-into-delayed-calls\n\n    maxint = np.iinfo(np.int64).max\n    self.misc[\"unboundID\"] = maxint\n\n    # Group ID\n    if \"Group\" not in self.data:  # can happen for empty catalogs\n        for key in self.data:\n            if not (key.startswith(\"PartType\")):\n                continue\n            uid = self.data[key][\"uid\"]\n            self.data[key][\"GroupID\"] = self.misc[\"unboundID\"] * da.ones_like(\n                uid, dtype=np.int64\n            )\n            self.data[key][\"SubhaloID\"] = self.misc[\"unboundID\"] * da.ones_like(\n                uid, dtype=np.int64\n            )\n        return\n\n    glen = self.data[\"Group\"][\"GroupLenType\"]\n    ngrp = glen.shape[0]\n    da_halocelloffsets = da.concatenate(\n        [\n            np.zeros((1, 6), dtype=np.int64),\n            da.cumsum(glen, axis=0, dtype=np.int64),\n        ]\n    )\n    # remove last entry to match shapematch shape\n    self.data[\"Group\"][\"GroupOffsetsType\"] = da_halocelloffsets[:-1].rechunk(\n        glen.chunks\n    )\n    halocelloffsets = da_halocelloffsets.rechunk(-1)\n\n    index_unbound = self.misc[\"unboundID\"]\n\n    for key in self.data:\n        if not (key.startswith(\"PartType\")):\n            continue\n        num = int(key[-1])\n        if \"uid\" not in self.data[key]:\n            continue  # can happen for empty containers\n        gidx = self.data[key][\"uid\"]\n        hidx = compute_haloindex(\n            gidx, halocelloffsets[:, num], index_unbound=index_unbound\n        )\n        self.data[key][\"GroupID\"] = hidx\n\n    # Subhalo ID\n    if \"Subhalo\" not in self.data:  # can happen for empty catalogs\n        for key in self.data:\n            if not (key.startswith(\"PartType\")):\n                continue\n            self.data[key][\"SubhaloID\"] = -1 * da.ones_like(\n                da[key][\"uid\"], dtype=np.int64\n            )\n        return\n\n    shnr_attr = \"SubhaloGrNr\"\n    if shnr_attr not in self.data[\"Subhalo\"]:\n        shnr_attr = \"SubhaloGroupNr\"  # what MTNG does\n    if shnr_attr not in self.data[\"Subhalo\"]:\n        raise ValueError(\n            f\"Could not find 'SubhaloGrNr' or 'SubhaloGroupNr' in {self.catalog}\"\n        )\n\n    subhalogrnr = self.data[\"Subhalo\"][shnr_attr]\n    subhalocellcounts = self.data[\"Subhalo\"][\"SubhaloLenType\"]\n\n    # remove \"units\" for numba funcs\n    if hasattr(subhalogrnr, \"magnitude\"):\n        subhalogrnr = subhalogrnr.magnitude\n    if hasattr(subhalocellcounts, \"magnitude\"):\n        subhalocellcounts = subhalocellcounts.magnitude\n\n    grp = self.data[\"Group\"]\n    if \"GroupFirstSub\" not in grp or \"GroupNsubs\" not in grp:\n        # if not provided, we calculate:\n        # \"GroupFirstSub\": First subhalo index for each halo\n        # \"GroupNsubs\": Number of subhalos for each halo\n        dlyd = delayed(get_shcounts_shcells)(subhalogrnr, ngrp)\n        grp[\"GroupFirstSub\"] = dask.compute(dlyd[1])[0]\n        grp[\"GroupNsubs\"] = dask.compute(dlyd[0])[0]\n\n    # remove \"units\" for numba funcs\n    grpfirstsub = grp[\"GroupFirstSub\"]\n    if hasattr(grpfirstsub, \"magnitude\"):\n        grpfirstsub = grpfirstsub.magnitude\n    grpnsubs = grp[\"GroupNsubs\"]\n    if hasattr(grpnsubs, \"magnitude\"):\n        grpnsubs = grpnsubs.magnitude\n\n    for key in self.data:\n        if not (key.startswith(\"PartType\")):\n            continue\n        num = int(key[-1])\n        pdata = self.data[key]\n        if \"uid\" not in self.data[key]:\n            continue  # can happen for empty containers\n        gidx = pdata[\"uid\"]\n\n        # we need to make other dask arrays delayed,\n        # map_block does not incorrectly infer output shape from these\n        halocelloffsets_dlyd = delayed(halocelloffsets[:, num])\n        grpfirstsub_dlyd = delayed(grpfirstsub)\n        grpnsubs_dlyd = delayed(grpnsubs)\n        subhalocellcounts_dlyd = delayed(subhalocellcounts[:, num])\n\n        sidx = compute_localsubhaloindex(\n            gidx,\n            halocelloffsets_dlyd,\n            grpfirstsub_dlyd,\n            grpnsubs_dlyd,\n            subhalocellcounts_dlyd,\n            index_unbound=index_unbound,\n        )\n\n        pdata[\"LocalSubhaloID\"] = sidx\n\n        # reconstruct SubhaloID from Group's GroupFirstSub and LocalSubhaloID\n        # should be easier to do it directly, but quicker to write down like this:\n\n        # calculate first subhalo of each halo that a particle belongs to\n        self.add_groupquantity_to_particles(\"GroupFirstSub\", parttype=key)\n        pdata[\"SubhaloID\"] = pdata[\"GroupFirstSub\"] + pdata[\"LocalSubhaloID\"]\n        pdata[\"SubhaloID\"] = da.where(\n            pdata[\"SubhaloID\"] == index_unbound, index_unbound, pdata[\"SubhaloID\"]\n        )\n</code></pre>"},{"location":"api_docs/#scida.customs.arepo.dataset.ArepoSnapshot.discover_catalog","title":"<code>discover_catalog()</code>","text":"<p>Discover the group catalog given the current path</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def discover_catalog(self):\n    \"\"\"\n    Discover the group catalog given the current path\n    Returns\n    -------\n\n    \"\"\"\n    p = str(self.path)\n    # order of candidates matters. For Illustris \"groups\" must precede \"fof_subhalo_tab\"\n    candidates = [\n        p.replace(\"snapshot\", \"group\"),\n        p.replace(\"snapshot\", \"groups\"),\n        p.replace(\"snapdir\", \"groups\").replace(\"snap\", \"groups\"),\n        p.replace(\"snapdir\", \"groups\").replace(\"snap\", \"fof_subhalo_tab\"),\n    ]\n    for candidate in candidates:\n        if not os.path.exists(candidate):\n            continue\n        if candidate == self.path:\n            continue\n        self.catalog = candidate\n        break\n</code></pre>"},{"location":"api_docs/#scida.customs.arepo.dataset.ArepoSnapshot.get_grouplengths","title":"<code>get_grouplengths(parttype='PartType0')</code>","text":"<p>Get the total number of particles of a given type in all halos.</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def get_grouplengths(self, parttype=\"PartType0\"):\n    \"\"\"Get the total number of particles of a given type in all halos.\"\"\"\n    pnum = part_type_num(parttype)\n    ptype = \"PartType%i\" % pnum\n    if ptype not in self._grouplengths:\n        lengths = self.data[\"Group\"][\"GroupLenType\"][:, pnum].compute()\n        if isinstance(lengths, pint.Quantity):\n            lengths = lengths.magnitude\n        self._grouplengths[ptype] = lengths\n    return self._grouplengths[ptype]\n</code></pre>"},{"location":"api_docs/#scida.customs.arepo.dataset.ArepoSnapshot.get_subhalolengths","title":"<code>get_subhalolengths(parttype='PartType0')</code>","text":"<p>Get the total number of particles of a given type in all halos.</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def get_subhalolengths(self, parttype=\"PartType0\"):\n    \"\"\"Get the total number of particles of a given type in all halos.\"\"\"\n    pnum = part_type_num(parttype)\n    ptype = \"PartType%i\" % pnum\n    if ptype in self._subhalolengths:\n        return self._subhalolengths[ptype]\n    lengths = self.data[\"Subhalo\"][\"SubhaloLenType\"][:, pnum].compute()\n    if isinstance(lengths, pint.Quantity):\n        lengths = lengths.magnitude\n    self._subhalolengths[ptype] = lengths\n    return self._subhalolengths[ptype]\n</code></pre>"},{"location":"api_docs/#scida.customs.arepo.dataset.ArepoSnapshot.map_group_operation","title":"<code>map_group_operation(func, cpucost_halo=10000.0, nchunks_min=None, chunksize_bytes=None, nmax=None, idxlist=None, objtype='halo')</code>","text":"<p>Apply a function to each halo in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>objtype</code> <p>Type of object to process. Can be \"halo\" or \"subhalo\". Default: \"halo\"</p> <code>'halo'</code> <code>idxlist</code> <p>List of halo indices to process. If not provided, all halos are processed.</p> <code>None</code> <code>func</code> <p>Function to apply to each halo. Must take a dictionary of arrays as input.</p> required <code>cpucost_halo</code> <p>\"CPU cost\" of processing a single halo. This is a relative value to the processing time per input particle used for calculating the dask chunks. Default: 1e4</p> <code>10000.0</code> <code>nchunks_min</code> <p>Minimum number of particles in a halo to process it. Default: None</p> <code>None</code> <code>chunksize_bytes</code> <code>None</code> <code>nmax</code> <p>Only process the first nmax halos.</p> <code>None</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>@computedecorator\ndef map_group_operation(\n    self,\n    func,\n    cpucost_halo=1e4,\n    nchunks_min=None,\n    chunksize_bytes=None,\n    nmax=None,\n    idxlist=None,\n    objtype=\"halo\",\n):\n    \"\"\"\n    Apply a function to each halo in the catalog.\n\n    Parameters\n    ----------\n    objtype: str\n        Type of object to process. Can be \"halo\" or \"subhalo\". Default: \"halo\"\n    idxlist: Optional[np.ndarray]\n        List of halo indices to process. If not provided, all halos are processed.\n    func: function\n        Function to apply to each halo. Must take a dictionary of arrays as input.\n    cpucost_halo:\n        \"CPU cost\" of processing a single halo. This is a relative value to the processing time per input particle\n        used for calculating the dask chunks. Default: 1e4\n    nchunks_min: Optional[int]\n        Minimum number of particles in a halo to process it. Default: None\n    chunksize_bytes: Optional[int]\n    nmax: Optional[int]\n        Only process the first nmax halos.\n    Returns\n    -------\n\n    \"\"\"\n    dfltkwargs = get_kwargs(func)\n    fieldnames = dfltkwargs.get(\"fieldnames\", None)\n    if fieldnames is None:\n        fieldnames = get_args(func)\n    parttype = dfltkwargs.get(\"parttype\", \"PartType0\")\n    entry_nbytes_in = np.sum([self.data[parttype][f][0].nbytes for f in fieldnames])\n    objtype = grp_type_str(objtype)\n    if objtype == \"halo\":\n        lengths = self.get_grouplengths(parttype=parttype)\n        offsets = self.get_groupoffsets(parttype=parttype)\n    elif objtype == \"subhalo\":\n        lengths = self.get_subhalolengths(parttype=parttype)\n        offsets = self.get_subhalooffsets(parttype=parttype)\n    else:\n        raise ValueError(f\"objtype must be 'halo' or 'subhalo', not {objtype}\")\n    arrdict = self.data[parttype]\n    return map_group_operation(\n        func,\n        offsets,\n        lengths,\n        arrdict,\n        cpucost_halo=cpucost_halo,\n        nchunks_min=nchunks_min,\n        chunksize_bytes=chunksize_bytes,\n        entry_nbytes_in=entry_nbytes_in,\n        nmax=nmax,\n        idxlist=idxlist,\n    )\n</code></pre>"},{"location":"api_docs/#scida.customs.arepo.dataset.ArepoSnapshot.register_field","title":"<code>register_field(parttype, name=None, construct=False)</code>","text":"<p>Register a field.</p> <p>Parameters:</p> Name Type Description Default <code>parttype</code> <code>str</code> <p>name of particle type</p> required <code>name</code> <code>str</code> <p>name of field</p> <code>None</code> <code>construct</code> <code>bool</code> <p>construct field immediately</p> <code>False</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def register_field(self, parttype: str, name: str = None, construct: bool = False):\n    \"\"\"\n    Register a field.\n    Parameters\n    ----------\n    parttype: str\n        name of particle type\n    name: str\n        name of field\n    construct: bool\n        construct field immediately\n\n    Returns\n    -------\n\n    \"\"\"\n    num = part_type_num(parttype)\n    if construct:  # TODO: introduce (immediate) construct option later\n        raise NotImplementedError\n    if num == -1:  # TODO: all particle species\n        key = \"all\"\n        raise NotImplementedError\n    elif isinstance(num, int):\n        key = \"PartType\" + str(num)\n    else:\n        key = parttype\n    return super().register_field(key, name=name)\n</code></pre>"},{"location":"api_docs/#scida.customs.arepo.dataset.ArepoSnapshot.return_data","title":"<code>return_data()</code>","text":"<p>Return data.</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>@ArepoSelector()\ndef return_data(self):\n    \"\"\"\n    Return data.\n    Returns\n    -------\n\n    \"\"\"\n    return super().return_data()\n</code></pre>"},{"location":"api_docs/#scida.customs.arepo.dataset.Temperature","title":"<code>Temperature(arrs, ureg=None, **kwargs)</code>","text":"<p>Compute gas temperature given (ElectronAbundance,InternalEnergy) in [K].</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>@fielddefs.register_field(\"PartType0\")\ndef Temperature(arrs, ureg=None, **kwargs):\n    \"\"\"Compute gas temperature given (ElectronAbundance,InternalEnergy) in [K].\"\"\"\n    xh = 0.76\n    gamma = 5.0 / 3.0\n\n    m_p = 1.672622e-24  # proton mass [g]\n    k_B = 1.380650e-16  # boltzmann constant [erg/K]\n\n    UnitEnergy_over_UnitMass = (\n        1e10  # standard unit system (TODO: can obtain from snapshot)\n    )\n    f = UnitEnergy_over_UnitMass\n    if ureg is not None:\n        f = 1.0\n        m_p = m_p * ureg.g\n        k_B = k_B * ureg.erg / ureg.K\n\n    xe = arrs[\"ElectronAbundance\"]\n    u_internal = arrs[\"InternalEnergy\"]\n\n    mu = 4 / (1 + 3 * xh + 4 * xh * xe) * m_p\n    temp = f * (gamma - 1.0) * u_internal / k_B * mu\n\n    return temp\n</code></pre>"},{"location":"api_docs/#scida.customs.arepo.dataset.compute_haloindex","title":"<code>compute_haloindex(gidx, halocelloffsets, *args, index_unbound=None)</code>","text":"<p>Computes the halo index for each particle with dask.</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def compute_haloindex(gidx, halocelloffsets, *args, index_unbound=None):\n    \"\"\"Computes the halo index for each particle with dask.\"\"\"\n    return da.map_blocks(\n        get_hidx_daskwrap,\n        gidx,\n        halocelloffsets,\n        index_unbound=index_unbound,\n        meta=np.array((), dtype=np.int64),\n    )\n</code></pre>"},{"location":"api_docs/#scida.customs.arepo.dataset.compute_haloquantity","title":"<code>compute_haloquantity(gidx, halocelloffsets, hvals, *args)</code>","text":"<p>Computes a halo quantity for each particle with dask.</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def compute_haloquantity(gidx, halocelloffsets, hvals, *args):\n    \"\"\"Computes a halo quantity for each particle with dask.\"\"\"\n    units = None\n    if hasattr(hvals, \"units\"):\n        units = hvals.units\n    res = map_blocks(\n        get_haloquantity_daskwrap,\n        gidx,\n        halocelloffsets,\n        hvals,\n        meta=np.array((), dtype=hvals.dtype),\n        output_units=units,\n    )\n    return res\n</code></pre>"},{"location":"api_docs/#scida.customs.arepo.dataset.get_hidx","title":"<code>get_hidx(gidx_start, gidx_count, celloffsets, index_unbound=None)</code>","text":"<p>Get halo index of a given cell</p> <p>Parameters:</p> Name Type Description Default <code>gidx_start</code> <p>The first unique integer ID for the first particle</p> required <code>gidx_count</code> <p>The amount of halo indices we are querying after \"gidx_start\"</p> required <code>celloffsets</code> <code>array</code> <p>An array holding the starting cell offset for each halo. Needs to include the offset after the last halo. The required shape is thus (Nhalo+1,).</p> required <code>index_unbound</code> <code>integer</code> <p>The index to use for unbound particles. If None, the maximum integer value of the dtype is used.</p> <code>None</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>@jit(nopython=True)\ndef get_hidx(gidx_start, gidx_count, celloffsets, index_unbound=None):\n    \"\"\"Get halo index of a given cell\n\n    Parameters\n    ----------\n    gidx_start: integer\n        The first unique integer ID for the first particle\n    gidx_count: integer\n        The amount of halo indices we are querying after \"gidx_start\"\n    celloffsets : array\n        An array holding the starting cell offset for each halo. Needs to include the\n        offset after the last halo. The required shape is thus (Nhalo+1,).\n    index_unbound : integer, optional\n        The index to use for unbound particles. If None, the maximum integer value\n        of the dtype is used.\n    \"\"\"\n    dtype = np.int64\n    if index_unbound is None:\n        index_unbound = np.iinfo(dtype).max\n    res = index_unbound * np.ones(gidx_count, dtype=dtype)\n    # find initial celloffset\n    hidx_idx = np.searchsorted(celloffsets, gidx_start, side=\"right\") - 1\n    if hidx_idx + 1 &gt;= celloffsets.shape[0]:\n        # we are done. Already out of scope of lookup =&gt; all unbound gas.\n        return res\n    celloffset = celloffsets[hidx_idx + 1]\n    endid = celloffset - gidx_start\n    startid = 0\n\n    # Now iterate through list.\n    while startid &lt; gidx_count:\n        res[startid:endid] = hidx_idx\n        hidx_idx += 1\n        startid = endid\n        if hidx_idx &gt;= celloffsets.shape[0] - 1:\n            break\n        count = celloffsets[hidx_idx + 1] - celloffsets[hidx_idx]\n        endid = startid + count\n    return res\n</code></pre>"},{"location":"api_docs/#scida.customs.arepo.dataset.get_localshidx","title":"<code>get_localshidx(gidx_start, gidx_count, celloffsets, shnumber, shcounts, shcellcounts, index_unbound=None)</code>","text":"<p>Get the local subhalo index for each particle. This is the subhalo index within each halo group. Particles belonging to the central galaxies will have index 0, particles belonging to the first satellite will have index 1, etc.</p> <p>Parameters:</p> Name Type Description Default <code>gidx_start</code> <code>int</code> required <code>gidx_count</code> <code>int</code> required <code>celloffsets</code> <code>NDArray[int64]</code> required <code>shnumber</code> required <code>shcounts</code> required <code>shcellcounts</code> required <code>index_unbound</code> <p>The index to use for unbound particles. If None, the maximum integer value of the dtype is used.</p> <code>None</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>@jit(nopython=True)\ndef get_localshidx(\n    gidx_start: int,\n    gidx_count: int,\n    celloffsets: NDArray[np.int64],\n    shnumber,\n    shcounts,\n    shcellcounts,\n    index_unbound=None,\n):\n    \"\"\"\n    Get the local subhalo index for each particle. This is the subhalo index within each\n    halo group. Particles belonging to the central galaxies will have index 0, particles\n    belonging to the first satellite will have index 1, etc.\n    Parameters\n    ----------\n    gidx_start\n    gidx_count\n    celloffsets\n    shnumber\n    shcounts\n    shcellcounts\n    index_unbound: integer, optional\n        The index to use for unbound particles. If None, the maximum integer value\n        of the dtype is used.\n\n    Returns\n    -------\n\n    \"\"\"\n    dtype = np.int32\n    if index_unbound is None:\n        index_unbound = np.iinfo(dtype).max\n    res = index_unbound * np.ones(gidx_count, dtype=dtype)  # fuzz has negative index.\n\n    # find initial Group we are in\n    hidx_start_idx = np.searchsorted(celloffsets, gidx_start, side=\"right\") - 1\n    if hidx_start_idx + 1 &gt;= celloffsets.shape[0]:\n        # we are done. Already out of scope of lookup =&gt; all unbound gas.\n        return res\n    celloffset = celloffsets[hidx_start_idx + 1]\n    endid = celloffset - gidx_start\n    startid = 0\n\n    # find initial subhalo we are in\n    hidx = hidx_start_idx\n    shcumsum = np.zeros(shcounts[hidx] + 1, dtype=np.int64)\n    shcumsum[1:] = np.cumsum(\n        shcellcounts[shnumber[hidx] : shnumber[hidx] + shcounts[hidx]]\n    )  # collect halo's subhalo offsets\n    shcumsum += celloffsets[hidx_start_idx]\n    sidx_start_idx: int = int(np.searchsorted(shcumsum, gidx_start, side=\"right\") - 1)\n    if sidx_start_idx &lt; shcounts[hidx]:\n        endid = shcumsum[sidx_start_idx + 1] - gidx_start\n\n    # Now iterate through list.\n    cont = True\n    while cont and (startid &lt; gidx_count):\n        res[startid:endid] = (\n            sidx_start_idx if sidx_start_idx + 1 &lt; shcumsum.shape[0] else -1\n        )\n        sidx_start_idx += 1\n        if sidx_start_idx &lt; shcounts[hidx_start_idx]:\n            # we prepare to fill the next available subhalo for current halo\n            count = shcumsum[sidx_start_idx + 1] - shcumsum[sidx_start_idx]\n            startid = endid\n        else:\n            # we need to find the next halo to start filling its subhalos\n            dbgcount = 0\n            while dbgcount &lt; 100:  # find next halo with &gt;0 subhalos\n                hidx_start_idx += 1\n                if hidx_start_idx &gt;= shcounts.shape[0]:\n                    cont = False\n                    break\n                if shcounts[hidx_start_idx] &gt; 0:\n                    break\n                dbgcount += 1\n            hidx = hidx_start_idx\n            if hidx_start_idx &gt;= celloffsets.shape[0] - 1:\n                startid = gidx_count\n            else:\n                count = celloffsets[hidx_start_idx + 1] - celloffsets[hidx_start_idx]\n                if hidx &lt; shcounts.shape[0]:\n                    shcumsum = np.zeros(shcounts[hidx] + 1, dtype=np.int64)\n                    shcumsum[1:] = np.cumsum(\n                        shcellcounts[shnumber[hidx] : shnumber[hidx] + shcounts[hidx]]\n                    )\n                    shcumsum += celloffsets[hidx_start_idx]\n                    sidx_start_idx = 0\n                    if sidx_start_idx &lt; shcounts[hidx]:\n                        count = shcumsum[sidx_start_idx + 1] - shcumsum[sidx_start_idx]\n                    startid = celloffsets[hidx_start_idx] - gidx_start\n        endid = startid + count\n    return res\n</code></pre>"},{"location":"api_docs/#scida.customs.arepo.dataset.get_shcounts_shcells","title":"<code>get_shcounts_shcells(SubhaloGrNr, hlength)</code>","text":"<p>Returns the id of the first subhalo and count of subhalos per halo.</p> <p>Parameters:</p> Name Type Description Default <code>SubhaloGrNr</code> required <code>The</code> required <code>hlength</code> required <code>The</code> required Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>@jit(nopython=True)\ndef get_shcounts_shcells(SubhaloGrNr, hlength):\n    \"\"\"\n    Returns the id of the first subhalo and count of subhalos per halo.\n    Parameters\n    ----------\n    SubhaloGrNr: np.ndarray\n    The group identifier that each subhalo belongs to respectively\n    hlength: int\n    The number of halos in the snapshot\n\n    Returns\n    -------\n\n    \"\"\"\n    shcounts = np.zeros(hlength, dtype=np.int32)  # number of subhalos per halo\n    shnumber = np.zeros(hlength, dtype=np.int32)  # index of first subhalo per halo\n    i = 0\n    hid_old = 0\n    while i &lt; SubhaloGrNr.shape[0]:\n        hid = SubhaloGrNr[i]\n        if hid == hid_old:\n            shcounts[hid] += 1\n        else:\n            shnumber[hid] = i\n            shcounts[hid] += 1\n            hid_old = hid\n        i += 1\n    return shcounts, shnumber\n</code></pre>"},{"location":"api_docs/#scida.customs.arepo.dataset.map_group_operation","title":"<code>map_group_operation(func, offsets, lengths, arrdict, cpucost_halo=10000.0, nchunks_min=None, chunksize_bytes=None, entry_nbytes_in=4, fieldnames=None, nmax=None, idxlist=None)</code>","text":"<p>Map a function to all halos in a halo catalog.</p> <p>Parameters:</p> Name Type Description Default <code>idxlist</code> <code>Optional[ndarray]</code> <p>Only process the halos with these indices.</p> <code>None</code> <code>nmax</code> <code>Optional[int]</code> <p>Only process the first nmax halos.</p> <code>None</code> <code>func</code> required <code>offsets</code> <p>Offset of each group in the particle catalog.</p> required <code>lengths</code> <p>Number of particles per halo.</p> required <code>arrdict</code> required <code>cpucost_halo</code> <code>10000.0</code> <code>nchunks_min</code> <code>Optional[int]</code> <p>Lower bound on the number of halos per chunk.</p> <code>None</code> <code>chunksize_bytes</code> <code>Optional[int]</code> <code>None</code> <code>entry_nbytes_in</code> <code>Optional[int]</code> <code>4</code> <code>fieldnames</code> <code>Optional[List[str]]</code> <code>None</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def map_group_operation(\n    func,\n    offsets,\n    lengths,\n    arrdict,\n    cpucost_halo=1e4,\n    nchunks_min: Optional[int] = None,\n    chunksize_bytes: Optional[int] = None,\n    entry_nbytes_in: Optional[int] = 4,\n    fieldnames: Optional[List[str]] = None,\n    nmax: Optional[int] = None,\n    idxlist: Optional[np.ndarray] = None,\n) -&gt; da.Array:\n    \"\"\"\n    Map a function to all halos in a halo catalog.\n    Parameters\n    ----------\n    idxlist: Optional[np.ndarray]\n        Only process the halos with these indices.\n    nmax: Optional[int]\n        Only process the first nmax halos.\n    func\n    offsets: np.ndarray\n        Offset of each group in the particle catalog.\n    lengths: np.ndarray\n        Number of particles per halo.\n    arrdict\n    cpucost_halo\n    nchunks_min: Optional[int]\n        Lower bound on the number of halos per chunk.\n    chunksize_bytes\n    entry_nbytes_in\n    fieldnames\n\n    Returns\n    -------\n\n    \"\"\"\n    if isinstance(func, ChainOps):\n        dfltkwargs = func.kwargs\n    else:\n        dfltkwargs = get_kwargs(func)\n    if fieldnames is None:\n        fieldnames = dfltkwargs.get(\"fieldnames\", None)\n    if fieldnames is None:\n        fieldnames = get_args(func)\n    units = dfltkwargs.get(\"units\", None)\n    shape = dfltkwargs.get(\"shape\", None)\n    dtype = dfltkwargs.get(\"dtype\", \"float64\")\n    fill_value = dfltkwargs.get(\"fill_value\", 0)\n\n    if idxlist is not None and nmax is not None:\n        raise ValueError(\"Cannot specify both idxlist and nmax.\")\n\n    lengths_all = lengths\n    offsets_all = offsets\n    if len(lengths) == len(offsets):\n        # the offsets array here is one longer here, holding the total number of particles in the last halo.\n        offsets_all = np.concatenate([offsets_all, [offsets_all[-1] + lengths[-1]]])\n\n    if nmax is not None:\n        lengths = lengths[:nmax]\n        offsets = offsets[:nmax]\n\n    if idxlist is not None:\n        # make sure idxlist is sorted and unique\n        if not np.all(np.diff(idxlist) &gt; 0):\n            raise ValueError(\"idxlist must be sorted and unique.\")\n        # make sure idxlist is within range\n        if np.min(idxlist) &lt; 0 or np.max(idxlist) &gt;= lengths.shape[0]:\n            raise ValueError(\n                \"idxlist elements must be in [%i, %i), but covers range [%i, %i].\"\n                % (0, lengths.shape[0], np.min(idxlist), np.max(idxlist))\n            )\n        offsets = offsets[idxlist]\n        lengths = lengths[idxlist]\n\n    if len(lengths) == len(offsets):\n        # the offsets array here is one longer here, holding the total number of particles in the last halo.\n        offsets = np.concatenate([offsets, [offsets[-1] + lengths[-1]]])\n\n    # shape/units inference\n    infer_shape = shape is None or (isinstance(shape, str) and shape == \"auto\")\n    infer_units = units is None\n    infer = infer_shape or infer_units\n    if infer:\n        # attempt to determine shape.\n        if infer_shape:\n            log.debug(\n                \"No shape specified. Attempting to determine shape of func output.\"\n            )\n        if infer_units:\n            log.debug(\n                \"No units specified. Attempting to determine units of func output.\"\n            )\n        arrs = [arrdict[f][:1].compute() for f in fieldnames]\n        # remove units if present\n        # arrs = [arr.magnitude if hasattr(arr, \"magnitude\") else arr for arr in arrs]\n        # arrs = [arr.magnitude for arr in arrs]\n        dummyres = None\n        try:\n            dummyres = func(*arrs)\n        except Exception as e:  # noqa\n            log.warning(\"Exception during shape/unit inference: %s.\" % str(e))\n        if dummyres is not None:\n            if infer_units and hasattr(dummyres, \"units\"):\n                units = dummyres.units\n            log.debug(\"Shape inference: %s.\" % str(shape))\n        if infer_units and dummyres is None:\n            units_present = any([hasattr(arr, \"units\") for arr in arrs])\n            if units_present:\n                log.warning(\"Exception during unit inference. Assuming no units.\")\n        if dummyres is None and infer_shape:\n            # due to https://github.com/hgrecco/pint/issues/1037 innocent np.array operations on unit scalars can fail.\n            # we can still attempt to infer shape by removing units prior to calling func.\n            arrs = [arr.magnitude if hasattr(arr, \"magnitude\") else arr for arr in arrs]\n            try:\n                dummyres = func(*arrs)\n            except Exception as e:  # noqa\n                # no more logging needed here\n                pass\n        if dummyres is not None and infer_shape:\n            if np.isscalar(dummyres):\n                shape = (1,)\n            else:\n                shape = dummyres.shape\n        if infer_shape and dummyres is None and shape is None:\n            log.warning(\"Exception during shape inference. Using shape (1,).\")\n            shape = ()\n    # unit inference\n\n    # Determine chunkedges automatically\n    # TODO: very messy and inefficient routine. improve some time.\n    # TODO: Set entry_bytes_out\n    nbytes_dtype_out = 4  # TODO: hardcode 4 byte output dtype as estimate for now\n    entry_nbytes_out = nbytes_dtype_out * np.product(shape)\n\n    # list_chunkedges refers to bounds of index intervals to be processed together\n    # if idxlist is specified, then these indices do not have to refer to group indices.\n    # if idxlist is given, we enforce that particle data is contiguous\n    # by putting each idx from idxlist into its own chunk.\n    # in the future, we should optimize this\n    if idxlist is not None:\n        list_chunkedges = [[idx, idx + 1] for idx in np.arange(len(idxlist))]\n    else:\n        list_chunkedges = map_group_operation_get_chunkedges(\n            lengths,\n            entry_nbytes_in,\n            entry_nbytes_out,\n            cpucost_halo=cpucost_halo,\n            nchunks_min=nchunks_min,\n            chunksize_bytes=chunksize_bytes,\n        )\n\n    minentry = offsets[0]\n    maxentry = offsets[-1]  # the last particle that needs to be processed\n\n    # chunks specify the number of groups in each chunk\n    chunks = [tuple(np.diff(list_chunkedges, axis=1).flatten())]\n    # need to add chunk information for additional output axes if needed\n    new_axis = None\n    if isinstance(shape, tuple) and shape != (1,):\n        chunks += [(s,) for s in shape]\n        new_axis = np.arange(1, len(shape) + 1).tolist()\n\n    # slcoffsets = [offsets[chunkedge[0]] for chunkedge in list_chunkedges]\n    # the actual length of relevant data in each chunk\n    slclengths = [\n        offsets[chunkedge[1]] - offsets[chunkedge[0]] for chunkedge in list_chunkedges\n    ]\n    if idxlist is not None:\n        # the chunk length to be fed into map_blocks\n        tmplist = np.concatenate([idxlist, [len(lengths_all)]])\n        slclengths_map = [\n            offsets_all[tmplist[chunkedge[1]]] - offsets_all[tmplist[chunkedge[0]]]\n            for chunkedge in list_chunkedges\n        ]\n        slcoffsets_map = [\n            offsets_all[tmplist[chunkedge[0]]] for chunkedge in list_chunkedges\n        ]\n        slclengths_map[0] = slcoffsets_map[0]\n        slcoffsets_map[0] = 0\n    else:\n        slclengths_map = slclengths\n\n    slcs = [slice(chunkedge[0], chunkedge[1]) for chunkedge in list_chunkedges]\n    offsets_in_chunks = [offsets[slc] - offsets[slc.start] for slc in slcs]\n    lengths_in_chunks = [lengths[slc] for slc in slcs]\n    d_oic = delayed(offsets_in_chunks)\n    d_hic = delayed(lengths_in_chunks)\n\n    arrs = [arrdict[f][minentry:maxentry] for f in fieldnames]\n    for i, arr in enumerate(arrs):\n        arrchunks = ((tuple(slclengths)),)\n        if len(arr.shape) &gt; 1:\n            arrchunks = arrchunks + (arr.shape[1:],)\n        arrs[i] = arr.rechunk(chunks=arrchunks)\n    arrdims = np.array([len(arr.shape) for arr in arrs])\n\n    assert np.all(arrdims == arrdims[0])  # Cannot handle different input dims for now\n\n    drop_axis = []\n    if arrdims[0] &gt; 1:\n        drop_axis = np.arange(1, arrdims[0])\n\n    if dtype is None:\n        raise ValueError(\n            \"dtype must be specified, dask will not be able to automatically determine this here.\"\n        )\n\n    calc = map_blocks(\n        wrap_func_scalar,\n        func,\n        d_oic,\n        d_hic,\n        *arrs,\n        dtype=dtype,\n        chunks=chunks,\n        new_axis=new_axis,\n        drop_axis=drop_axis,\n        func_output_shape=shape,\n        func_output_dtype=dtype,\n        fill_value=fill_value,\n        output_units=units,\n    )\n\n    return calc\n</code></pre>"},{"location":"api_docs/#scida.customs.arepo.dataset.map_group_operation_get_chunkedges","title":"<code>map_group_operation_get_chunkedges(lengths, entry_nbytes_in, entry_nbytes_out, cpucost_halo=1.0, nchunks_min=None, chunksize_bytes=None)</code>","text":"<p>Compute the chunking of a halo operation.</p> <p>Parameters:</p> Name Type Description Default <code>lengths</code> <p>The number of particles per halo.</p> required <code>entry_nbytes_in</code> required <code>entry_nbytes_out</code> required <code>cpucost_halo</code> <code>1.0</code> <code>nchunks_min</code> <code>None</code> <code>chunksize_bytes</code> <code>None</code> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def map_group_operation_get_chunkedges(\n    lengths,\n    entry_nbytes_in,\n    entry_nbytes_out,\n    cpucost_halo=1.0,\n    nchunks_min=None,\n    chunksize_bytes=None,\n):\n    \"\"\"\n    Compute the chunking of a halo operation.\n\n    Parameters\n    ----------\n    lengths: np.ndarray\n        The number of particles per halo.\n    entry_nbytes_in\n    entry_nbytes_out\n    cpucost_halo\n    nchunks_min\n    chunksize_bytes\n\n    Returns\n    -------\n\n    \"\"\"\n    cpucost_particle = 1.0  # we only care about ratio, so keep particle cost fixed.\n    cost = cpucost_particle * lengths + cpucost_halo\n    sumcost = cost.cumsum()\n\n    # let's allow a maximal chunksize of 16 times the dask default setting for an individual array [here: multiple]\n    if chunksize_bytes is None:\n        chunksize_bytes = 16 * parse_humansize(dask.config.get(\"array.chunk-size\"))\n    cost_memory = entry_nbytes_in * lengths + entry_nbytes_out\n\n    if not np.max(cost_memory) &lt; chunksize_bytes:\n        raise ValueError(\n            \"Some halo requires more memory than allowed (%i allowed, %i requested). Consider overriding \"\n            \"chunksize_bytes.\" % (chunksize_bytes, np.max(cost_memory))\n        )\n\n    nchunks = int(np.ceil(np.sum(cost_memory) / chunksize_bytes))\n    nchunks = int(np.ceil(1.3 * nchunks))  # fudge factor\n    if nchunks_min is not None:\n        nchunks = max(nchunks_min, nchunks)\n    targetcost = sumcost[-1] / nchunks  # chunk target cost = total cost / nchunks\n\n    arr = np.diff(sumcost % targetcost)  # find whenever exceeding modulo target cost\n    idx = [0] + list(np.where(arr &lt; 0)[0] + 1)\n    if idx[-1] != sumcost.shape[0]:\n        idx.append(sumcost.shape[0])\n    list_chunkedges = []\n    for i in range(len(idx) - 1):\n        list_chunkedges.append([idx[i], idx[i + 1]])\n\n    list_chunkedges = np.asarray(\n        memorycost_limiter(cost_memory, cost, list_chunkedges, chunksize_bytes)\n    )\n\n    # make sure we did not lose any halos.\n    assert np.all(\n        ~(list_chunkedges.flatten()[2:-1:2] - list_chunkedges.flatten()[1:-1:2]).astype(\n            bool\n        )\n    )\n    return list_chunkedges\n</code></pre>"},{"location":"api_docs/#scida.customs.arepo.dataset.memorycost_limiter","title":"<code>memorycost_limiter(cost_memory, cost_cpu, list_chunkedges, cost_memory_max)</code>","text":"<p>If a chunk too memory expensive, split into equal cpu expense operations.</p> Source code in <code>src/scida/customs/arepo/dataset.py</code> <pre><code>def memorycost_limiter(cost_memory, cost_cpu, list_chunkedges, cost_memory_max):\n    \"\"\"If a chunk too memory expensive, split into equal cpu expense operations.\"\"\"\n    list_chunkedges_new = []\n    for chunkedges in list_chunkedges:\n        slc = slice(*chunkedges)\n        totcost_mem = np.sum(cost_memory[slc])\n        list_chunkedges_new.append(chunkedges)\n        if totcost_mem &gt; cost_memory_max:\n            sumcost = cost_cpu[slc].cumsum()\n            sumcost /= sumcost[-1]\n            idx = slc.start + np.argmin(np.abs(sumcost - 0.5))\n            if idx == chunkedges[0]:\n                idx += 1\n            elif idx == chunkedges[-1]:\n                idx -= 1\n            chunkedges1 = [chunkedges[0], idx]\n            chunkedges2 = [idx, chunkedges[1]]\n            if idx == chunkedges[0] or idx == chunkedges[1]:\n                raise ValueError(\"This should not happen.\")\n            list_chunkedges_new.pop()\n            list_chunkedges_new += memorycost_limiter(\n                cost_memory, cost_cpu, [chunkedges1], cost_memory_max\n            )\n            list_chunkedges_new += memorycost_limiter(\n                cost_memory, cost_cpu, [chunkedges2], cost_memory_max\n            )\n    return list_chunkedges_new\n</code></pre>"},{"location":"api_docs/#scida.customs.arepo.helpers","title":"<code>helpers</code>","text":""},{"location":"api_docs/#scida.customs.arepo.helpers.part_type_num","title":"<code>part_type_num(ptype)</code>","text":"<p>Mapping between common names and numeric particle types.</p> Source code in <code>src/scida/customs/arepo/helpers.py</code> <pre><code>def part_type_num(ptype):\n    \"\"\"Mapping between common names and numeric particle types.\"\"\"\n    ptype = str(ptype).replace(\"PartType\", \"\")\n    if ptype.isdigit():\n        return int(ptype)\n\n    if str(ptype).lower() in [\"gas\", \"cells\"]:\n        return 0\n    if str(ptype).lower() in [\"dm\", \"darkmatter\"]:\n        return 1\n    if str(ptype).lower() in [\"dmlowres\"]:\n        return 2  # only zoom simulations, not present in full periodic boxes\n    if str(ptype).lower() in [\"tracer\", \"tracers\", \"tracermc\", \"trmc\"]:\n        return 3\n    if str(ptype).lower() in [\"star\", \"stars\", \"stellar\"]:\n        return 4  # only those with GFM_StellarFormationTime&gt;0\n    if str(ptype).lower() in [\"wind\"]:\n        return 4  # only those with GFM_StellarFormationTime&lt;0\n    if str(ptype).lower() in [\"bh\", \"bhs\", \"blackhole\", \"blackholes\", \"black\"]:\n        return 5\n    if str(ptype).lower() in [\"all\"]:\n        return -1\n</code></pre>"},{"location":"api_docs/#scida.customs.arepo.selector","title":"<code>selector</code>","text":""},{"location":"api_docs/#scida.customs.arepo.series","title":"<code>series</code>","text":""},{"location":"api_docs/#scida.customs.arepo.series.ArepoSimulation","title":"<code>ArepoSimulation</code>","text":"<p>             Bases: <code>GadgetStyleSimulation</code></p> <p>A series representing an arepo simulation.</p> Source code in <code>src/scida/customs/arepo/series.py</code> <pre><code>class ArepoSimulation(GadgetStyleSimulation):\n    \"\"\"A series representing an arepo simulation.\"\"\"\n\n    def __init__(self, path, lazy=True, async_caching=False, **interface_kwargs):\n        # choose parent folder as path if we are passed \"output\" dir\n        p = pathlib.Path(path)\n        if p.name == \"output\":\n            path = str(p.parent)\n        prefix_dict = dict(paths=\"snapdir\", gpaths=\"group\")\n        arg_dict = dict(gpaths=\"catalog\")\n        super().__init__(\n            path,\n            prefix_dict=prefix_dict,\n            arg_dict=arg_dict,\n            lazy=lazy,\n            async_caching=async_caching,\n            **interface_kwargs\n        )\n\n    @classmethod\n    def validate_path(cls, path, *args, **kwargs) -&gt; CandidateStatus:\n        valid = CandidateStatus.NO\n        if not os.path.isdir(path):\n            return CandidateStatus.NO\n        fns = os.listdir(path)\n        if \"gizmo_parameters.txt\" in fns:\n            return CandidateStatus.NO\n        sprefixs = [\"snapdir\", \"snapshot\"]\n        opath = path\n        if \"output\" in fns:\n            opath = join(path, \"output\")\n        folders = os.listdir(opath)\n        folders = [f for f in folders if os.path.isdir(join(opath, f))]\n        if any([f.startswith(k) for f in folders for k in sprefixs]):\n            valid = CandidateStatus.MAYBE\n        return valid\n</code></pre>"},{"location":"api_docs/#scida.customs.gadgetstyle","title":"<code>gadgetstyle</code>","text":""},{"location":"api_docs/#scida.customs.gadgetstyle.dataset","title":"<code>dataset</code>","text":""},{"location":"api_docs/#scida.customs.gadgetstyle.dataset.GadgetStyleSnapshot","title":"<code>GadgetStyleSnapshot</code>","text":"<p>             Bases: <code>Dataset</code></p> Source code in <code>src/scida/customs/gadgetstyle/dataset.py</code> <pre><code>class GadgetStyleSnapshot(Dataset):\n    def __init__(self, path, chunksize=\"auto\", virtualcache=True, **kwargs) -&gt; None:\n        \"\"\"We define gadget-style snapshots as nbody/hydrodynamical simulation snapshots that follow\n        the common /PartType0, /PartType1 grouping scheme.\"\"\"\n        self.boxsize = np.nan\n        super().__init__(path, chunksize=chunksize, virtualcache=virtualcache, **kwargs)\n\n        defaultattributes = [\"config\", \"header\", \"parameters\"]\n        for k in self._metadata_raw:\n            name = k.strip(\"/\").lower()\n            if name in defaultattributes:\n                self.__dict__[name] = self._metadata_raw[k]\n                if \"BoxSize\" in self.__dict__[name]:\n                    self.boxsize = self.__dict__[name][\"BoxSize\"]\n                elif \"Boxsize\" in self.__dict__[name]:\n                    self.boxsize = self.__dict__[name][\"Boxsize\"]\n\n        sanity_check = kwargs.get(\"sanity_check\", False)\n        key_nparts = \"NumPart_Total\"\n        key_nparts_hw = \"NumPart_Total_HighWord\"\n        if sanity_check and key_nparts in self.header and key_nparts_hw in self.header:\n            nparts = self.header[key_nparts_hw] * 2**32 + self.header[key_nparts]\n            for i, n in enumerate(nparts):\n                pkey = \"PartType%i\" % i\n                if pkey in self.data:\n                    pdata = self.data[pkey]\n                    fkey = next(iter(pdata.keys()))\n                    nparts_loaded = pdata[fkey].shape[0]\n                    if nparts_loaded != n:\n                        raise ValueError(\n                            \"Number of particles in header (%i) does not match number of particles loaded (%i) \"\n                            \"for particle type %i\" % (n, nparts_loaded, i)\n                        )\n\n    @classmethod\n    def _get_fileprefix(cls, path: Union[str, os.PathLike], **kwargs) -&gt; str:\n        \"\"\"\n        Get the fileprefix used to identify files belonging to given dataset.\n        Parameters\n        ----------\n        path: str, os.PathLike\n            path to check\n        kwargs\n\n        Returns\n        -------\n        str\n        \"\"\"\n        if os.path.isfile(path):\n            return \"\"  # nothing to do, we have a single file, not a directory\n        # order matters: groups will be taken before fof_subhalo, requires py&gt;3.7 for dict order\n        prfxs = [\"groups\", \"fof_subhalo\", \"snap\"]\n        prfxs_prfx_sim = dict.fromkeys(prfxs)\n        files = sorted(os.listdir(path))\n        prfxs_lst = []\n        for fn in files:\n            s = re.search(r\"^(\\w*)_(\\d*)\", fn)\n            if s is not None:\n                prfxs_lst.append(s.group(1))\n        prfxs_lst = [p for s in prfxs_prfx_sim for p in prfxs_lst if p.startswith(s)]\n        prfxs = dict.fromkeys(prfxs_lst)\n        prfxs = list(prfxs.keys())\n        if len(prfxs) &gt; 1:\n            log.debug(\"We have more than one prefix avail: %s\" % prfxs)\n        elif len(prfxs) == 0:\n            return \"\"\n        if set(prfxs) == {\"groups\", \"fof_subhalo_tab\"}:\n            return \"groups\"  # \"groups\" over \"fof_subhalo_tab\"\n        return prfxs[0]\n\n    @classmethod\n    def validate_path(\n        cls, path: Union[str, os.PathLike], *args, expect_grp=False, **kwargs\n    ) -&gt; CandidateStatus:\n        \"\"\"\n        Check if path is valid for this interface.\n        Parameters\n        ----------\n        path: str, os.PathLike\n            path to check\n        args\n        kwargs\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        path = str(path)\n        possibly_valid = CandidateStatus.NO\n        iszarr = path.rstrip(\"/\").endswith(\".zarr\")\n        if path.endswith(\".hdf5\") or iszarr:\n            possibly_valid = CandidateStatus.MAYBE\n        if os.path.isdir(path):\n            files = os.listdir(path)\n            sufxs = [f.split(\".\")[-1] for f in files]\n            if not iszarr and len(set(sufxs)) &gt; 1:\n                possibly_valid = CandidateStatus.NO\n            if sufxs[0] == \"hdf5\":\n                possibly_valid = CandidateStatus.MAYBE\n        if possibly_valid != CandidateStatus.NO:\n            metadata_raw = load_metadata(path, **kwargs)\n            # need some silly combination of attributes to be sure\n            if all([k in metadata_raw for k in [\"/Header\"]]):\n                # identifying snapshot or group catalog\n                is_snap = all(\n                    [\n                        k in metadata_raw[\"/Header\"]\n                        for k in [\"NumPart_ThisFile\", \"NumPart_Total\"]\n                    ]\n                )\n                is_grp = all(\n                    [\n                        k in metadata_raw[\"/Header\"]\n                        for k in [\"Ngroups_ThisFile\", \"Ngroups_Total\"]\n                    ]\n                )\n                if is_grp:\n                    return CandidateStatus.MAYBE\n                if is_snap and not expect_grp:\n                    return CandidateStatus.MAYBE\n        return CandidateStatus.NO\n\n    def register_field(self, parttype, name=None, description=\"\"):\n        res = self.data.register_field(parttype, name=name, description=description)\n        return res\n\n    def merge_data(\n        self, secondobj, fieldname_suffix=\"\", root_group: Optional[str] = None\n    ):\n        data = self.data\n        if root_group is not None:\n            if root_group not in data._containers:\n                data.add_container(root_group)\n            data = self.data[root_group]\n        for k in secondobj.data:\n            key = k + fieldname_suffix\n            if key not in data:\n                data[key] = secondobj.data[k]\n            else:\n                log.debug(\"Not overwriting field '%s' during merge_data.\" % key)\n            secondobj.data.fieldrecipes_kwargs[\"snap\"] = self\n\n    def merge_hints(self, secondobj):\n        # merge hints from snap and catalog\n        for h in secondobj.hints:\n            if h not in self.hints:\n                self.hints[h] = secondobj.hints[h]\n            elif isinstance(self.hints[h], dict):\n                # merge dicts\n                for k in secondobj.hints[h]:\n                    if k not in self.hints[h]:\n                        self.hints[h][k] = secondobj.hints[h][k]\n            else:\n                pass  # nothing to do; we do not overwrite with catalog props\n\n    @classmethod\n    def _clean_metadata_from_raw(cls, rawmetadata):\n        \"\"\"\n        Set metadata from raw metadata.\n        \"\"\"\n        metadata = dict()\n        if \"/Header\" in rawmetadata:\n            header = rawmetadata[\"/Header\"]\n            if \"Redshift\" in header:\n                metadata[\"redshift\"] = float(header[\"Redshift\"])\n                metadata[\"z\"] = metadata[\"redshift\"]\n            if \"BoxSize\" in header:\n                # can be scalar or array\n                metadata[\"boxsize\"] = header[\"BoxSize\"]\n            if \"Time\" in header:\n                metadata[\"time\"] = float(header[\"Time\"])\n                metadata[\"t\"] = metadata[\"time\"]\n        return metadata\n\n    def _set_metadata(self):\n        \"\"\"\n        Set metadata from header and config.\n        \"\"\"\n        md = self._clean_metadata_from_raw(self._metadata_raw)\n        self.metadata = md\n</code></pre>"},{"location":"api_docs/#scida.customs.gadgetstyle.dataset.GadgetStyleSnapshot.__init__","title":"<code>__init__(path, chunksize='auto', virtualcache=True, **kwargs)</code>","text":"<p>We define gadget-style snapshots as nbody/hydrodynamical simulation snapshots that follow the common /PartType0, /PartType1 grouping scheme.</p> Source code in <code>src/scida/customs/gadgetstyle/dataset.py</code> <pre><code>def __init__(self, path, chunksize=\"auto\", virtualcache=True, **kwargs) -&gt; None:\n    \"\"\"We define gadget-style snapshots as nbody/hydrodynamical simulation snapshots that follow\n    the common /PartType0, /PartType1 grouping scheme.\"\"\"\n    self.boxsize = np.nan\n    super().__init__(path, chunksize=chunksize, virtualcache=virtualcache, **kwargs)\n\n    defaultattributes = [\"config\", \"header\", \"parameters\"]\n    for k in self._metadata_raw:\n        name = k.strip(\"/\").lower()\n        if name in defaultattributes:\n            self.__dict__[name] = self._metadata_raw[k]\n            if \"BoxSize\" in self.__dict__[name]:\n                self.boxsize = self.__dict__[name][\"BoxSize\"]\n            elif \"Boxsize\" in self.__dict__[name]:\n                self.boxsize = self.__dict__[name][\"Boxsize\"]\n\n    sanity_check = kwargs.get(\"sanity_check\", False)\n    key_nparts = \"NumPart_Total\"\n    key_nparts_hw = \"NumPart_Total_HighWord\"\n    if sanity_check and key_nparts in self.header and key_nparts_hw in self.header:\n        nparts = self.header[key_nparts_hw] * 2**32 + self.header[key_nparts]\n        for i, n in enumerate(nparts):\n            pkey = \"PartType%i\" % i\n            if pkey in self.data:\n                pdata = self.data[pkey]\n                fkey = next(iter(pdata.keys()))\n                nparts_loaded = pdata[fkey].shape[0]\n                if nparts_loaded != n:\n                    raise ValueError(\n                        \"Number of particles in header (%i) does not match number of particles loaded (%i) \"\n                        \"for particle type %i\" % (n, nparts_loaded, i)\n                    )\n</code></pre>"},{"location":"api_docs/#scida.customs.gadgetstyle.dataset.GadgetStyleSnapshot.validate_path","title":"<code>validate_path(path, *args, expect_grp=False, **kwargs)</code>  <code>classmethod</code>","text":"<p>Check if path is valid for this interface.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, PathLike]</code> <p>path to check</p> required <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> Source code in <code>src/scida/customs/gadgetstyle/dataset.py</code> <pre><code>@classmethod\ndef validate_path(\n    cls, path: Union[str, os.PathLike], *args, expect_grp=False, **kwargs\n) -&gt; CandidateStatus:\n    \"\"\"\n    Check if path is valid for this interface.\n    Parameters\n    ----------\n    path: str, os.PathLike\n        path to check\n    args\n    kwargs\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    path = str(path)\n    possibly_valid = CandidateStatus.NO\n    iszarr = path.rstrip(\"/\").endswith(\".zarr\")\n    if path.endswith(\".hdf5\") or iszarr:\n        possibly_valid = CandidateStatus.MAYBE\n    if os.path.isdir(path):\n        files = os.listdir(path)\n        sufxs = [f.split(\".\")[-1] for f in files]\n        if not iszarr and len(set(sufxs)) &gt; 1:\n            possibly_valid = CandidateStatus.NO\n        if sufxs[0] == \"hdf5\":\n            possibly_valid = CandidateStatus.MAYBE\n    if possibly_valid != CandidateStatus.NO:\n        metadata_raw = load_metadata(path, **kwargs)\n        # need some silly combination of attributes to be sure\n        if all([k in metadata_raw for k in [\"/Header\"]]):\n            # identifying snapshot or group catalog\n            is_snap = all(\n                [\n                    k in metadata_raw[\"/Header\"]\n                    for k in [\"NumPart_ThisFile\", \"NumPart_Total\"]\n                ]\n            )\n            is_grp = all(\n                [\n                    k in metadata_raw[\"/Header\"]\n                    for k in [\"Ngroups_ThisFile\", \"Ngroups_Total\"]\n                ]\n            )\n            if is_grp:\n                return CandidateStatus.MAYBE\n            if is_snap and not expect_grp:\n                return CandidateStatus.MAYBE\n    return CandidateStatus.NO\n</code></pre>"},{"location":"api_docs/#scida.customs.gadgetstyle.series","title":"<code>series</code>","text":""},{"location":"api_docs/#scida.customs.gadgetstyle.series.GadgetStyleSimulation","title":"<code>GadgetStyleSimulation</code>","text":"<p>             Bases: <code>DatasetSeries</code></p> <p>A series representing a gadgetstyle simulation.</p> Source code in <code>src/scida/customs/gadgetstyle/series.py</code> <pre><code>class GadgetStyleSimulation(DatasetSeries):\n    \"\"\"A series representing a gadgetstyle simulation.\"\"\"\n\n    def __init__(\n        self,\n        path,\n        prefix_dict: Optional[Dict] = None,\n        subpath_dict: Optional[Dict] = None,\n        arg_dict: Optional[Dict] = None,\n        lazy=True,\n        async_caching=False,\n        **interface_kwargs\n    ):\n        self.path = path\n        self.name = os.path.basename(path)\n        if prefix_dict is None:\n            prefix_dict = dict()\n        if subpath_dict is None:\n            subpath_dict = dict()\n        if arg_dict is None:\n            arg_dict = dict()\n        p = Path(path)\n        if not (p.exists()):\n            raise ValueError(\"Specified path '%s' does not exist.\" % path)\n        paths_dict = dict()\n        keys = []\n        for d in [prefix_dict, subpath_dict, arg_dict]:\n            keys.extend(list(d.keys()))\n        keys = set(keys)\n        for k in keys:\n            subpath = subpath_dict.get(k, \"output\")\n            sp = p / subpath\n\n            prefix = _get_snapshotfolder_prefix(sp)\n            prefix = prefix_dict.get(k, prefix)\n            if not sp.exists():\n                if k != \"paths\":\n                    continue  # do not require optional sources\n                raise ValueError(\"Specified path '%s' does not exist.\" % (p / subpath))\n            fns = os.listdir(sp)\n            prfxs = set([f.split(\"_\")[0] for f in fns if f.startswith(prefix)])\n            if len(prfxs) == 0:\n                raise ValueError(\n                    \"Could not find any files with prefix '%s' in '%s'.\" % (prefix, sp)\n                )\n            prfx = prfxs.pop()\n\n            paths = sorted([p for p in sp.glob(prfx + \"_*\")])\n            # sometimes there are backup folders with different suffix, exclude those.\n            paths = [\n                p\n                for p in paths\n                if str(p).split(\"_\")[-1].isdigit() or str(p).endswith(\".hdf5\")\n            ]\n            paths_dict[k] = paths\n\n        # make sure we have the same amount of paths respectively\n        length = None\n        for k in paths_dict.keys():\n            paths = paths_dict[k]\n            if length is None:\n                length = len(paths)\n            else:\n                assert length == len(paths)\n\n        paths = paths_dict.pop(\"paths\", None)\n        if paths is None:\n            raise ValueError(\"Could not find any snapshot paths.\")\n        p = paths[0]\n        cls = _determine_type(p)[1][0]\n\n        mixins = _determine_mixins(path=p)\n        cls = create_MixinDataset(cls, mixins)\n\n        kwargs = {arg_dict.get(k, \"catalog\"): paths_dict[k] for k in paths_dict.keys()}\n        kwargs.update(**interface_kwargs)\n\n        super().__init__(\n            paths, datasetclass=cls, lazy=lazy, async_caching=async_caching, **kwargs\n        )\n</code></pre>"},{"location":"api_docs/#scida.discovertypes","title":"<code>discovertypes</code>","text":""},{"location":"api_docs/#scida.fields","title":"<code>fields</code>","text":""},{"location":"api_docs/#scida.fields.FieldContainer","title":"<code>FieldContainer</code>","text":"<p>             Bases: <code>MutableMapping</code></p> <p>A mutable collection of fields. Attempt to construct from derived fields recipes if needed.</p> Source code in <code>src/scida/fields.py</code> <pre><code>class FieldContainer(MutableMapping):\n    \"\"\"A mutable collection of fields. Attempt to construct from derived fields recipes\n    if needed.\"\"\"\n\n    def __init__(\n        self,\n        *args,\n        fieldrecipes_kwargs=None,\n        containers=None,\n        aliases=None,\n        withunits=False,\n        ureg=None,\n        parent: Optional[FieldContainer] = None,\n        **kwargs,\n    ):\n        if aliases is None:\n            aliases = {}\n        if fieldrecipes_kwargs is None:\n            fieldrecipes_kwargs = {}\n        self.aliases = aliases\n        self.name = kwargs.pop(\"name\", None)\n        self._fields: Dict[str, da.Array] = {}\n        self._fields.update(*args, **kwargs)\n        self._fieldrecipes = {}\n        self._fieldlength = None\n        self.fieldrecipes_kwargs = fieldrecipes_kwargs\n        self.withunits = withunits\n        self._ureg: Optional[pint.UnitRegistry] = ureg\n        self._containers: Dict[\n            str, FieldContainer\n        ] = dict()  # other containers as subgroups\n        if containers is not None:\n            for k in containers:\n                self.add_container(k)\n        self.internals = [\"uid\"]  # names of internal fields/groups\n        self.parent = parent\n\n    def set_ureg(self, ureg):\n        self._ureg = ureg\n\n    def get_ureg(self):\n        keys = self.keys(withgroups=False, withrecipes=False, withinternal=True)\n        for k in keys:\n            if hasattr(self[k], \"units\"):\n                if isinstance(self[k].units, pint.Unit):\n                    self._ureg = self[k].units._REGISTRY\n        if self._ureg is not None:\n            return self._ureg\n        return None\n\n    def copy_skeleton(self) -&gt; FieldContainer:\n        res = FieldContainer()\n        for k, cntr in self._containers.items():\n            res[k] = cntr.copy_skeleton()\n        return res\n\n    def info(self, level=0, name: Optional[str] = None) -&gt; str:\n        rep = \"\"\n        length = self.fieldlength\n        count = self.fieldcount\n        if name is None:\n            name = self.name\n        ncontainers = len(self._containers)\n        statstrs = []\n        if length is not None and length &gt; 0:\n            statstrs.append(\"fields: %i\" % count)\n            statstrs.append(\"entries: %i\" % length)\n        if ncontainers &gt; 0:\n            statstrs.append(\"containers: %i\" % ncontainers)\n        if len(statstrs) &gt; 0:\n            statstr = \", \".join(statstrs)\n            rep += sprint((level + 1) * \"+\", name, \"(%s)\" % statstr)\n        for k in sorted(self._containers.keys()):\n            v = self._containers[k]\n            rep += v.info(level=level + 1)\n        return rep\n\n    def merge(self, collection, overwrite=True):\n        if not isinstance(collection, FieldContainer):\n            raise TypeError(\"Can only merge FieldContainers.\")\n        # TODO: support nested containers\n        for k in collection._containers:\n            if k not in self._containers:\n                continue\n            if overwrite:\n                c1 = self._containers[k]\n                c2 = collection._containers[k]\n            else:\n                c1 = collection._containers[k]\n                c2 = self._containers[k]\n            c1._fields.update(**c2._fields)\n            c1._fieldrecipes.update(**c2._fieldrecipes)\n\n    @property\n    def fieldcount(self):\n        rcps = set(self._fieldrecipes)\n        flds = set([k for k in self._fields if k not in self.internals])\n        ntot = len(rcps | flds)\n        return ntot\n\n    @property\n    def fieldlength(self):\n        if self._fieldlength is not None:\n            return self._fieldlength\n        fvals = self._fields.values()\n        itr = iter(fvals)\n        if len(fvals) == 0:\n            # can we infer from recipes?\n            if len(self._fieldrecipes) &gt; 0:\n                # get first recipe\n                name = next(iter(self._fieldrecipes.keys()))\n                first = self._getitem(name, evaluate_recipe=True)\n            else:\n                return None\n        else:\n            first = next(itr)\n        if all(first.shape[0] == v.shape[0] for v in self._fields.values()):\n            self._fieldlength = first.shape[0]\n            return self._fieldlength\n        else:\n            return None\n\n    def keys(\n        self, withgroups=True, withrecipes=True, withinternal=False, withfields=True\n    ):\n        fieldkeys = []\n        recipekeys = []\n        if withfields:\n            fieldkeys = list(self._fields.keys())\n            if not withinternal:\n                for ikey in self.internals:\n                    if ikey in fieldkeys:\n                        fieldkeys.remove(ikey)\n        if withrecipes:\n            recipekeys = self._fieldrecipes.keys()\n        fieldkeys = list(set(fieldkeys) | set(recipekeys))\n        if withgroups:\n            groupkeys = self._containers.keys()\n            fieldkeys = list(set(fieldkeys) | set(groupkeys))\n        return sorted(fieldkeys)\n\n    def items(self, withrecipes=True, withfields=True, evaluate=True):\n        return (\n            (k, self._getitem(k, evaluate_recipe=evaluate))\n            for k in self.keys(withrecipes=withrecipes, withfields=withfields)\n        )\n\n    def values(self, evaluate=True):\n        return (self._getitem(k, evaluate_recipe=evaluate) for k in self.keys())\n\n    def register_field(\n        self,\n        containernames=None,\n        name: Optional[str] = None,\n        description=\"\",\n        units=None,\n    ):\n        # we only construct field upon first call to it (default)\n        # if to_containers, we register to the respective children containers\n        containers = []\n        if isinstance(containernames, list):\n            containers = [self._containers[c] for c in containernames]\n        elif containernames == \"all\":\n            containers = self._containers.values()\n        elif containernames is None:\n            containers = [self]\n        elif isinstance(containernames, str):  # just a single container as a string?\n            containers.append(self._containers[containernames])\n        else:\n            raise ValueError(\"Unknown type.\")\n\n        def decorator(func, name=name, description=description, units=units):\n            if name is None:\n                name = func.__name__\n            for container in containers:\n                drvfields = container._fieldrecipes\n                drvfields[name] = DerivedFieldRecipe(\n                    name, func, description=description, units=units\n                )\n            return func\n\n        return decorator\n\n    def __setitem__(self, key, value):\n        if key in self.aliases:\n            key = self.aliases[key]\n        if isinstance(value, FieldContainer):\n            self._containers[key] = value\n        elif isinstance(value, DerivedFieldRecipe):\n            self._fieldrecipes[key] = value\n        else:\n            self._fields[key] = value\n\n    def __getitem__(self, key):\n        return self._getitem(key)\n\n    def __iter__(self):\n        return iter(self.keys())\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Return a string representation of the object.\n        Returns\n        -------\n        str\n        \"\"\"\n        txt = \"\"\n        txt += \"FieldContainer[containers=%s, fields=%s]\" % (\n            len(self._containers),\n            self.fieldcount,\n        )\n        return txt\n\n    @property\n    def dataframe(self):\n        return self.get_dataframe()\n\n    def get_dataframe(self, fields=None):\n        dss = {}\n        if fields is None:\n            fields = self.keys()\n        for k in fields:\n            idim = None\n            if k not in self.keys():\n                # could still be an index two 2D dataset\n                i = -1\n                while k[i:].isnumeric():\n                    i += -1\n                i += 1\n                if i == 0:\n                    raise ValueError(\"Field '%s' not found\" % k)\n                idim = int(k[i:])\n                k = k.split(k[i:])[0]\n            v = self[k]\n            assert v.ndim &lt;= 2  # cannot support more than 2 here...\n            if idim is not None:\n                if v.ndim &lt;= 1:\n                    raise ValueError(\"No second dimensional index for %s\" % k)\n                if idim &gt;= v.shape[1]:\n                    raise ValueError(\n                        \"Second dimensional index %i not defined for %s\" % (idim, k)\n                    )\n\n            if v.ndim &gt; 1:\n                for i in range(v.shape[1]):\n                    if idim is None or idim == i:\n                        dss[k + str(i)] = v[:, i]\n            else:\n                dss[k] = v\n        dfs = [dd.from_dask_array(v, columns=[k]) for k, v in dss.items()]\n        ddf = dd.concat(dfs, axis=1)\n        return ddf\n\n    def add_alias(self, alias, name):\n        self.aliases[alias] = name\n\n    def add_container(self, key, **kwargs):\n        tkwargs = dict(**kwargs)\n        if \"name\" not in tkwargs:\n            tkwargs[\"name\"] = key\n        self._containers[key] = FieldContainer(\n            fieldrecipes_kwargs=self.fieldrecipes_kwargs,\n            withunits=self.withunits,\n            parent=self,\n            **tkwargs,\n        )\n        self._containers[key].set_ureg(self.get_ureg())\n\n    def _getitem(\n        self, key, force_derived=False, update_dict=True, evaluate_recipe=True\n    ):\n        if key in self.aliases:\n            key = self.aliases[key]\n        if key in self._containers:\n            return self._containers[key]\n        if key in self._fields and not force_derived:\n            return self._fields[key]\n        else:\n            if key in self._fieldrecipes:\n                if not evaluate_recipe:\n                    return self._fieldrecipes[key]\n                field = self._instantiate_field(key)\n                if update_dict:\n                    self._fields[key] = field\n                return field\n            else:\n                raise KeyError(\"Unknown field '%s'\" % key)\n\n    def _instantiate_field(self, key):\n        func = self._fieldrecipes[key].func\n        units = self._fieldrecipes[key].units\n        accept_kwargs = inspect.getfullargspec(func).varkw is not None\n        func_kwargs = get_kwargs(func)\n        dkwargs = self.fieldrecipes_kwargs\n        ureg = None\n        if \"ureg\" not in dkwargs:\n            ureg = self.get_ureg()\n            dkwargs[\"ureg\"] = ureg\n        # first, we overwrite all optional arguments with class instance defaults where func kwarg is None\n        kwargs = {\n            k: dkwargs[k]\n            for k in (\n                set(dkwargs) &amp; set([k for k, v in func_kwargs.items() if v is None])\n            )\n        }\n        # next, we add all optional arguments if func is accepting **kwargs and varname not yet in signature\n        if accept_kwargs:\n            kwargs.update(\n                **{\n                    k: v\n                    for k, v in dkwargs.items()\n                    if k not in inspect.getfullargspec(func).args\n                }\n            )\n        # finally, instantiate field\n        field = func(self, **kwargs)\n        if self.withunits and units is not None:\n            if not hasattr(field, \"units\"):\n                field = field * units\n            else:\n                if field.units != units:\n                    # if unit is present, but unit from metadata is unknown,\n                    # we stick with the former\n                    if not (hasattr(units, \"units\") and str(units.units) == \"unknown\"):\n                        try:\n                            field = field.to(units)\n                        except pint.errors.DimensionalityError as e:\n                            print(e)\n                            raise ValueError(\n                                \"Field '%s' units '%s' do not match '%s'\"\n                                % (key, field.units, units)\n                            )\n        return field\n\n    def __delitem__(self, key):\n        if key in self._fieldrecipes:\n            del self._fieldrecipes[key]\n        if key in self._containers:\n            del self._containers[key]\n        elif key in self._fields:\n            del self._fields[key]\n        else:\n            raise KeyError(\"Unknown key '%s'\" % key)\n\n    def __len__(self):\n        return len(self.keys())\n\n    def get(self, key, value=None, allow_derived=True, force_derived=False):\n        if key in self._fieldrecipes and not allow_derived:\n            raise KeyError(\"Field '%s' is derived (allow_derived=False)\" % key)\n        else:\n            try:\n                return self._getitem(\n                    key, force_derived=force_derived, update_dict=False\n                )\n            except KeyError:\n                return value\n</code></pre>"},{"location":"api_docs/#scida.fields.FieldContainer.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the object.</p> <p>Returns:</p> Type Description <code>str</code> Source code in <code>src/scida/fields.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Return a string representation of the object.\n    Returns\n    -------\n    str\n    \"\"\"\n    txt = \"\"\n    txt += \"FieldContainer[containers=%s, fields=%s]\" % (\n        len(self._containers),\n        self.fieldcount,\n    )\n    return txt\n</code></pre>"},{"location":"api_docs/#scida.helpers_hdf5","title":"<code>helpers_hdf5</code>","text":""},{"location":"api_docs/#scida.helpers_hdf5.create_mergedhdf5file","title":"<code>create_mergedhdf5file(fn, files, max_workers=None, virtual=True, groupwise_shape=False)</code>","text":"<p>Creates a virtual hdf5 file from list of given files. Virtual by default.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> required <code>files</code> required <code>max_workers</code> <code>None</code> <code>virtual</code> <code>True</code> <code>groupwise_shape</code> <code>False</code> Source code in <code>src/scida/helpers_hdf5.py</code> <pre><code>def create_mergedhdf5file(\n    fn, files, max_workers=None, virtual=True, groupwise_shape=False\n):\n    \"\"\"\n    Creates a virtual hdf5 file from list of given files. Virtual by default.\n\n    Parameters\n    ----------\n    fn: file to write to\n    files: files to merge\n    max_workers: parallel workers to process files\n    virtual: whether to create linked (\"virtual\") dataset on disk (otherwise copy)\n    groupwise_shape: Require shapes to be the same within a group\n\n    Returns\n    -------\n\n    \"\"\"\n    if max_workers is None:\n        # read from config\n        config = get_config()\n        max_workers = config.get(\"nthreads\", 16)\n    # first obtain all datasets and groups\n    trees = [{} for i in range(len(files))]\n\n    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n        result = executor.map(walk_hdf5file, files, trees)\n    result = list(result)\n\n    groups = set([item for r in result for item in r[\"groups\"]])\n    datasets = set([item[0] for r in result for item in r[\"datasets\"]])\n\n    def todct(lst):\n        return {item[0]: (item[1], item[2]) for item in lst[\"datasets\"]}\n\n    dcts = [todct(lst) for lst in result]\n    shapes = OrderedDict((d, {}) for d in datasets)\n\n    def shps(i, k, s):\n        return shapes[k].update({i: s[0]}) if s is not None else None\n\n    [shps(i, k, dct.get(k)) for k in datasets for i, dct in enumerate(dcts)]\n    dtypes = {}\n\n    def dtps(k, s):\n        return dtypes.update({k: s[1]}) if s is not None else None\n\n    [dtps(k, dct.get(k)) for k in datasets for i, dct in enumerate(dcts)]\n\n    # get shapes of the respective chunks\n    chunks = {}\n    for field in sorted(shapes.keys()):\n        chunks[field] = [[k, shapes[field][k][0]] for k in shapes[field]]\n    groupchunks = {}\n\n    # assert that all datasets in a given group have the same chunks.\n    for group in sorted(groups):\n        if group == \"/\":\n            group = \"\"  # this is needed to have consistent levels for 0th level\n        groupfields = [f for f in shapes.keys() if f.startswith(group)]\n        groupfields = [f for f in groupfields if f.count(\"/\") - 1 == group.count(\"/\")]\n        groupfields = sorted(groupfields)\n        if len(groupfields) == 0:\n            continue\n        arr0 = chunks[groupfields[0]]\n        for field in groupfields[1:]:\n            arr = np.array(chunks[field])\n            if groupwise_shape and not np.array_equal(arr0, arr):\n                raise ValueError(\"Requiring same shape (see 'groupwise_shape' flag)\")\n            # then save the chunking information for this group\n            groupchunks[field] = arr0\n\n    # next fill merger file\n    with h5py.File(fn, \"w\", libver=\"latest\") as hf:\n        # create groups\n        for group in sorted(groups):\n            if group == \"/\":\n                continue  # nothing to do.\n            hf.create_group(group)\n            groupfields = [\n                field\n                for field in shapes.keys()\n                if field.startswith(group) and field.count(\"/\") - 1 == group.count(\"/\")\n            ]\n            if len(groupfields) == 0:\n                continue\n\n            # fill fields\n            if virtual:\n                # for virtual datasets, iterate over all fields and concat each file to virtual dataset\n                for field in groupfields:\n                    totentries = np.array([k[1] for k in chunks[field]]).sum()\n                    newshape = (totentries,) + shapes[field][next(iter(shapes[field]))][\n                        1:\n                    ]\n\n                    # create virtual sources\n                    vsources = []\n                    for k in shapes[field]:\n                        vsources.append(\n                            h5py.VirtualSource(\n                                files[k],\n                                name=field,\n                                shape=shapes[field][k],\n                                dtype=dtypes[field],\n                            )\n                        )\n                    layout = h5py.VirtualLayout(\n                        shape=tuple(newshape), dtype=dtypes[field]\n                    )\n\n                    # fill virtual dataset\n                    offset = 0\n                    for vsource in vsources:\n                        length = vsource.shape[0]\n                        layout[offset : offset + length] = vsource\n                        offset += length\n                    assert (\n                        newshape[0] == offset\n                    )  # make sure we filled the array up fully.\n                    hf.create_virtual_dataset(field, layout)\n            else:  # copied dataset. For performance, we iterate differently: Loop over each file's fields\n                for field in groupfields:\n                    totentries = np.array([k[1] for k in chunks[field]]).sum()\n                    extrashapes = shapes[field][next(iter(shapes[field]))][1:]\n                    newshape = (totentries,) + extrashapes\n                    hf.create_dataset(field, shape=newshape, dtype=dtypes[field])\n                counters = {field: 0 for field in groupfields}\n                for k, fl in enumerate(files):\n                    with h5py.File(fl) as hf_load:\n                        for field in groupfields:\n                            n = shapes[field].get(k, [0, 0])[0]\n                            if n == 0:\n                                continue\n                            offset = counters[field]\n                            hf[field][offset : offset + n] = hf_load[field]\n                            counters[field] = offset + n\n\n        # save information regarding chunks\n        grp = hf.create_group(\"_chunks\")\n        for k, v in groupchunks.items():\n            grp.attrs[k] = v\n\n        # write the attributes\n        # find attributes that change across data sets\n        attrs_key_lists = [\n            list(v[\"attrs\"].keys()) for v in result\n        ]  # attribute paths for each file\n        attrspaths_all = set().union(*attrs_key_lists)\n        attrspaths_intersec = set(attrspaths_all).intersection(*attrs_key_lists)\n        attrspath_diff = attrspaths_all.difference(attrspaths_intersec)\n        if attrspaths_all != attrspaths_intersec:\n            # if difference only stems from missing datasets (and their assoc. attrs); thats fine\n            if not attrspath_diff.issubset(datasets):\n                raise NotImplementedError(\n                    \"Some attribute paths not present in each partial data file.\"\n                )\n        # check for common key+values across all files\n        attrs_same = {}\n        attrs_differ = {}\n\n        nfiles = len(files)\n\n        for apath in sorted(attrspaths_all):\n            attrs_same[apath] = {}\n            attrs_differ[apath] = {}\n            attrsnames = set().union(\n                *[\n                    result[i][\"attrs\"][apath]\n                    for i in range(nfiles)\n                    if apath in result[i][\"attrs\"]\n                ]\n            )\n            for k in attrsnames:\n                # we ignore apaths and k existing in some files.\n                attrvallist = [\n                    result[i][\"attrs\"][apath][k]\n                    for i in range(nfiles)\n                    if apath in result[i][\"attrs\"] and k in result[i][\"attrs\"][apath]\n                ]\n                attrval0 = attrvallist[0]\n                if isinstance(attrval0, np.ndarray):\n                    if not (np.all([np.array_equal(attrval0, v) for v in attrvallist])):\n                        log.debug(\"%s: %s has different values.\" % (apath, k))\n                        attrs_differ[apath][k] = np.stack(attrvallist)\n                        continue\n                else:\n                    same = len(set(attrvallist)) == 1\n                    if isinstance(attrval0, np.floating):\n                        # for floats we do not require binary equality\n                        # (we had some incident...)\n                        same = np.allclose(attrval0, attrvallist)\n                    if not same:\n                        log.debug(\"%s: %s has different values.\" % (apath, k))\n                        attrs_differ[apath][k] = np.array(attrvallist)\n                        continue\n                attrs_same[apath][k] = attrval0\n        for apath in attrspaths_all:\n            for k, v in attrs_same.get(apath, {}).items():\n                hf[apath].attrs[k] = v\n            for k, v in attrs_differ.get(apath, {}).items():\n                hf[apath].attrs[k] = v\n</code></pre>"},{"location":"api_docs/#scida.helpers_misc","title":"<code>helpers_misc</code>","text":""},{"location":"api_docs/#scida.helpers_misc.RecursiveNamespace","title":"<code>RecursiveNamespace</code>","text":"<p>             Bases: <code>SimpleNamespace</code></p> Source code in <code>src/scida/helpers_misc.py</code> <pre><code>class RecursiveNamespace(types.SimpleNamespace):\n    # see https://stackoverflow.com/questions/2597278/python-load-variables-in-a-dict-into-namespace\n    def __init__(self, **kwargs):\n        \"\"\"Create a SimpleNamespace recursively\"\"\"\n        super().__init__(**kwargs)\n        self.__dict__.update({k: self.__elt(v) for k, v in kwargs.items()})\n\n    def __elt(self, elt):\n        \"\"\"Recurse into elt to create leaf namespace objects\"\"\"\n        if isinstance(elt, dict):\n            return type(self)(**elt)\n        if type(elt) in (list, tuple):\n            return [self.__elt(i) for i in elt]\n        return elt\n</code></pre>"},{"location":"api_docs/#scida.helpers_misc.RecursiveNamespace.__elt","title":"<code>__elt(elt)</code>","text":"<p>Recurse into elt to create leaf namespace objects</p> Source code in <code>src/scida/helpers_misc.py</code> <pre><code>def __elt(self, elt):\n    \"\"\"Recurse into elt to create leaf namespace objects\"\"\"\n    if isinstance(elt, dict):\n        return type(self)(**elt)\n    if type(elt) in (list, tuple):\n        return [self.__elt(i) for i in elt]\n    return elt\n</code></pre>"},{"location":"api_docs/#scida.helpers_misc.RecursiveNamespace.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Create a SimpleNamespace recursively</p> Source code in <code>src/scida/helpers_misc.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Create a SimpleNamespace recursively\"\"\"\n    super().__init__(**kwargs)\n    self.__dict__.update({k: self.__elt(v) for k, v in kwargs.items()})\n</code></pre>"},{"location":"api_docs/#scida.helpers_misc.map_blocks","title":"<code>map_blocks(func, *args, name=None, token=None, dtype=None, chunks=None, drop_axis=None, new_axis=None, enforce_ndim=False, meta=None, output_units=None, **kwargs)</code>","text":"<p>map_blocks with units</p> Source code in <code>src/scida/helpers_misc.py</code> <pre><code>def map_blocks(\n    func,\n    *args,\n    name=None,\n    token=None,\n    dtype=None,\n    chunks=None,\n    drop_axis=None,\n    new_axis=None,\n    enforce_ndim=False,\n    meta=None,\n    output_units=None,\n    **kwargs,\n):\n    \"\"\"map_blocks with units\"\"\"\n    da_kwargs = dict(\n        name=name,\n        token=token,\n        dtype=dtype,\n        chunks=chunks,\n        drop_axis=drop_axis,\n        new_axis=new_axis,\n        enforce_ndim=enforce_ndim,\n        meta=meta,\n    )\n    res = da.map_blocks(\n        func,\n        *args,\n        **da_kwargs,\n        **kwargs,\n    )\n    if output_units is not None:\n        if hasattr(res, \"magnitude\"):\n            log.info(\"map_blocks output already has units, overwriting.\")\n            res = res.magnitude * output_units\n        res = res * output_units\n\n    return res\n</code></pre>"},{"location":"api_docs/#scida.helpers_misc.sprint","title":"<code>sprint(*args, end='\\n', **kwargs)</code>","text":"<p>print to string</p> Source code in <code>src/scida/helpers_misc.py</code> <pre><code>def sprint(*args, end=\"\\n\", **kwargs):\n    \"\"\"print to string\"\"\"\n    output = io.StringIO()\n    print(*args, file=output, end=end, **kwargs)\n    contents = output.getvalue()\n    output.close()\n    return contents\n</code></pre>"},{"location":"api_docs/#scida.interface","title":"<code>interface</code>","text":""},{"location":"api_docs/#scida.interface.BaseDataset","title":"<code>BaseDataset</code>","text":"Source code in <code>src/scida/interface.py</code> <pre><code>class BaseDataset(metaclass=MixinMeta):\n    def __init__(\n        self,\n        path,\n        chunksize=\"auto\",\n        virtualcache=True,\n        overwritecache=False,\n        fileprefix=\"\",\n        hints=None,\n        **kwargs\n    ):\n        super().__init__()\n        self.hints = hints if hints is not None else {}\n        self.path = path\n        self.file = None\n        # need this 'tempfile' reference to keep garbage collection away for the tempfile\n        self.tempfile = None\n        self.location = str(path)\n        self.chunksize = chunksize\n        self.virtualcache = virtualcache\n        self.overwritecache = overwritecache\n        self.withunits = kwargs.get(\"units\", False)\n\n        # Let's find the data and metadata for the object at 'path'\n        self.metadata = {}\n        self._metadata_raw = {}\n        self.data = FieldContainer(withunits=self.withunits)\n\n        if not os.path.exists(self.path):\n            raise Exception(\"Specified path '%s' does not exist.\" % self.path)\n\n        loadkwargs = dict(\n            overwrite=self.overwritecache,\n            fileprefix=fileprefix,\n            virtualcache=virtualcache,\n            derivedfields_kwargs=dict(snap=self),\n            token=self.__dask_tokenize__(),\n            withunits=self.withunits,\n        )\n        if \"choose_prefix\" in kwargs:\n            loadkwargs[\"choose_prefix\"] = kwargs[\"choose_prefix\"]\n\n        res = scida.io.load(path, **loadkwargs)\n        self.data = res[0]\n        self._metadata_raw = res[1]\n        self.file = res[2]\n        self.tempfile = res[3]\n        self._cached = False\n\n        # any identifying metadata?\n        if \"dsname\" not in self.hints:\n            candidates = check_config_for_dataset(self._metadata_raw, path=self.path)\n            if len(candidates) &gt; 0:\n                dsname = candidates[0]\n                log.debug(\"Dataset is identified as '%s'.\" % dsname)\n                self.hints[\"dsname\"] = dsname\n\n    def _info_custom(self):\n        \"\"\"\n        Custom information to be printed by info() method.\n        Returns\n        -------\n\n        \"\"\"\n        return None\n\n    def info(self, listfields: bool = False):\n        \"\"\"\n        Print information about the dataset.\n        Parameters\n        ----------\n        listfields: bool\n            If True, list all fields in the dataset.\n\n        Returns\n        -------\n\n        \"\"\"\n        rep = \"\"\n        rep += \"class: \" + sprint(self.__class__.__name__)\n        props = self._repr_dict()\n        for k, v in props.items():\n            rep += sprint(\"%s: %s\" % (k, v))\n        if self._info_custom() is not None:\n            rep += self._info_custom()\n        rep += sprint(\"=== data ===\")\n        rep += self.data.info(name=\"root\")\n        rep += sprint(\"============\")\n        print(rep)\n\n    def _repr_dict(self) -&gt; Dict[str, str]:\n        \"\"\"\n        Return a dictionary of properties to be printed by __repr__ method.\n        Returns\n        -------\n        dict\n        \"\"\"\n        props = dict()\n        props[\"source\"] = self.path\n        return props\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Return a string representation of the object.\n        Returns\n        -------\n        str\n        \"\"\"\n        props = self._repr_dict()\n        clsname = self.__class__.__name__\n        result = clsname + \"[\"\n        for k, v in props.items():\n            result += \"%s=%s, \" % (k, v)\n        result = result[:-2] + \"]\"\n        return result\n\n    def _repr_pretty_(self, p, cycle):\n        \"\"\"\n        Pretty print representation for IPython.\n        Parameters\n        ----------\n        p\n        cycle\n\n        Returns\n        -------\n\n        \"\"\"\n        rpr = self.__repr__()\n        p.text(rpr)\n\n    def __init_subclass__(cls, *args, **kwargs):\n        super().__init_subclass__(*args, **kwargs)\n        if cls.__name__ == \"Delay\":\n            return  # nothing to register for Delay objects\n        if \"Mixin\" in cls.__name__:\n            return  # do not register classes with Mixins\n        dataset_type_registry[cls.__name__] = cls\n\n    @classmethod\n    @abc.abstractmethod\n    def validate_path(cls, path, *args, **kwargs):\n        \"\"\"\n        Validate whether the given path is a valid path for this dataset.\n        Parameters\n        ----------\n        path\n        args\n        kwargs\n\n        Returns\n        -------\n\n        \"\"\"\n        return False\n\n    def __hash__(self) -&gt; int:\n        \"\"\"\n        Hash for Dataset instance to be derived from the file location.\n\n        Returns\n        -------\n        int\n        \"\"\"\n        # determinstic hash; note that hash() on a string is no longer deterministic in python3.\n        hash_value = (\n            int(hashlib.sha256(self.location.encode(\"utf-8\")).hexdigest(), 16)\n            % 10**10\n        )\n        return hash_value\n\n    def __getitem__(self, item):\n        return self.data[item]\n\n    def __dask_tokenize__(self) -&gt; int:\n        \"\"\"\n        Token for dask to be derived -- naively from the file location.\n        Returns\n        -------\n        int\n        \"\"\"\n        return self.__hash__()\n\n    def return_data(self) -&gt; FieldContainer:\n        \"\"\"\n        Return the data container.\n        Returns\n        -------\n\n        \"\"\"\n        return self.data\n\n    def save(\n        self,\n        fname,\n        fields: Union[\n            str, Dict[str, Union[List[str], Dict[str, da.Array]]], FieldContainer\n        ] = \"all\",\n        overwrite: bool = True,\n        zarr_kwargs: Optional[dict] = None,\n        cast_uints: bool = False,\n        extra_attrs: Optional[dict] = None,\n    ) -&gt; None:\n        \"\"\"\n        Save the dataset to a file using the 'zarr' format.\n        Parameters\n        ----------\n        fname: str\n            Filename to save to.\n        fields: str or dict\n            dictionary of dask arrays to save. If equal to 'all', save all fields in current dataset.\n        overwrite\n            overwrite existing file\n        zarr_kwargs\n            optional arguments to pass to zarr\n        cast_uints\n            need to potentially cast uints to ints for some compressions; TODO: clean this up\n\n        Returns\n        -------\n\n        \"\"\"\n        # We use zarr, as this way we have support to directly write into the file by the workers\n        # (rather than passing back the data chunk over the scheduler to the interface)\n        # Also, this way we can leverage new features, such as a large variety of compression methods.\n        # cast_uints: if true, we cast uints to ints; needed for some compressions (particularly zfp)\n        if zarr_kwargs is None:\n            zarr_kwargs = {}\n        store = zarr.DirectoryStore(fname, **zarr_kwargs)\n        root = zarr.group(store, overwrite=overwrite)\n\n        # Metadata\n        defaultattributes = [\"Config\", \"Header\", \"Parameters\"]\n        for dctname in defaultattributes:\n            if dctname.lower() in self.__dict__:\n                grp = root.create_group(dctname)\n                dct = self.__dict__[dctname.lower()]\n                for k, v in dct.items():\n                    v = make_serializable(v)\n                    grp.attrs[k] = v\n        if extra_attrs is not None:\n            for k, v in extra_attrs.items():\n                root.attrs[k] = v\n        # Data\n        tasks = []\n        ptypes = self.data.keys()\n        if isinstance(fields, dict):\n            ptypes = fields.keys()\n        elif isinstance(fields, str):\n            if not fields == \"all\":\n                raise ValueError(\"Invalid field specifier.\")\n        else:\n            raise ValueError(\"Invalid type for fields.\")\n        for p in ptypes:\n            root.create_group(p)\n            if fields == \"all\":\n                fieldkeys = self.data[p]\n            else:\n                if isinstance(fields[p], dict):\n                    fieldkeys = fields[p].keys()\n                else:\n                    fieldkeys = fields[p]\n            for k in fieldkeys:\n                if not isinstance(fields, str) and isinstance(fields[p], dict):\n                    arr = fields[p][k]\n                else:\n                    arr = self.data[p][k]\n                if hasattr(arr, \"magnitude\"):  # if we have units, remove those here\n                    # TODO: save units in metadata!\n                    arr = arr.magnitude\n                if np.any(np.isnan(arr.shape)):\n                    arr.compute_chunk_sizes()  # very inefficient (have to do it separately for every array)\n                    arr = arr.rechunk(chunks=\"auto\")\n                if cast_uints:\n                    if arr.dtype == np.uint64:\n                        arr = arr.astype(np.int64)\n                    elif arr.dtype == np.uint32:\n                        arr = arr.astype(np.int32)\n                task = da.to_zarr(\n                    arr, os.path.join(fname, p, k), overwrite=True, compute=False\n                )\n                tasks.append(task)\n        dask.compute(tasks)\n</code></pre>"},{"location":"api_docs/#scida.interface.BaseDataset.__dask_tokenize__","title":"<code>__dask_tokenize__()</code>","text":"<p>Token for dask to be derived -- naively from the file location.</p> <p>Returns:</p> Type Description <code>int</code> Source code in <code>src/scida/interface.py</code> <pre><code>def __dask_tokenize__(self) -&gt; int:\n    \"\"\"\n    Token for dask to be derived -- naively from the file location.\n    Returns\n    -------\n    int\n    \"\"\"\n    return self.__hash__()\n</code></pre>"},{"location":"api_docs/#scida.interface.BaseDataset.__hash__","title":"<code>__hash__()</code>","text":"<p>Hash for Dataset instance to be derived from the file location.</p> <p>Returns:</p> Type Description <code>int</code> Source code in <code>src/scida/interface.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"\n    Hash for Dataset instance to be derived from the file location.\n\n    Returns\n    -------\n    int\n    \"\"\"\n    # determinstic hash; note that hash() on a string is no longer deterministic in python3.\n    hash_value = (\n        int(hashlib.sha256(self.location.encode(\"utf-8\")).hexdigest(), 16)\n        % 10**10\n    )\n    return hash_value\n</code></pre>"},{"location":"api_docs/#scida.interface.BaseDataset.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the object.</p> <p>Returns:</p> Type Description <code>str</code> Source code in <code>src/scida/interface.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Return a string representation of the object.\n    Returns\n    -------\n    str\n    \"\"\"\n    props = self._repr_dict()\n    clsname = self.__class__.__name__\n    result = clsname + \"[\"\n    for k, v in props.items():\n        result += \"%s=%s, \" % (k, v)\n    result = result[:-2] + \"]\"\n    return result\n</code></pre>"},{"location":"api_docs/#scida.interface.BaseDataset.info","title":"<code>info(listfields=False)</code>","text":"<p>Print information about the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>listfields</code> <code>bool</code> <p>If True, list all fields in the dataset.</p> <code>False</code> Source code in <code>src/scida/interface.py</code> <pre><code>def info(self, listfields: bool = False):\n    \"\"\"\n    Print information about the dataset.\n    Parameters\n    ----------\n    listfields: bool\n        If True, list all fields in the dataset.\n\n    Returns\n    -------\n\n    \"\"\"\n    rep = \"\"\n    rep += \"class: \" + sprint(self.__class__.__name__)\n    props = self._repr_dict()\n    for k, v in props.items():\n        rep += sprint(\"%s: %s\" % (k, v))\n    if self._info_custom() is not None:\n        rep += self._info_custom()\n    rep += sprint(\"=== data ===\")\n    rep += self.data.info(name=\"root\")\n    rep += sprint(\"============\")\n    print(rep)\n</code></pre>"},{"location":"api_docs/#scida.interface.BaseDataset.return_data","title":"<code>return_data()</code>","text":"<p>Return the data container.</p> Source code in <code>src/scida/interface.py</code> <pre><code>def return_data(self) -&gt; FieldContainer:\n    \"\"\"\n    Return the data container.\n    Returns\n    -------\n\n    \"\"\"\n    return self.data\n</code></pre>"},{"location":"api_docs/#scida.interface.BaseDataset.save","title":"<code>save(fname, fields='all', overwrite=True, zarr_kwargs=None, cast_uints=False, extra_attrs=None)</code>","text":"<p>Save the dataset to a file using the 'zarr' format.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <p>Filename to save to.</p> required <code>fields</code> <code>Union[str, Dict[str, Union[List[str], Dict[str, Array]]], FieldContainer]</code> <p>dictionary of dask arrays to save. If equal to 'all', save all fields in current dataset.</p> <code>'all'</code> <code>overwrite</code> <code>bool</code> <p>overwrite existing file</p> <code>True</code> <code>zarr_kwargs</code> <code>Optional[dict]</code> <p>optional arguments to pass to zarr</p> <code>None</code> <code>cast_uints</code> <code>bool</code> <p>need to potentially cast uints to ints for some compressions; TODO: clean this up</p> <code>False</code> Source code in <code>src/scida/interface.py</code> <pre><code>def save(\n    self,\n    fname,\n    fields: Union[\n        str, Dict[str, Union[List[str], Dict[str, da.Array]]], FieldContainer\n    ] = \"all\",\n    overwrite: bool = True,\n    zarr_kwargs: Optional[dict] = None,\n    cast_uints: bool = False,\n    extra_attrs: Optional[dict] = None,\n) -&gt; None:\n    \"\"\"\n    Save the dataset to a file using the 'zarr' format.\n    Parameters\n    ----------\n    fname: str\n        Filename to save to.\n    fields: str or dict\n        dictionary of dask arrays to save. If equal to 'all', save all fields in current dataset.\n    overwrite\n        overwrite existing file\n    zarr_kwargs\n        optional arguments to pass to zarr\n    cast_uints\n        need to potentially cast uints to ints for some compressions; TODO: clean this up\n\n    Returns\n    -------\n\n    \"\"\"\n    # We use zarr, as this way we have support to directly write into the file by the workers\n    # (rather than passing back the data chunk over the scheduler to the interface)\n    # Also, this way we can leverage new features, such as a large variety of compression methods.\n    # cast_uints: if true, we cast uints to ints; needed for some compressions (particularly zfp)\n    if zarr_kwargs is None:\n        zarr_kwargs = {}\n    store = zarr.DirectoryStore(fname, **zarr_kwargs)\n    root = zarr.group(store, overwrite=overwrite)\n\n    # Metadata\n    defaultattributes = [\"Config\", \"Header\", \"Parameters\"]\n    for dctname in defaultattributes:\n        if dctname.lower() in self.__dict__:\n            grp = root.create_group(dctname)\n            dct = self.__dict__[dctname.lower()]\n            for k, v in dct.items():\n                v = make_serializable(v)\n                grp.attrs[k] = v\n    if extra_attrs is not None:\n        for k, v in extra_attrs.items():\n            root.attrs[k] = v\n    # Data\n    tasks = []\n    ptypes = self.data.keys()\n    if isinstance(fields, dict):\n        ptypes = fields.keys()\n    elif isinstance(fields, str):\n        if not fields == \"all\":\n            raise ValueError(\"Invalid field specifier.\")\n    else:\n        raise ValueError(\"Invalid type for fields.\")\n    for p in ptypes:\n        root.create_group(p)\n        if fields == \"all\":\n            fieldkeys = self.data[p]\n        else:\n            if isinstance(fields[p], dict):\n                fieldkeys = fields[p].keys()\n            else:\n                fieldkeys = fields[p]\n        for k in fieldkeys:\n            if not isinstance(fields, str) and isinstance(fields[p], dict):\n                arr = fields[p][k]\n            else:\n                arr = self.data[p][k]\n            if hasattr(arr, \"magnitude\"):  # if we have units, remove those here\n                # TODO: save units in metadata!\n                arr = arr.magnitude\n            if np.any(np.isnan(arr.shape)):\n                arr.compute_chunk_sizes()  # very inefficient (have to do it separately for every array)\n                arr = arr.rechunk(chunks=\"auto\")\n            if cast_uints:\n                if arr.dtype == np.uint64:\n                    arr = arr.astype(np.int64)\n                elif arr.dtype == np.uint32:\n                    arr = arr.astype(np.int32)\n            task = da.to_zarr(\n                arr, os.path.join(fname, p, k), overwrite=True, compute=False\n            )\n            tasks.append(task)\n    dask.compute(tasks)\n</code></pre>"},{"location":"api_docs/#scida.interface.BaseDataset.validate_path","title":"<code>validate_path(path, *args, **kwargs)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Validate whether the given path is a valid path for this dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> required <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> Source code in <code>src/scida/interface.py</code> <pre><code>@classmethod\n@abc.abstractmethod\ndef validate_path(cls, path, *args, **kwargs):\n    \"\"\"\n    Validate whether the given path is a valid path for this dataset.\n    Parameters\n    ----------\n    path\n    args\n    kwargs\n\n    Returns\n    -------\n\n    \"\"\"\n    return False\n</code></pre>"},{"location":"api_docs/#scida.interface.Dataset","title":"<code>Dataset</code>","text":"<p>             Bases: <code>BaseDataset</code></p> Source code in <code>src/scida/interface.py</code> <pre><code>class Dataset(BaseDataset):\n    @classmethod\n    def validate_path(cls, path, *args, **kwargs):\n        \"\"\"\n        Validate whether the given path is a valid path for this dataset.\n        Parameters\n        ----------\n        path\n        args\n        kwargs\n\n        Returns\n        -------\n\n        \"\"\"\n        return True\n\n    @classmethod\n    def _clean_metadata_from_raw(cls, rawmetadata):\n        return {}\n</code></pre>"},{"location":"api_docs/#scida.interface.Dataset.validate_path","title":"<code>validate_path(path, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Validate whether the given path is a valid path for this dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> required <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> Source code in <code>src/scida/interface.py</code> <pre><code>@classmethod\ndef validate_path(cls, path, *args, **kwargs):\n    \"\"\"\n    Validate whether the given path is a valid path for this dataset.\n    Parameters\n    ----------\n    path\n    args\n    kwargs\n\n    Returns\n    -------\n\n    \"\"\"\n    return True\n</code></pre>"},{"location":"api_docs/#scida.interface.Selector","title":"<code>Selector</code>","text":"<p>             Bases: <code>object</code></p> <p>Base Class for data selection decorator factory</p> Source code in <code>src/scida/interface.py</code> <pre><code>class Selector(object):\n    \"\"\"Base Class for data selection decorator factory\"\"\"\n\n    def __init__(self):\n        self.keys = None  # the keys we check for.\n        # holds a copy of the species' fields\n        self.data_backup = FieldContainer()\n        # holds the species' fields we operate on\n        self.data: FieldContainer = FieldContainer()\n\n    def __call__(self, fn, *args, **kwargs):\n        def newfn(*args, **kwargs):\n            # TODO: Add graceful exit/restore after exception in self.prepare\n            self.data_backup = args[0].data\n            self.data = args[0].data.copy_skeleton()\n            # deepdictkeycopy(self.data_backup, self.data)\n\n            self.prepare(*args, **kwargs)\n            if self.keys is None:\n                raise NotImplementedError(\n                    \"Subclass implementation needed for self.keys!\"\n                )\n            if kwargs.pop(\"dropkeys\", True):\n                for k in self.keys:\n                    kwargs.pop(k, None)\n            try:\n                result = fn(*args, **kwargs)\n                return result\n            finally:\n                self.finalize(*args, **kwargs)\n\n        return newfn\n\n    def prepare(self, *args, **kwargs) -&gt; None:\n        raise NotImplementedError(\"Subclass implementation needed!\")\n\n    def finalize(self, *args, **kwargs) -&gt; None:\n        args[0].data = self.data_backup\n</code></pre>"},{"location":"api_docs/#scida.io","title":"<code>io</code>","text":""},{"location":"api_docs/#scida.misc","title":"<code>misc</code>","text":""},{"location":"api_docs/#scida.misc.check_config_for_dataset","title":"<code>check_config_for_dataset(metadata, path=None, unique=True)</code>","text":"<p>Check whether the given dataset can be identified to be a certain simulation (type) by its metadata.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <p>metadata of the dataset used for identification</p> required <code>path</code> <code>Optional[str]</code> <p>path to the dataset, sometimes helpful for identification</p> <code>None</code> <code>unique</code> <code>bool</code> <p>whether to expect return to be unique</p> <code>True</code> Source code in <code>src/scida/misc.py</code> <pre><code>def check_config_for_dataset(metadata, path: Optional[str] = None, unique: bool = True):\n    \"\"\"\n    Check whether the given dataset can be identified to be a certain simulation (type) by its metadata.\n    Parameters\n    ----------\n    metadata: dict\n        metadata of the dataset used for identification\n    path: str\n        path to the dataset, sometimes helpful for identification\n    unique: bool\n        whether to expect return to be unique\n\n    Returns\n    -------\n\n    \"\"\"\n    c = get_simulationconfig()\n\n    candidates = []\n    if \"data\" not in c:\n        return candidates\n    simdct = c[\"data\"]\n    if simdct is None:\n        simdct = {}\n    for k, vals in simdct.items():\n        if vals is None:\n            continue\n        possible_candidate = True\n        if \"identifiers\" in vals:\n            idtfrs = vals[\"identifiers\"]\n            # special key not specifying identifying metadata\n            specialkeys = [\"name_contains\"]\n            allkeys = idtfrs.keys()\n            keys = list([k for k in allkeys if k not in specialkeys])\n            if \"name_contains\" in idtfrs and path is not None:\n                p = pathlib.Path(path)\n                # we only check the last three path elements\n                dirnames = [p.name, p.parents[0].name, p.parents[1].name]\n                substring = idtfrs[\"name_contains\"]\n                if not any([substring.lower() in d.lower() for d in dirnames]):\n                    possible_candidate = False\n            if len(allkeys) == 0:\n                possible_candidate = False\n            for grp in keys:\n                v = idtfrs[grp]\n                h5path = \"/\" + grp\n                if h5path not in metadata:\n                    possible_candidate = False\n                    break\n                attrs = metadata[h5path]\n                for ikey, ival in v.items():\n                    if ikey not in attrs:\n                        possible_candidate = False\n                        break\n                    av = attrs[ikey]\n                    matchtype = None\n                    if isinstance(ival, dict):\n                        matchtype = ival.get(\"match\", matchtype)  # default means equal\n                        ival = ival[\"content\"]\n\n                    if isinstance(av, bytes):\n                        av = av.decode(\"UTF-8\")\n                    if matchtype is None:\n                        if av != ival:\n                            possible_candidate = False\n                            break\n                    elif matchtype == \"substring\":\n                        if ival not in av:\n                            possible_candidate = False\n                            break\n        else:\n            possible_candidate = False\n        if possible_candidate:\n            candidates.append(k)\n    if unique and len(candidates) &gt; 1:\n        raise ValueError(\"Multiple dataset candidates (set unique=False?):\", candidates)\n    return candidates\n</code></pre>"},{"location":"api_docs/#scida.misc.deepdictkeycopy","title":"<code>deepdictkeycopy(olddict, newdict)</code>","text":"<p>Recursively walk nested dictionary, only creating empty dictionaries for entries that are dictionaries themselves.</p> <p>Parameters:</p> Name Type Description Default <code>olddict</code> <code>object</code> required <code>newdict</code> <code>object</code> required Source code in <code>src/scida/misc.py</code> <pre><code>def deepdictkeycopy(olddict: object, newdict: object) -&gt; None:\n    \"\"\"\n    Recursively walk nested dictionary, only creating empty dictionaries for entries that are dictionaries themselves.\n    Parameters\n    ----------\n    olddict\n    newdict\n\n    Returns\n    -------\n\n    \"\"\"\n    cls = olddict.__class__\n    for k, v in olddict.items():\n        if isinstance(v, MutableMapping):\n            newdict[k] = cls()\n            deepdictkeycopy(v, newdict[k])\n</code></pre>"},{"location":"api_docs/#scida.misc.map_interface_args","title":"<code>map_interface_args(paths, *args, **kwargs)</code>","text":"<p>Map arguments for interface if they are not lists.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list</code> required <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> Source code in <code>src/scida/misc.py</code> <pre><code>def map_interface_args(paths: list, *args, **kwargs):\n    \"\"\"\n    Map arguments for interface if they are not lists.\n    Parameters\n    ----------\n    paths\n    args\n    kwargs\n\n    Returns\n    -------\n\n    \"\"\"\n    n = len(paths)\n    for i, path in enumerate(paths):\n        targs = []\n        for arg in args:\n            if not (isinstance(arg, list)) or len(arg) != n:\n                targs.append(arg)\n            else:\n                targs.append(arg[i])\n        tkwargs = {}\n        for k, v in kwargs.items():\n            if not (isinstance(v, list)) or len(v) != n:\n                tkwargs[k] = v\n            else:\n                tkwargs[k] = v[i]\n        yield path, targs, tkwargs\n</code></pre>"},{"location":"api_docs/#scida.misc.path_hdf5cachefile_exists","title":"<code>path_hdf5cachefile_exists(path, **kwargs)</code>","text":"<p>Checks whether a cache file exists for given path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>path to the dataset</p> required <code>kwargs</code> <p>passed to return_hdf5cachepath</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> Source code in <code>src/scida/misc.py</code> <pre><code>def path_hdf5cachefile_exists(path, **kwargs) -&gt; bool:\n    \"\"\"\n    Checks whether a cache file exists for given path.\n    Parameters\n    ----------\n    path:\n        path to the dataset\n    kwargs:\n        passed to return_hdf5cachepath\n    Returns\n    -------\n    bool\n\n    \"\"\"\n    fp = return_hdf5cachepath(path, **kwargs)\n    if os.path.isfile(fp):\n        return True\n    return False\n</code></pre>"},{"location":"api_docs/#scida.misc.return_cachefile_path","title":"<code>return_cachefile_path(fname)</code>","text":"<p>Return the path to the cache file, return None if path cannot be generated.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>filename of cache file</p> required Source code in <code>src/scida/misc.py</code> <pre><code>def return_cachefile_path(fname: str) -&gt; Optional[str]:\n    \"\"\"\n    Return the path to the cache file, return None if path cannot be generated.\n    Parameters\n    ----------\n    fname: str\n        filename of cache file\n\n    Returns\n    -------\n\n    \"\"\"\n    config = get_config()\n    if \"cache_path\" not in config:\n        return None\n    cp = config[\"cache_path\"]\n    cp = os.path.expanduser(cp)\n    path = pathlib.Path(cp)\n    path.mkdir(parents=True, exist_ok=True)\n    fp = os.path.join(cp, fname)\n    fp = os.path.expanduser(fp)\n    bp = os.path.dirname(fp)\n    if not os.path.exists(bp):\n        os.mkdir(bp)\n    return fp\n</code></pre>"},{"location":"api_docs/#scida.misc.return_hdf5cachepath","title":"<code>return_hdf5cachepath(path, fileprefix=None)</code>","text":"<p>Returns the path to the cache file for a given path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>path to the dataset</p> required <code>fileprefix</code> <code>Optional[str]</code> <p>Can be used to specify the fileprefix used for the dataset.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> Source code in <code>src/scida/misc.py</code> <pre><code>def return_hdf5cachepath(path, fileprefix: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Returns the path to the cache file for a given path.\n    Parameters\n    ----------\n    path: str\n        path to the dataset\n    fileprefix: Optional[str]\n        Can be used to specify the fileprefix used for the dataset.\n\n    Returns\n    -------\n    str\n\n    \"\"\"\n    if fileprefix is not None:\n        path = os.path.join(path, fileprefix)\n    hsh = hash_path(path)\n    fp = return_cachefile_path(os.path.join(hsh, \"data.hdf5\"))\n    return fp\n</code></pre>"},{"location":"api_docs/#scida.misc.str_is_float","title":"<code>str_is_float(element)</code>","text":"<p>Check whether a string can be converted to a float.</p> <p>Parameters:</p> Name Type Description Default <code>element</code> <code>str</code> <p>string to check</p> required <p>Returns:</p> Type Description <code>bool</code> Source code in <code>src/scida/misc.py</code> <pre><code>def str_is_float(element: str) -&gt; bool:\n    \"\"\"\n    Check whether a string can be converted to a float.\n    Parameters\n    ----------\n    element: str\n        string to check\n\n    Returns\n    -------\n    bool\n\n    \"\"\"\n    try:\n        float(element)\n        return True\n    except ValueError:\n        return False\n</code></pre>"},{"location":"api_docs/#scida.series","title":"<code>series</code>","text":""},{"location":"api_docs/#scida.series.DatasetSeries","title":"<code>DatasetSeries</code>","text":"<p>             Bases: <code>object</code></p> <p>A container for collection of interface instances</p> Source code in <code>src/scida/series.py</code> <pre><code>class DatasetSeries(object):\n    \"\"\"A container for collection of interface instances\"\"\"\n\n    def __init__(\n        self,\n        paths: Union[List[str], List[Path]],\n        *interface_args,\n        datasetclass=None,\n        overwrite_cache=False,\n        lazy=True,  # lazy will only initialize data sets on demand.\n        async_caching=False,\n        names=None,\n        **interface_kwargs\n    ):\n        self.paths = paths\n        self.names = names\n        self.hash = hash_path(\"\".join([str(p) for p in paths]))\n        self._metadata = None\n        self._metadatafile = return_cachefile_path(os.path.join(self.hash, \"data.json\"))\n        self.lazy = lazy\n        for p in paths:\n            if not (isinstance(p, Path)):\n                p = Path(p)\n            if not (p.exists()):\n                raise ValueError(\"Specified path '%s' does not exist.\" % p)\n        dec = delay_init  # lazy loading\n\n        # Catch Mixins and create type:\n        mixins = interface_kwargs.pop(\"mixins\", [])\n        datasetclass = create_MixinDataset(datasetclass, mixins)\n        self._dataset_cls = datasetclass\n\n        gen = map_interface_args(paths, *interface_args, **interface_kwargs)\n        self.datasets = [dec(datasetclass)(p, *a, **kw) for p, a, kw in gen]\n\n        if self.metadata is None:\n            print(\"Have not cached this data series. Can take a while.\")\n            dct = {}\n            for i, (path, d) in enumerate(\n                tqdm(zip(self.paths, self.datasets), total=len(self.paths))\n            ):\n                rawmeta = load_metadata(path, choose_prefix=True)\n                # class method does not initiate obj.\n                dct[i] = d._clean_metadata_from_raw(rawmeta)\n            self.metadata = dct\n        elif async_caching:\n            # hacky and should not be here this explicitly, just a proof of concept\n            # for p in paths:\n            #     loader = scida.io.determine_loader(p)\n            pass\n\n    def __init_subclass__(cls, *args, **kwargs):\n        super().__init_subclass__(*args, **kwargs)\n        dataseries_type_registry[cls.__name__] = cls\n\n    def __len__(self):\n        return len(self.datasets)\n\n    def __getitem__(self, key):\n        return self.datasets[key]\n\n    def info(self):\n        rep = \"\"\n        rep += \"class: \" + sprint(self.__class__.__name__)\n        props = self._repr_dict()\n        for k, v in props.items():\n            rep += sprint(\"%s: %s\" % (k, v))\n        if self.metadata is not None:\n            rep += sprint(\"=== metadata ===\")\n            # we print the range of each metadata attribute\n            minmax_dct = {}\n            for mdct in self.metadata.values():\n                for k, v in mdct.items():\n                    if k not in minmax_dct:\n                        minmax_dct[k] = [v, v]\n                    else:\n                        if not np.isscalar(v):\n                            continue  # cannot compare arrays\n                        minmax_dct[k][0] = min(minmax_dct[k][0], v)\n                        minmax_dct[k][1] = max(minmax_dct[k][1], v)\n            for k in minmax_dct:\n                reprval1, reprval2 = minmax_dct[k][0], minmax_dct[k][1]\n                if isinstance(reprval1, float):\n                    reprval1 = \"%.2f\" % reprval1\n                    reprval2 = \"%.2f\" % reprval2\n                m1 = minmax_dct[k][0]\n                m2 = minmax_dct[k][1]\n                if (not np.isscalar(m1)) or (np.isscalar(m1) and m1 == m2):\n                    rep += sprint(\"%s: %s\" % (k, minmax_dct[k][0]))\n                else:\n                    rep += sprint(\n                        \"%s: %s -- %s\" % (k, minmax_dct[k][0], minmax_dct[k][1])\n                    )\n            rep += sprint(\"============\")\n        print(rep)\n\n    @property\n    def data(self):\n        raise AttributeError(\n            \"Series do not have 'data' attribute. Load a dataset from series.get_dataset().\"\n        )\n\n    def _repr_dict(self) -&gt; Dict[str, str]:\n        \"\"\"\n        Return a dictionary of properties to be printed by __repr__ method.\n        Returns\n        -------\n        dict\n        \"\"\"\n        props = dict()\n        sources = [str(p) for p in self.paths]\n        props[\"source(id=0)\"] = sources[0]\n        props[\"Ndatasets\"] = len(self.datasets)\n        return props\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Return a string representation of the object.\n        Returns\n        -------\n        str\n        \"\"\"\n        props = self._repr_dict()\n        clsname = self.__class__.__name__\n        result = clsname + \"[\"\n        for k, v in props.items():\n            result += \"%s=%s, \" % (k, v)\n        result = result[:-2] + \"]\"\n        return result\n\n    @classmethod\n    def validate_path(cls, path, *args, **kwargs) -&gt; CandidateStatus:\n        # estimate whether we have a valid path for this dataseries\n        return CandidateStatus.NO\n\n    @classmethod\n    def from_directory(\n        cls, path, *interface_args, datasetclass=None, pattern=None, **interface_kwargs\n    ):\n        p = Path(path)\n        if not (p.exists()):\n            raise ValueError(\"Specified path does not exist.\")\n        if pattern is None:\n            pattern = \"*\"\n        paths = [f for f in p.glob(pattern)]\n        return cls(\n            paths, *interface_args, datasetclass=datasetclass, **interface_kwargs\n        )\n\n    def get_dataset(\n        self,\n        index: Optional[int] = None,\n        name: Optional[str] = None,\n        reltol=1e-2,\n        **kwargs\n    ):\n        \"\"\"Get dataset by some metadata property. In the base class, we go by list index.\"\"\"\n        if index is None and name is None and len(kwargs) == 0:\n            raise ValueError(\"Specify index/name or some parameter to select for.\")\n        # aliases for index:\n        aliases = [\"snap\", \"snapshot\"]\n        aliases_given = [k for k in aliases if k in kwargs]\n        if index is not None:\n            aliases_given += [index]\n        if len(aliases_given) &gt; 1:\n            raise ValueError(\"Multiple aliases for index specified.\")\n        for a in aliases_given:\n            if kwargs.get(a) is not None:\n                index = kwargs.pop(a)\n\n        if index is not None:\n            return self.datasets[index]\n        if name is not None:\n            if self.names is None:\n                raise ValueError(\"No names specified for members of this series.\")\n            if name not in self.names:\n                raise ValueError(\"Name %s not found in this series.\" % name)\n            return self.datasets[self.names.index(name)]\n        if len(kwargs) &gt; 0 and self.metadata is None:\n            if self.lazy:\n                raise ValueError(\n                    \"Cannot select by given keys before dataset evaluation.\"\n                )\n            raise ValueError(\"Unknown error.\")  # should not happen?\n        candidates = []\n        candidates_props = {}\n        props_compare = set()  # save names of fields we want to compare\n        for k, v in kwargs.items():\n            candidates_props[k] = []\n        for i, (j, dm) in enumerate(self.metadata.items()):\n            assert int(i) == int(j)\n            is_candidate = True\n            for k, v in kwargs.items():\n                if k not in dm:\n                    is_candidate = False\n                    continue\n                if isinstance(v, int) or isinstance(v, float):\n                    candidates_props[k].append(dm[k])\n                    props_compare.add(k)\n                elif v != dm[k]:\n                    is_candidate = False\n            if is_candidate:\n                candidates.append(i)\n            else:  # unroll changes\n                for lst in candidates_props.values():\n                    if len(lst) &gt; len(candidates):\n                        lst.pop()\n        # find candidate closest to request\n        idxlist = []\n        for k in props_compare:\n            idx = np.argmin(np.abs(np.array(candidates_props[k]) - kwargs[k]))\n            idxlist.append(idx)\n        if len(set(idxlist)) &gt; 1:\n            raise ValueError(\"Ambiguous selection request\")\n        elif len(idxlist) == 0:\n            raise ValueError(\"No candidate found.\")\n        index = candidates[idxlist[0]]\n        # TODO: reintroduce tolerance check\n        return self.get_dataset(index=index)\n\n    @property\n    def metadata(self):\n        if self._metadata is not None:\n            return self._metadata\n        fp = self._metadatafile\n        if os.path.exists(fp):\n            md = json.load(open(fp, \"r\"))\n            ikeys = sorted([int(k) for k in md.keys()])\n            mdnew = {}\n            for ik in ikeys:\n                mdnew[ik] = md[str(ik)]\n            self._metadata = mdnew\n            return self._metadata\n        return None\n\n    @metadata.setter\n    def metadata(self, dct):\n        class ComplexEncoder(json.JSONEncoder):\n            def default(self, obj):\n                if isinstance(obj, np.int64):\n                    return int(obj)\n                if isinstance(obj, np.int32):\n                    return int(obj)\n                if isinstance(obj, np.uint32):\n                    return int(obj)\n                if isinstance(obj, bytes):\n                    return obj.decode(\"utf-8\")\n                if isinstance(obj, np.ndarray):\n                    assert len(obj) &lt; 1000  # dont want large obs here...\n                    return list(obj)\n                try:\n                    return json.JSONEncoder.default(self, obj)\n                except TypeError as e:\n                    print(\"obj failing json encoding:\", obj)\n                    raise e\n\n        # def serialize_numpy(obj):\n        #    if isinstance(obj, np.int64): return int(obj)\n        #    if isinstance(obj, np.int32): return int(obj)\n        #    return json.JSONEncoder.default(self, obj)\n        self._metadata = dct\n        fp = self._metadatafile\n        # print(dct)\n        if not os.path.exists(fp):\n            json.dump(dct, open(fp, \"w\"), cls=ComplexEncoder)\n</code></pre>"},{"location":"api_docs/#scida.series.DatasetSeries.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the object.</p> <p>Returns:</p> Type Description <code>str</code> Source code in <code>src/scida/series.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Return a string representation of the object.\n    Returns\n    -------\n    str\n    \"\"\"\n    props = self._repr_dict()\n    clsname = self.__class__.__name__\n    result = clsname + \"[\"\n    for k, v in props.items():\n        result += \"%s=%s, \" % (k, v)\n    result = result[:-2] + \"]\"\n    return result\n</code></pre>"},{"location":"api_docs/#scida.series.DatasetSeries.get_dataset","title":"<code>get_dataset(index=None, name=None, reltol=0.01, **kwargs)</code>","text":"<p>Get dataset by some metadata property. In the base class, we go by list index.</p> Source code in <code>src/scida/series.py</code> <pre><code>def get_dataset(\n    self,\n    index: Optional[int] = None,\n    name: Optional[str] = None,\n    reltol=1e-2,\n    **kwargs\n):\n    \"\"\"Get dataset by some metadata property. In the base class, we go by list index.\"\"\"\n    if index is None and name is None and len(kwargs) == 0:\n        raise ValueError(\"Specify index/name or some parameter to select for.\")\n    # aliases for index:\n    aliases = [\"snap\", \"snapshot\"]\n    aliases_given = [k for k in aliases if k in kwargs]\n    if index is not None:\n        aliases_given += [index]\n    if len(aliases_given) &gt; 1:\n        raise ValueError(\"Multiple aliases for index specified.\")\n    for a in aliases_given:\n        if kwargs.get(a) is not None:\n            index = kwargs.pop(a)\n\n    if index is not None:\n        return self.datasets[index]\n    if name is not None:\n        if self.names is None:\n            raise ValueError(\"No names specified for members of this series.\")\n        if name not in self.names:\n            raise ValueError(\"Name %s not found in this series.\" % name)\n        return self.datasets[self.names.index(name)]\n    if len(kwargs) &gt; 0 and self.metadata is None:\n        if self.lazy:\n            raise ValueError(\n                \"Cannot select by given keys before dataset evaluation.\"\n            )\n        raise ValueError(\"Unknown error.\")  # should not happen?\n    candidates = []\n    candidates_props = {}\n    props_compare = set()  # save names of fields we want to compare\n    for k, v in kwargs.items():\n        candidates_props[k] = []\n    for i, (j, dm) in enumerate(self.metadata.items()):\n        assert int(i) == int(j)\n        is_candidate = True\n        for k, v in kwargs.items():\n            if k not in dm:\n                is_candidate = False\n                continue\n            if isinstance(v, int) or isinstance(v, float):\n                candidates_props[k].append(dm[k])\n                props_compare.add(k)\n            elif v != dm[k]:\n                is_candidate = False\n        if is_candidate:\n            candidates.append(i)\n        else:  # unroll changes\n            for lst in candidates_props.values():\n                if len(lst) &gt; len(candidates):\n                    lst.pop()\n    # find candidate closest to request\n    idxlist = []\n    for k in props_compare:\n        idx = np.argmin(np.abs(np.array(candidates_props[k]) - kwargs[k]))\n        idxlist.append(idx)\n    if len(set(idxlist)) &gt; 1:\n        raise ValueError(\"Ambiguous selection request\")\n    elif len(idxlist) == 0:\n        raise ValueError(\"No candidate found.\")\n    index = candidates[idxlist[0]]\n    # TODO: reintroduce tolerance check\n    return self.get_dataset(index=index)\n</code></pre>"},{"location":"api_docs/#scida.series.DirectoryCatalog","title":"<code>DirectoryCatalog</code>","text":"<p>             Bases: <code>object</code></p> <p>A catalog consisting of interface instances contained in a directory.</p> Source code in <code>src/scida/series.py</code> <pre><code>class DirectoryCatalog(object):\n    \"\"\"A catalog consisting of interface instances contained in a directory.\"\"\"\n\n    def __init__(self, path):\n        self.path = path\n</code></pre>"},{"location":"api_docs/#scida.series.HomogeneousSeries","title":"<code>HomogeneousSeries</code>","text":"<p>             Bases: <code>DatasetSeries</code></p> <p>Series consisting of same-type data sets.</p> Source code in <code>src/scida/series.py</code> <pre><code>class HomogeneousSeries(DatasetSeries):\n    \"\"\"Series consisting of same-type data sets.\"\"\"\n\n    def __init__(self, path, **interface_kwargs):\n        # TODO\n        super().__init__()\n</code></pre>"},{"location":"api_docs/#scida.utilities","title":"<code>utilities</code>","text":""},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#configuration","title":"Configuration","text":""},{"location":"configuration/#main-configuration-file","title":"Main configuration file","text":"<p>The main configuration file is located at <code>~/.scida/config.yaml</code>. If this file does not exist, it is created with the first use of scida. The file is using the YAML format. The following options are available:</p> <code>copied_default</code> <p>If this option is set True, a warning is printed because the copied default config has not been adjusted by the user    yet. Once you have done so, remove this line.</p> <code>cache_path</code> <p>Sets the folder to use as a cache for scida. Recommended to be moved out of the home directory to a fast disk.</p> <code>datafolders</code> <p>A list of folders to scan for data specifiers when using <code>scida.load(\"specifier\")</code>.</p> <code>nthreads</code> <p>scida itself might use multiple threads for some operations. This option sets the number of threads to use.   This is independent of any dask threading. Default: 8</p> <code>missing_units</code> <p>How to handle missing units. Can be \"warn\", \"raise\", or \"ignore\". \"warn\" will print a warning, \"raise\" will raise an   exception, and \"ignore\" will silently continue without the right units. Default: \"warn\"</p>"},{"location":"configuration/#simulation-configuration","title":"Simulation configuration","text":"<p>By default, scida will load supported simulation configurations from the package. User configurations for simulations are loaded from <code>~/.config/scida/simulations.yaml</code>. This file is also in YAML format.</p> <p>The configuration has to have the following structure: </p><pre><code>data:\n  SIMNAME1:\n\n  SIMNAME2:\n</code></pre> <p>Each simulation could look something like this:</p> <pre><code>data:\n  SIMNAME1:\n    aliases:\n      - SIMNAME\n      - SMN1\n    identifiers:\n      Parameters:\n        SimName: SIMNAME1\n      Config:\n        SavePath:\n          content: /path/to/simname\n          match: substr\n    unitfile: units/simnameunits.yaml\n    dataset_type:\n      series: ArepoSimulation\n      dataset: ArepoSnapshot\n</code></pre> <code>aliases</code> <p>A list of aliases for the simulation. These can be used to load the simulation with <code>scida.load(\"alias\")</code>.</p> <code>identifiers</code> <p>A dictionary of identifiers from the metadata of a given dataset to identify it as such.   In above example \"/Parameters\" is the path to an attribute \"SimName\" in the HDF5/zarr metadata   with the exact content as given. Multiple identifiers can be given, in which case all have to match.   Partial matches of a given key-value key are possible by passing a dictionary {\"content\": \"valuesubstr\", match: substring}   rather than a string.</p> <code>unitfile</code> <p>The path to the unitfile relative to the user/repository simulation configuration. user configurations   take precedence over the package configuration.</p> <code>dataset_type</code> <p>Can explicitly fix the dataset/series type for a simulation.</p>"},{"location":"configuration/#unit-files","title":"Unit files","text":"<p>Unit files are used to determine the units of datasets, particularly for datasets that do not have metadata that can be used to infer units. Unit files are specified either explicitly via the <code>unitfile</code> option in <code>scida.load</code> or implicitly via the simulation configuration, see above. Relative paths, such as <code>units/simnameunits.yaml</code> are relative to the user/package simulation config folder. The former (<code>~/.config/scida/</code>) takes precedence.</p> <p>A unit file could look like this:</p> <pre><code>metadata_unitsystem: cgs\nunits:\n  unit_length: 100.0 * km\n  unit_mass: g\nfields:\n  _all:\n    CounterID: none\n    Coordinates: unit_length\n  InternalArrays: none\n  PartType0:\n    SubPartType0:\n      FurthestSubgroupDistance: unit_length\n    NearestNeighborDistance: unit_length\n    Energy: 10.0 * erg\n</code></pre> <code>metadata_unitsystem</code> <p>The unitsystem assumed when deducing units from metadata dimensions where available.   Only cgs supported right now.</p> <code>units</code> <p>unit definitions that are used in the following <code>fields</code> section. The units are defined as   pint expressions.</p> <code>fields</code> <p>A dictionary of fields and their units. The fields are specified as a path to the field in the dataset.   The special field <code>_all</code> can be used to set the default unit for all fields with a given name irrespective   of the path of the field. Other than that, entries represent the fields or containers of fields. The special   field <code>none</code> can be used to set the unit to None, i.e. no unit. This is differently handled than \" \"/\"dimensionless\" as   the field will be treated as array rather than dimensionless pint array.</p>"},{"location":"dataset_structure/","title":"Dataset structure","text":""},{"location":"dataset_structure/#dataset-structure","title":"Dataset structure","text":"<p>Here, we discuss the requirements for easy extension/support of new datasets.</p>"},{"location":"dataset_structure/#supported-file-formats","title":"Supported file formats","text":"<p>Currently, input files need to have one of the following formats:</p> <ul> <li>hdf5</li> <li>multi-file hdf5</li> <li>zarr</li> </ul>"},{"location":"dataset_structure/#supported-file-structures","title":"Supported file structures","text":"<p>Just like this package, above file formats use a hierarchical structure to store data with three fundamental objects:</p> <ul> <li>Groups are containers for other groups or datasets.</li> <li>Datasets are multidimensional arrays of a homogeneous type, usually bundled into some Group.</li> <li>Attributes provide various metadata.</li> </ul>"},{"location":"dataset_structure/#supported-data-structures","title":"Supported data structures","text":"<p>At this point, we only support unstructured datasets, i.e. datasets that do not depend on the memory layout for their interpretation. For example, this implies that simulation codes utilizing uniform or adaptive grids are not supported.</p>"},{"location":"dataset_structure/#examples-of-supported-simulation-codes","title":"Examples of supported simulation codes","text":"<p>We explicitly support simulations run with the following codes:</p> <ul> <li>Gadget</li> <li>Gizmo</li> <li>Arepo</li> <li>Swift</li> </ul>"},{"location":"derived_fields/","title":"Derived fields","text":""},{"location":"derived_fields/#derived-fields","title":"Derived fields","text":"<p>Commonly during analysis, newly derived quantities/fields are to be synthesized from one or more snapshot fields into a new field. For example, while the temperature, pressure, or entropy of gas is not stored directly in the snapshots, they can be computed from fields which are present on disk.</p> <p>There are two ways to create new derived fields. For quick analysis, we can simply leverage dask arrays themselves.</p>"},{"location":"derived_fields/#defining-new-quantities-with-dask-arrays","title":"Defining new quantities with dask arrays","text":"<pre><code>from scida import load\nds = load(\"TNG50-4_snapshot\") # (1)!\ngas = ds.data['gas']\nkineticenergy = 0.5*gas['Masses']*(gas['Velocities']**2).sum(axis=1)\n</code></pre> <ol> <li>In this example, we assume a dataset, such as the 'TNG50_snapshot' test data set, that has its fields (Masses, Velocities) nested by particle type (gas)</li> </ol> <p>In the example above, we define a new dask array called kineticenergy. Note that just like all other dask arrays and dataset fields, these fields are \"virtual\", i.e. only the graph of their construction is held in memory, which can be instantiated by applying the .compute() method.</p> <p>We can also add this field from above example to the existing ones in the dataset.</p> <pre><code>gas['kineticenergy'] = kineticenergy\n</code></pre>"},{"location":"derived_fields/#defining-new-quantities-with-field-recipes","title":"Defining new quantities with field recipes","text":"<p>Working with complex datasets over a longer period, it is often useful to have a large range of fields available. The above approach with dask arrays suffers from some shortcomings. For example, in some cases the memory footprint and instantiation time for each field can add up to substantial loading times. Also, when defining fields with dask arrays, these fields need to be defined in order of their respective dependencies.</p> <p>For this purpose, field recipes are available. An example of such recipe is given below.</p> <pre><code>import numpy as np\n\nfrom scida import load\nds = load(\"TNG50-4_snapshot\")\n\n@ds.register_field(\"stars\")  # (1)!\ndef VelMag(arrs, **kwargs):\n    import dask.array as da\n    vel = arrs['Velocities']\n    return da.sqrt(vel[:,0]**2 + vel[:,1]**2 + vel[:,2]**2)\n</code></pre> <ol> <li>Here, stars is the name of the field container the field should be added to. The field will now be available as ds['stars']['VelMag']</li> </ol> <p>The field recipe is translated into a regular field, i.e. dask array, the first time it is queried for. Above example can be queried as:</p> <pre><code>ds['stars']['VelMag']\n</code></pre> <p>Practically working with these fields, there is no difference between derived and on-disk fields.</p>"},{"location":"derived_fields/#adding-multiple-fields","title":"Adding multiple fields","text":"<p>It can be useful to write (a) dedicated field definition file(s). First, initialize a FieldContainer</p> <pre><code>from scida.fields import FieldContainer\ngroupnames = [\"PartType0\", \"Subhalo\"]  # (1)!\nfielddefs = FieldContainer(containers=groupnames)\n\n@fielddefs.register_field(\"PartType0\") # (2)!\ndef Volume(arrs, **kwargs):\n    return arrs[\"Masses\"]/arrs[\"Density\"]\n\n@fielddefs.register_field(\"all\") # (3)!\ndef GroupDistance3D(arrs, snap=None):\n    \"\"\"Returns distance to hosting group center. Returns rubbish if not actually associated with a group.\"\"\"\n    import dask.array as da\n    boxsize = snap.header[\"BoxSize\"]\n    pos_part = arrs[\"Coordinates\"]\n    groupid = arrs[\"GroupID\"]\n    if hasattr(groupid, \"magnitude\"):\n        groupid = groupid.magnitude\n        boxsize *= snap.ureg(\"code_length\")\n    pos_cat = snap.data[\"Group\"][\"GroupPos\"][groupid]\n    dist3 = pos_part-pos_cat\n    dist3 = da.where(dist3&gt;boxsize/2.0, boxsize-dist3, dist3)\n    dist3 = da.where(dist3&lt;=-boxsize/2.0, boxsize+dist3, dist3) # PBC\n    return dist3\n\n@fielddefs.register_field(\"all\")\ndef GroupDistance(arrs, snap=None):\n    import dask.array as da\n    dist3 = arrs[\"GroupDistance3D\"]\n    dist = da.sqrt((dist3**2).sum(axis=1))\n    dist = da.where(arrs[\"GroupID\"]==-1, np.nan, dist) # set unbound gas to nan\n    return dist\n</code></pre> <ol> <li>We define a list of field containers that we want to add particles to.</li> <li>Specify the container we want to have the field added to.</li> <li>Using the \"all\" identifier, we can also attempt to add this field to all containers we have specified.</li> </ol> <p>Finally, we just need to import the fielddefs object (if we have defined it in another file) and merge them with a dataset that we loaded:</p> <pre><code>ds = load(\"TNG50-4_snapshot\")\nds.data.merge(fielddefs)\n</code></pre> <p>In above example, we now have the following fields available:</p> <pre><code>gas = ds.data[\"PartType0\"]\nprint(gas[\"Volume\"])\nprint(gas[\"GroupDistance\"])\nprint(gas[\"Subhalo\"])\n</code></pre>"},{"location":"developer/","title":"Developer Guide","text":""},{"location":"developer/#developer-guide","title":"Developer Guide","text":"<p>We welcome contributions to scida, such as bug reports, feature requests, and design proposals. This page contains information on how to contribute to scida.</p>"},{"location":"developer/#development-environment","title":"Development environment","text":""},{"location":"developer/#clone-the-repository","title":"Clone the repository","text":"<p>Make a fork of the repository, then clone the repository to your local machine:</p> <pre><code>git clone https://github.com/YOURUSERNAME/scida\ncd scida\n</code></pre>"},{"location":"developer/#install","title":"Install","text":"<p>We use poetry to manage dependencies and the development environment. After installing poetry, you can install scida and its dependencies with</p> <pre><code>poetry install\n</code></pre> <p>This will create a virtual environment and install scida and its dependencies, including development dependencies. All commands, such as <code>python</code> and <code>pytest</code> will be run in this environment by prepending <code>poetry run ...</code> to the command.</p> <p>While using poetry is recommended, you can also install scida with pip in a virtual environment of your choice:</p> <pre><code>python -m venv scida_venv\nsource scida_venv/bin/activate\npip install -e .\n</code></pre> <p>Note that in latter case, you will have to manage the dependencies yourself, including development dependencies. If choosing this path, remove any <code>poetry run</code> prefixes from the commands below accordingly.</p>"},{"location":"developer/#run-tests","title":"Run tests","text":"<p>To run the tests, use</p> <pre><code>poetry run pytest\n</code></pre> <p>Many tests require test data sets. These might not be available to you and lead to many tests being skipped.</p>"},{"location":"developer/#contributing-code","title":"Contributing code","text":""},{"location":"developer/#code-formatting","title":"Code Formatting","text":"<p>We use the black code formatter to ensure a consistent code style. This style is ensured by the pre-commit hook config. Make sure to have pre-commit installed and run</p> <pre><code>pre-commit install\n</code></pre> <p>in the repository to install the hook.</p>"},{"location":"developer/#docstring-style","title":"Docstring style","text":"<p>We use numpydoc to format docstrings. Please annotate all functions and classes with docstrings accordingly.</p>"},{"location":"developer/#testing","title":"Testing","text":"<p>We use pytest for testing. Add new tests for added functionality in a test file in the <code>tests</code> directory. Make sure to run the tests before submitting a pull request.</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"faq/#extending-existing-datasets","title":"Extending existing datasets","text":"<p>How do I add custom fields (that are not derived fields) to my existing dataset?</p> <p>Please also see how to add derived fields.</p> <p>After loading some dataset, you can add custom fields to a given container, here called PartType0 by simply assigning a dask array under the desired key to it.</p> <p>Please note that all fields within a container are expected to have the same shape in their first axis.</p> <pre><code>from scida import load\nimport dask.array as da\nds = load('TNG50-4_snapshot')\narray = da.zeros_like(ds.data[\"PartType0\"][\"Density\"])\nds.data['PartType0'][\"zerofield\"] = array\n</code></pre> <p>As we operate with dask, make sure to cast your array accordingly. For example, if your field is just a numpy array or a hdf5 memmap, you can use <code>da.from_array</code> to cast it to a dask array. Alternatively, if you have another dataset loaded, you can assign fields from one to another:</p> <pre><code>ds2 = load('TNG50-4_snapshot')\nds.data['PartType0'][\"NewDensity\"] = ds2.data['PartType0'][\"Density\"]\n</code></pre>"},{"location":"faq/#misc","title":"Misc","text":"<p>How does load() determine the right type of dataset/series to load?</p> <p>load() will step through all subclasses of Series() and Dataset() and call their validate_path() class method. A list of candidate classes that return True upon this call is assembled. If more than one candidate exists, the most specific candidate, i.e. the one furthest down the inheritance tree, is chosen.</p> <p>The candidate can be overwritten when a YAML configuration specifies \"dataset_type/series\" and/or \"dataset_type/dataset\" keys to the respective class name.</p> <p>In addition to this, different features, such as for datasets using Cartesian coordinates, are added as so-called Mixins to the dataset class.</p>"},{"location":"halocatalogs/","title":"Halo Catalogs","text":""},{"location":"halocatalogs/#halo-and-galaxy-catalogs","title":"Halo and galaxy catalogs","text":"<p>Cosmological simulations are often post-processed with a substructure identification algorithm in order to identify halos and galaxies. The resulting catalogs can be loaded and connect with the particle-level snapshot data.</p>"},{"location":"halocatalogs/#adding-and-using-halogalaxy-catalog-information","title":"Adding and using halo/galaxy catalog information","text":"<p>Currently, we support the usual FOF/Subfind combination and format. Their presence will be automatically detected and the catalogs will be loaded into ds.data as shown below.</p> <pre><code>from scida import load\nds = load(\"TNG50-4_snapshot\") # (1)!\n</code></pre> <ol> <li>In this example, we assume a dataset, such as the 'TNG50_snapshot' test data set, that has its fields (Masses, Velocities) nested by particle type (gas)</li> </ol> <p>The dataset itself passed to load does not possess information about the FoF/Subfind outputs as they are commonly saved in a separate folder or hdf5 file. For typical folder structures of GADGET/AREPO style simulations, an attempt is made to automatically discover and add such information. The path to the catalog can otherwise explicitly be passed to load() via the catalog=... keyword.</p>"},{"location":"halocatalogs/#accessing-halogalaxy-catalog-information","title":"Accessing halo/galaxy catalog information","text":"<p>Groups and subhalo information is added into the dataset with the data containers Group and Subhalo. For example, we can obtain the masses of each group as:</p> <pre><code>group_mass = ds.data[\"Group\"][\"GroupMass\"]\n</code></pre>"},{"location":"halocatalogs/#accessing-particle-level-halogalaxy-information","title":"Accessing particle-level halo/galaxy information","text":"<p>In addition to these two data containers, new information is added to all other containers about their belonging to a given group and subhalo.</p> <pre><code>groupid = ds.data[\"PartType0\"][\"GroupID\"] #(1)!\nsubhaloid = ds.data[\"PartType0\"][\"SubhaloID\"]\nlocalsubhaloid = ds.data[\"PartType0\"][\"LocalSubhaloID\"]\n</code></pre> <ol> <li>This information is also available for the other particle types.</li> </ol> <p>In above example, we fetch the virtual dask arrays holding information about the halo and subhalo association for each particle.</p> <code>GroupID</code> <p>The group ID of the group the particle belongs to. This is the index into the group catalog.</p> <code>SubhaloID</code> <p>The subhalo ID of the subhalo the particle belongs to. This is the index into the subhalo catalog.</p> <code>LocalSubhaloID</code> <p>This is the Subhalo ID relative to the central subhalo of a given group. For the central subhalo, this is 0.    Satellites accordingly start at index 1.</p> <p>Particles that are not associated with a group or subhalo that are queried for such ID will return <code>ds.misc['unboundID']'</code>. This is currently set to 9223372036854775807, but might change to -1 in the future.</p> <p>This operation allows us to efficiently query the belonging of given particles. So, for example, we can compute the group IDs of the gas particles 1000-1099 by running</p> <pre><code>groupid[1000:1100].compute()\n</code></pre>"},{"location":"halocatalogs/#working-with-halo-data","title":"Working with halo data","text":""},{"location":"halocatalogs/#query-all-particles-belonging-to-some-group","title":"Query all particles belonging to some group","text":"<p>Often we only want to operate with the particles of a given halo. We can efficiently return a virtual view of all fields in ds.data for a given halo ID as for example in:</p> <pre><code>data = ds.return_data(haloID=42)\n</code></pre> <p>data will have the same structure as ds.data but restricted to particles of a given group.</p>"},{"location":"halocatalogs/#applying-to-all-groups-in-parallel","title":"Applying to all groups in parallel","text":"<p>In many cases, we do not want the particle data of an individual group, but we want to calculate some reduced statistic from the bound particles of each group. For this, we provide the grouped functionality. In the following we give a range of examples of its use.</p> Warning <p>Executing the following commands can be demanding on compute resources and memory. Usually, one wants to restrict the groups to run on. You can either specify \"nmax\" to limit the maximum halo id to evaluate up to. This is usually desired in any case as halos are ordered (in descending order) by their mass. For more fine-grained control, you can also pass a list of halo IDs to evaluate via the \"idxlist\" keyword. These keywords should be passed to the \"evaluate\" call.</p> Note <p>By default, operations are done on for halos. By passing <code>objtype=\"subhalo\"</code> to the <code>grouped</code> call, the operation is done on subhalos instead.</p>"},{"location":"halocatalogs/#baryon-mass","title":"Baryon mass","text":"<p>Let's say we want to calculate the baryon mass for each halo from the particles.</p> <pre><code>mass = ds.grouped(\"Masses\", parttype=\"PartType0\").sum().evaluate(compute=True)\nmass\n</code></pre> <p>Unless compute=True a dask operation is returned.</p>"},{"location":"halocatalogs/#electron-mass","title":"Electron mass","text":"<p>Instead of an existing field, we can also pass another dask array of matching field for the given particle species (here: PartType0). The following example calculates the total mass of electrons in each halo.</p> <pre><code>import dask.array as da\ngas = ds.data[\"PartType0\"]\n# total electron mass\nme = 9.1e-28 # cgs\nmp = 1.7e-24 # cgs\n# me and mp units cancel each other\nne = gas[\"ElectronAbundance\"] * 0.76 * gas[\"Density\"]/mp\nvol = gas[\"Masses\"] / gas[\"Density\"]\nemass_field = vol * me * ne\nemass = ds.grouped(emass_field).sum().evaluate(compute=True)\nemass\n</code></pre>"},{"location":"halocatalogs/#heaviest-black-hole","title":"Heaviest black hole","text":"<pre><code>bhmassmax = ds.grouped(\"Masses\", parttype=\"PartType5\").max().evaluate()\nbhmassmax\n</code></pre>"},{"location":"halocatalogs/#radial-profile-for-each-halo","title":"Radial profile for each halo","text":"<pre><code>import numpy as np\nfrom scipy.stats import binned_statistic\n\ngrp = ds.data[\"Group\"]\npos3 = gas[\"Coordinates\"] - grp[\"GroupPos\"][gas[\"GroupID\"]]\ndist = da.sqrt(da.sum((pos3)**2, axis=1)) # (1)!\n\ndef customfunc(dist, density, volume):\n    a = binned_statistic(dist, density, statistic=\"sum\", bins=np.linspace(0, 200, 10))[0]\n    b = binned_statistic(dist, volume, statistic=\"sum\", bins=np.linspace(0, 200, 10))[0]\n    return a/b\n\ng = ds.grouped(dict(dist=dist, Density=gas[\"Density\"],\n                    Volume=vol))\ns = g.apply(customfunc).evaluate()\n</code></pre> <ol> <li>We do not incorporate periodic boundary conditions in this example for brevity.</li> </ol> <p>Note that here we defined a custom function customfunc that will be applied to each halo respectively. The custom function accepts any inputs we feed to ds.grouped(). The customfunc receives numpy representation (rather than dask arrays) as inputs.</p>"},{"location":"impressions/","title":"Visual impressions using scida","text":""},{"location":"impressions/#visual-impressions-using-scida","title":"Visual impressions using scida","text":""},{"location":"install/","title":"Installation","text":""},{"location":"install/#getting-started","title":"Getting started","text":""},{"location":"install/#installation","title":"Installation","text":"<p>scida can be installed via PyPI. scida requires a Python version 3.9, 3.10 or 3.11.</p> Encapsulating packages <p>We recommend encapsulating your python environments. For example using anaconda or virtualenv.</p> <p>If you use anaconda, we recommend running</p> <pre><code>conda create -n scida python=3.9\n</code></pre> <p>Activate the environment as needed (as for the following installation) as</p> <pre><code>conda activate scida\n</code></pre> <p>If you are using jupyter/ipython, install and register the scida kernel via</p> <pre><code>conda install ipykernel\npython -m ipykernel install --user --name scida --display-name \"scida\"\n</code></pre> <p>Now you can install scida as described below and use it in jupyter notebooks with the given kernel.</p> <pre><code>pip install scida\n</code></pre>"},{"location":"install/#next-steps","title":"Next steps","text":"<p>Next, get started with the tutorial for either simulations or observations:</p> <p>           Simulations         </p> <p>Tutorial on a simulation dataset.</p> <p>           Observations         </p> <p>Tutorial on an observational dataset.</p>"},{"location":"largedatasets/","title":"Large datasets","text":""},{"location":"largedatasets/#handling-large-data-sets","title":"Handling Large Data Sets","text":"<p>Until now, we have applied our framework to a very small simulation. However, what if we are working with a very large data set (like the TNG50-1 cosmological simulation, which has \\(2160^3\\) particles, \\(512\\) times more than TNG50-4)?</p>"},{"location":"largedatasets/#starting-simple-computing-in-chunks","title":"Starting simple: computing in chunks","text":"<p>First, we can still run the same calculation as above, and it will \"just work\" (hopefully).</p> <p>This is because Dask has many versions of common algorithms and functions which work on \"blocks\" or \"chunks\" of the data, which split up the large array into smaller arrays. Work is needed on each chunk, after which the final answer is assembled.</p> <p>Importantly, in our case above, even if the <code>mass</code> array above does not fit into memory, the <code>mass.sum().compute()</code> will chunk the operation up in a way that the task can be calculated.</p> <pre><code>&gt;&gt;&gt; from scida import load\n&gt;&gt;&gt; ds = load(\"TNG50_snapshot\")\n</code></pre> <p>Before we start, let's enable a progress indicator from dask (note that this will only work for local schedulers, see next section):</p> <pre><code>&gt;&gt;&gt; from dask.diagnostics import ProgressBar\n&gt;&gt;&gt; ProgressBar().register()\n</code></pre> <p>Let's benchmark this operation on our location machine.</p> <pre><code>&gt;&gt;&gt; %time ds.data[\"PartType0\"][\"Masses\"].sum().compute()\n[########################################] | 100% Completed | 194.28 s\nCPU times: user 12 s, sys: 16.2 s, total: 28.2 s\nWall time: 3min 16s\n52722.6796875 code_mass\n</code></pre>"},{"location":"largedatasets/#more-advanced-computing-in-parallel","title":"More advanced: computing in parallel","text":"<p>Rather than sequentially calculating large tasks, we can also run the computation in parallel.</p> <p>To do so different advanced dask schedulers are available. Here, we use the most straight forward distributed scheduler.</p>"},{"location":"largedatasets/#running-a-localcluster","title":"Running a LocalCluster","text":"<p>Usually, we would start a scheduler and then connect new workers (e.g. running on multiple compute/backend nodes of a HPC cluster). After, tasks (either interactively or scripted) can leverage the power of these connected resources.</p> <p>For this example, we will use the same \"distributed\" scheduler/API, but keep things simple by using just the one (local) node we are currently running on.</p> <p>While the result is eventually computed, it is a bit slow, primarily because the actual reading of the data off disk is the limiting factor, and we can only use resources available on our local machine.</p> <pre><code>&gt;&gt;&gt; from dask.distributed import Client, LocalCluster\n&gt;&gt;&gt; cluster = LocalCluster(n_workers=16, threads_per_worker=1,\n                           dashboard_address=\":8787\")\n&gt;&gt;&gt; client = Client(cluster)\n&gt;&gt;&gt; client\n</code></pre> <p>This is our client. We can access the scheduler on specified dashboard port to investigate its state.</p> <p>We can now perform the same operations, but it is performed in a distributed manner, in parallel.</p> <p>One significant advantage is that (even when using only a single node) individual workers will load just the subsets of data they need to work on, meaing that I/O operations become parallel.</p> <p>Note: after creating a <code>Client()</code>, all calls to <code>.compute()</code> will automatically use this scheduler and its set of workers.</p> <pre><code>&gt;&gt;&gt; %time ds.data[\"PartType0\"][\"Masses\"].sum().compute()\nCPU times: user 5.11 s, sys: 1.42 s, total: 6.53 s\nWall time: 24.7 s\n\n52722.6796875 code_mass\n</code></pre> <p>The progress bar, we could use for the default scheduler (before initializing <code>LocalCluster</code>), is unavailable for the distributed scheduler. However, we can still view the progress of this task as it executes using its status dashboard (as a webpage in a new browser tab or within jupyter lab). You can find it by clicking on the \"Dashboard\" link above. If running this notebook server remotely, e.g. on a login node of a HPC cluster, you may have to change the '127.0.0.1' part of the address to be the same machine name/IP.</p>"},{"location":"largedatasets/#running-a-slurmcluster","title":"Running a SLURMCluster","text":"<p>If you are working with HPC resources, such as compute clusters with common schedulers (e.g. SLURM), check out Dask-Jobqueue to automatically batch jobs spawning dask workers.</p> <p>Below is an example using the SLURMCluster. We configure the job and node resources before submitting the job via the <code>scale()</code> method.</p> <pre><code>&gt;&gt;&gt; from dask.distributed import Client\n&gt;&gt;&gt; from dask_jobqueue import SLURMCluster\n&gt;&gt;&gt; cluster = SLURMCluster(queue='p.large', cores=72, memory=\"500 GB\",\n&gt;&gt;&gt;                        processes=36,\n&gt;&gt;&gt;                        scheduler_options={\"dashboard_address\": \":8811\"})\n&gt;&gt;&gt; cluster.scale(jobs=1)  # submit 1 job for 1 node\n&gt;&gt;&gt; client = Client(cluster)\n\n&gt;&gt;&gt; from scida import load\n&gt;&gt;&gt; ds = load(\"TNG50_snapshot\")\n&gt;&gt;&gt; %time ds.data[\"PartType0\"][\"Masses\"].sum().compute()\nCPU times: user 1.27 s, sys: 152 ms, total: 1.43 s\nWall time: 21.4 s\n&gt;&gt;&gt; client.shutdown()\n</code></pre> <p>The SLURM job will be killed by invoking <code>client.shutdown()</code> or if the spawning python process or ipython kernel dies. Make sure to properly handle exceptions, particularly in active jupyter notebooks, as allocated nodes might otherwise idle and not be cleaned up.</p>"},{"location":"series/","title":"Data series","text":""},{"location":"series/#series","title":"Series","text":"<p>In the tutorial section, we have only considered individual data sets.  Often data sets are given in a series (e.g. multiple snapshots of a simulation, multiple exposures in a survey).  Loading this as a series provides convenient access to all contained objects.</p> <pre><code>&gt;&gt;&gt; from scida import load\n&gt;&gt;&gt; series = load(\"TNGvariation_simulation\") #(1)!\n</code></pre> <ol> <li>Pass the base path of the simulation. </li> </ol> <p>We can now access the individual data sets from the series object:</p> <pre><code>&gt;&gt;&gt; series[0] #(1)!\n</code></pre> <ol> <li>Alias for 'series.datasets[0]'</li> </ol> <p>Depending on the available metadata, we can select data sets by these.</p> <p>For example, cosmological simulations usually have information about their redshift:</p> <pre><code>&gt;&gt;&gt; snp = series.get_dataset(redshift=2.0)\n&gt;&gt;&gt; snp.header[\"Redshift\"]\n2.0020281392528516\n</code></pre>"},{"location":"supported_data/","title":"Supported datasets","text":""},{"location":"supported_data/#supported-datasets","title":"Supported datasets","text":"<p>The following table shows a selection of supported datasets. The table is not exhaustive, but should give an idea of the range of supported datasets. If you want to use a dataset that is not listed here, read on here and consider opening an issue or contact us directly.</p> Name Support Description AURIGA Cosmological zoom-in galaxy formation simulations EAGLE Cosmological galaxy formation simulations FIRE2 Cosmological zoom-in galaxy formation simulations FLAMINGO Cosmological galaxy formation simulations Gaia <sup>1</sup> Observations of a billion nearby stars Illustris Cosmological galaxy formation simulations LGalaxies Semi-analytical model for Millenium simulations SDSS DR16 Observations for millions of galaxies SIMBA Cosmological galaxy formation simulations TNG Cosmological galaxy formation simulations TNG-Cluster Cosmological zoom-in galaxy formation simulations <p>A  checkmark indicates support out-of-the-box, a  checkmark indicates work-in-progress support or the need to create a suitable configuration file. A  checkmark indicates support for converted HDF5 versions of the original data.</p>"},{"location":"supported_data/#file-format-requirements","title":"File-format requirements","text":"<p>As of now, two underlying file formats are supported: hdf5 and zarr. Multi-file hdf5 is supported, for which a directory is passed as path, which contains only hdf5 files of the pattern prefix.XXX.hdf5, where prefix will be determined automatically and XXX is a contiguous list of integers indicating the order of hdf5 files to be merged. Hdf5 files are expected to have the same structure and all fields, i.e. hdf5 datasets, will be concatenated along their first axis.</p> <p>Support for FITS is work-in-progress, also see here for a proof-of-concept.</p> <ol> <li> <p>The HDF5 version of GAIA DR3 is available here.\u00a0\u21a9</p> </li> </ol>"},{"location":"units/","title":"Units","text":""},{"location":"units/#units","title":"Units","text":""},{"location":"units/#loading-data-with-units","title":"Loading data with units","text":"<p>Loading data sets with</p> <pre><code>from scida import load\nds = load(\"TNG50-4_snapshot\")\n</code></pre> <p>will automatically attach units to the data. This can be deactivated by passing \"units=False\" to the load function. By default, code units are used, alternatively, cgs conversions can be applied by passing \"units='cgs'\".</p> <p>Units are introduced via the pint package, see there for more details.</p> <pre><code>&gt;&gt;&gt; gas = ds.data[\"PartType0\"]\n&gt;&gt;&gt; gas[\"Coordinates\"]\ndask.array&lt;mul, shape=(18540104, 3), dtype=float64, chunksize=(5592405, 3), chunktype=numpy.ndarray&gt; &lt;Unit('code_length')&gt;\n</code></pre> <p>We can access the underlying dask array and the units separately:</p> <pre><code>&gt;&gt;&gt; gas[\"Coordinates\"].magnitude, gas[\"Coordinates\"].units\n(dask.array&lt;mul, shape=(18540104, 3), dtype=float64, chunksize=(5592405, 3), chunktype=numpy.ndarray&gt;,\n &lt;Unit('code_length')&gt;)\n</code></pre>"},{"location":"units/#unit-conversions","title":"Unit conversions","text":"<p>We can change units for evaluation as desired:</p> <pre><code>&gt;&gt;&gt; coords = gas[\"Coordinates\"]\n&gt;&gt;&gt; coords.to(\"cm\")\n&gt;&gt;&gt; # here the default system is cgs, thus we get the same result from\n&gt;&gt;&gt; coords.to_base_units()\ndask.array&lt;mul, shape=(18540104, 3), dtype=float64, chunksize=(5592405, 3), chunktype=numpy.ndarray&gt; &lt;Unit('centimeter')&gt;\n</code></pre>"},{"location":"units/#the-unit-registry","title":"The unit registry","text":"<p>The unit registry keeps all units. There is no global registry, but each dataset has its own registry as attribute ureg. The use of a global registry (or lack thereof here) can lead to some confusion, please consult the pint documentation when in doubt.</p> <pre><code>&gt;&gt;&gt; ureg = ds.ureg\n&gt;&gt;&gt; # get the unit meter\n&gt;&gt;&gt; ureg(\"m\")\n1 &lt;Unit('meter')&gt;\n</code></pre> <pre><code>&gt;&gt;&gt; # define an array with units meter (dask arrays analogously)\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.arange(10) * ureg(\"m\")\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) &lt;Unit('meter')&gt;\n</code></pre>"},{"location":"units/#synthesize-new-dask-arrays-with-units","title":"Synthesize new dask arrays with units","text":"<pre><code>&gt;&gt;&gt; energy_restframe = (gas[\"Masses\"]*ureg(\"c\")**2).to(\"erg\")  # E=mc^2\n&gt;&gt;&gt; energy_restframe\ndask.array&lt;mul, shape=(18540104,), dtype=float64, chunksize=(18540104,), chunktype=numpy.ndarray&gt; &lt;Unit('erg')&gt;\n</code></pre>"},{"location":"units/#custom-units","title":"Custom units","text":"<pre><code>&gt;&gt;&gt; ureg.define(\"halfmeter = 0.5 * m\")\n&gt;&gt;&gt; # first particle coordinates in halfmeters\n&gt;&gt;&gt; coords.to(\"halfmeter\")[0].compute()\narray([6.64064027e+23, 2.23858253e+24, 1.94176712e+24]) &lt;Unit('halfmeter')&gt;\n</code></pre>"},{"location":"userguide/","title":"Userguide","text":"<p>TODO</p>"},{"location":"visualization/","title":"Visualization","text":""},{"location":"visualization/#visualization","title":"Visualization","text":""},{"location":"visualization/#creating-plots","title":"Creating plots","text":"<p>As we often use large datasets, we need to be careful with the amount of data we plot. Generally, we reduce the data by either selecting a subset or reducing it prior to plotting. For example, we can select a subset of particles by applying a cut on a given field.</p> Selecting a subset of particles<pre><code>from scida import load\nimport matplotlib.pyplot as plt\n\nds = load(\"TNG50-1_snapshot\")\ndens = ds.data[\"PartType0\"][\"Density\"][:10000].compute()  # (1)!\ntemp = ds.data[\"PartType0\"][\"Temperature\"][:10000].compute()\nplt.plot(dens, temp, \"o\", markersize=0.1)\nplt.show()\n</code></pre> <ol> <li>Note the subselection of the first 10000 particles and conversion to a numpy array. Replace this operation with a meaninguful selection operation (e.g. a certain spatial region selection).</li> </ol> <p>Instead of subselection, we sometimes want to visualize all of the data. We can do so by first applying reduction operations using dask. A common example would be a 2D histogram.</p> 2D histograms<pre><code>import dask.array as da\nfrom scida import load\nimport matplotlib.pyplot as plt\n\nds = load(\"TNG50-1_snapshot\")\ndens = ds.data[\"PartType0\"][\"Density\"]\ntemp = ds.data[\"PartType0\"][\"Temperature\"]\nhist, xedges, yedges = da.histogram2d(dens, temp, bins=100)\nhist = hist.compute()\nextent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\nplt.imshow(hist.T, origin=\"lower\", extent=extent, aspect=\"auto\")\nplt.show()\n</code></pre>"},{"location":"visualization/#interactive-visualization","title":"Interactive visualization","text":"<p>We can do interactive visualization with holoviews. For example, we can create a scatter plot of the particle positions.</p> <pre><code>import holoviews as hv\nimport holoviews.operation.datashader as hd\nimport datashader as dshdr\nfrom scida import load\n\nds = load(\"TNG50-1_snapshot\")\nddf = ds.data[\"PartType0\"].get_dataframe([\"Coordinates0\", \"Coordinates1\", \"Masses\"])  # (1)!\n\nhv.extension(\"bokeh\")\nshaded = hd.datashade(hv.Points(ddf, [\"Coordinates0\", \"Coordinates1\"]), cmap=\"viridis\", interpolation=\"linear\",\n                      aggregator=dshdr.sum(\"Masses\"), x_sampling=5, y_sampling=5)\nhd.dynspread(shaded, threshold=0.9, max_px=50).opts(bgcolor=\"black\", xaxis=None, yaxis=None, width=500, height=500)\n</code></pre> <ol> <li>Visualization operations in holowview primarily run with dataframes, which we thus need to create using this wrapper for given fields.</li> </ol> <p></p>"},{"location":"customs/lgalaxies/","title":"LGalaxies","text":""},{"location":"customs/lgalaxies/#lgalaxies","title":"LGalaxies","text":"<p>Access via individual datasets are supported, e.g.:</p> <pre><code>&gt;&gt;&gt; from scida import load\n&gt;&gt;&gt; load(\"LGal_Ayromlou2021_snap58.hdf5\")\n</code></pre> <p>while access to the series at once (i.e. loading all data for all snapshots in a folder) is not supported.</p>"},{"location":"notebooks/gettingstarted/","title":"Getting Started","text":"In\u00a0[1]: Copied! <pre>from scida import load\nds = load(\"TNG50-4_snapshot\", units=True)\n</pre> from scida import load ds = load(\"TNG50-4_snapshot\", units=True) <p>We can get some general information about this dataset:</p> In\u00a0[2]: Copied! <pre>ds.info()\n</pre> ds.info() <pre>class: ArepoSnapshotWithUnitMixinAndCosmologyMixin\nsource: /fastdata/public/testdata-astrodask/TNG50-4_snapshot\n=== Cosmological Simulation ===\nz = 0.00\ncosmology = FlatLambdaCDM(H0=67.74 km / (Mpc s), Om0=0.3089, Tcmb0=0.0 K, Neff=3.04, m_nu=None, Ob0=0.0486)\n===============================\n=== Unit-aware Dataset ===\n==========================\n=== data ===\n+ root (containers: 7)\n++ Group (fields: 26, entries: 25257)\n++ PartType0 (fields: 31, entries: 18540104)\n++ PartType1 (fields: 12, entries: 19683000)\n++ PartType3 (fields: 6, entries: 19683000)\n++ PartType4 (fields: 22, entries: 605779)\n++ PartType5 (fields: 28, entries: 3486)\n++ Subhalo (fields: 51, entries: 22869)\n============\n\n</pre> In\u00a0[3]: Copied! <pre>print(\"some ds.config entry:\", next(iter(ds.config.items())))\nprint(\"some ds.header entry:\", next(iter(ds.header.items())))\nprint(\"some ds.parameters entry:\",next(iter(ds.parameters.items())))\n</pre> print(\"some ds.config entry:\", next(iter(ds.config.items()))) print(\"some ds.header entry:\", next(iter(ds.header.items()))) print(\"some ds.parameters entry:\",next(iter(ds.parameters.items()))) <pre>some ds.config entry: ('ADAPTIVE_HYDRO_SOFTENING', b'')\nsome ds.header entry: ('BoxSize', 35000.0)\nsome ds.parameters entry: ('AGB_MassTransferOn', 1)\n</pre> <p>If you are familiar with AREPO snapshots, you will know that oftentimes the output is split into multiple files. Most of the metadata will be the same for all files, but some (such as the number of particles in given file <code>NumPart_ThisFile</code>) will not. In these cases, the differing entries are stacked along the first axis, so that we also have access to this information:</p> In\u00a0[4]: Copied! <pre>print(\"Gas cells for each file:\", ds.header['NumPart_ThisFile'][:, 0])\n</pre> print(\"Gas cells for each file:\", ds.header['NumPart_ThisFile'][:, 0]) <pre>Gas cells for each file: [1728468 1670262 1671632 1750813 1669237 1661327 1667147 1734838 1675884\n 1653026 1657470]\n</pre> In\u00a0[5]: Copied! <pre>for key,val in ds.data.items():\n    print(\"Particle species:\", key)\n    print(\"Three of its fields:\", list(val.keys())[:3], end='\\n\\n')\n</pre> for key,val in ds.data.items():     print(\"Particle species:\", key)     print(\"Three of its fields:\", list(val.keys())[:3], end='\\n\\n') <pre>Particle species: Group\nThree of its fields: ['GroupBHMass', 'GroupBHMdot', 'GroupCM']\n\nParticle species: PartType0\nThree of its fields: ['CenterOfMass', 'Coordinates', 'Density']\n\nParticle species: PartType1\nThree of its fields: ['Coordinates', 'GroupFirstSub', 'GroupID']\n\nParticle species: PartType3\nThree of its fields: ['GroupFirstSub', 'GroupID', 'LocalSubhaloID']\n\nParticle species: PartType4\nThree of its fields: ['BirthPos', 'BirthVel', 'Coordinates']\n\nParticle species: PartType5\nThree of its fields: ['BH_BPressure', 'BH_CumEgyInjection_QM', 'BH_CumEgyInjection_RM']\n\nParticle species: Subhalo\nThree of its fields: ['SubhaloBHMass', 'SubhaloBHMdot', 'SubhaloBfldDisk']\n\n</pre> In\u00a0[6]: Copied! <pre>masses = ds.data[\"PartType0\"][\"Masses\"]\ntask = masses.sum()\n</pre> masses = ds.data[\"PartType0\"][\"Masses\"] task = masses.sum() <p>Note that all objects remain 'virtual': they are not calculated or loaded from disk, but are merely the required instructions, encoded into tasks. In a notebook we can inspect these:</p> In\u00a0[7]: Copied! <pre>masses\n</pre> masses Out[7]: Magnitude  Array   Chunk   Bytes   70.72 MiB   70.72 MiB   Shape   (18540104,)   (18540104,)   Dask graph   1 chunks in 3 graph layers   Data type   float32 numpy.ndarray  18540104 1 Unitscode_mass In\u00a0[8]: Copied! <pre>task\n</pre> task Out[8]: Magnitude  Array   Chunk   Bytes   4 B   4 B   Shape   ()   ()   Dask graph   1 chunks in 5 graph layers   Data type   float32 numpy.ndarray  Unitscode_mass <p>We can request a calculation of the actual operation(s) by applying the <code>.compute()</code> method to the task.</p> In\u00a0[9]: Copied! <pre>task.compute()\n</pre> task.compute() Out[9]:  56507.4921875 code_mass  <p>As an example of calculating something more complicated than just <code>sum()</code>, let's do the usual \"poor man's projection\" via a 2D histogram.</p> <p>To do so, we use da.histogram2d() of dask, which is analogous to numpy.histogram2d(), except that it operates on a dask array. Later on, we will discuss more advanced, interactive visualization methods.</p> In\u00a0[10]: Copied! <pre>import dask.array as da\nimport numpy as np\n\ncoords = ds.data[\"PartType0\"][\"Coordinates\"]\nx = coords[:,0]\ny = coords[:,1]\n\nnbins = 512\nbins1d = np.linspace(0, ds.header[\"BoxSize\"], nbins+1)\n\nresult = da.histogram2d(x,y,bins=[bins1d,bins1d])\nim2d = result[0].compute()\n</pre> import dask.array as da import numpy as np  coords = ds.data[\"PartType0\"][\"Coordinates\"] x = coords[:,0] y = coords[:,1]  nbins = 512 bins1d = np.linspace(0, ds.header[\"BoxSize\"], nbins+1)  result = da.histogram2d(x,y,bins=[bins1d,bins1d]) im2d = result[0].compute() <p>The resulting <code>im2d</code> is just a two-dimensional array which we can display.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[11]: Copied! <pre>from io import BytesIO\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib.colors import LogNorm\nfig = plt.figure(figsize=(6, 6), dpi=300)\ncmap = mpl.cm.viridis\ncranges = np.logspace(*[np.percentile(np.log10(im2d), i) for i in [1, 99.9]], 10)\nnorm = mpl.colors.BoundaryNorm(cranges, cmap.N, extend='both')\nplt.imshow(im2d.T, norm=norm, extent=[0, ds.header[\"BoxSize\"], 0, ds.header[\"BoxSize\"]], interpolation=\"bilinear\", rasterized=True)\nplt.xlabel(\"x (ckpc/h)\")\nplt.ylabel(\"y (ckpc/h)\")\nram = BytesIO()\nplt.savefig(ram, bbox_inches=\"tight\", dpi=150)\nram.seek(0)\nim = Image.open(ram)\nim2 = im.convert('RGB').convert('P', palette=Image.ADAPTIVE)\nim2.save(\"hist.png\" , format='PNG')\nplt.show()\n</pre> from io import BytesIO import matplotlib.pyplot as plt from PIL import Image  import matplotlib.pyplot as plt import matplotlib as mpl from matplotlib.colors import LogNorm fig = plt.figure(figsize=(6, 6), dpi=300) cmap = mpl.cm.viridis cranges = np.logspace(*[np.percentile(np.log10(im2d), i) for i in [1, 99.9]], 10) norm = mpl.colors.BoundaryNorm(cranges, cmap.N, extend='both') plt.imshow(im2d.T, norm=norm, extent=[0, ds.header[\"BoxSize\"], 0, ds.header[\"BoxSize\"]], interpolation=\"bilinear\", rasterized=True) plt.xlabel(\"x (ckpc/h)\") plt.ylabel(\"y (ckpc/h)\") ram = BytesIO() plt.savefig(ram, bbox_inches=\"tight\", dpi=150) ram.seek(0) im = Image.open(ram) im2 = im.convert('RGB').convert('P', palette=Image.ADAPTIVE) im2.save(\"hist.png\" , format='PNG') plt.show()"},{"location":"notebooks/gettingstarted/#getting-started","title":"Getting Started\u00b6","text":"<p>This package is designed to aid in the efficient analysis of large simulations, such as cosmological (hydrodynamical) simulations of large-scale structure.</p> <p>It uses the dask library to perform computations, which has several key advantages:</p> <ul> <li>(i) very large datasets which cannot normally fit into memory can be analyzed,</li> <li>(ii) calculations can be automatically distributed onto parallel 'workers', across one or more nodes, to speed them up.</li> <li>(iii) we can create abstract graphs (\"recipes\", such as for derived quantities) and only evaluate on actual demand.</li> </ul>"},{"location":"notebooks/gettingstarted/#loading-an-individual-dataset","title":"Loading an individual dataset\u00b6","text":"<p>The first step is to choose an existing snapshot of a simulation. To start, we will intentionally select the $z=0$ output of TNG50-4, which is the lowest resolution version of TNG50, a suite for galaxy formation simulations in cosmological volumes. Choosing TNG50-4 means that the data size in the snapshot is small and easy to work with. We demonstrate how to work with larger data sets at a later stage.</p>"},{"location":"notebooks/gettingstarted/#metadata","title":"Metadata\u00b6","text":"<p>Loading this data gives us access to the simulation snapshot's contents. For example, in the case of AREPO we find most of the metadata in the attributes \"config\", \"header\" and \"parameters\". The raw metadata these dictionaries are derived from is given under</p> <pre><code>ds.metadata\n</code></pre> <p>irrespective of the dataset type.</p>"},{"location":"notebooks/gettingstarted/#particlecell-data","title":"Particle/cell data\u00b6","text":"<p>Within our <code>ds</code> object, <code>ds.data</code> contains references to all the particle/cell data in this snapshot. Data is organized in a nested dictionary depending on the type of data.</p> <p>If the snapshot is split across multiple file chunks on disk (as is the case for most large cosmological simulations), then these are virtually \"combined\" as for the metadata, see above.</p> <p>As a result, there is a single array per data entry at the leaves of the nested dictionary. Note that these arrays are not normal numpy arrays, but are instead dask arrays, which we will return to later.</p> <p>For the TNG50-4 datasets, the first level of <code>ds.data</code> maps the different particle types (such as gas and dark matter), and the second level holds the different physical field arrays (such as density and ionization).</p>"},{"location":"notebooks/gettingstarted/#analyzing-snapshot-data","title":"Analyzing snapshot data\u00b6","text":"<p>In order to perform a given analysis on some available snapshot data, we would normally first explicitly load the required data from disk, and then run some calculations on this data (in memory).</p> <p>Instead, with dask, our fields are loaded automatically as well as \"lazily\" -- only when actually required.</p>"},{"location":"notebooks/gettingstarted/#computing-a-simple-statistic-on-all-particles","title":"Computing a simple statistic on (all) particles\u00b6","text":"<p>The fields in our snapshot object behave similar to actual numpy arrays.</p> <p>As a first simple example, let's calculate the total mass of gas cells in the entire simulation. Just as in numpy we can write</p>"},{"location":"notebooks/gettingstarted/#creating-a-visualization-projecting-onto-a-2d-image","title":"Creating a visualization: projecting onto a 2D image\u00b6","text":""},{"location":"notebooks/series/","title":"Series","text":"In\u00a0[1]: Copied! <pre>from scida.convenience import load\n</pre> from scida.convenience import load In\u00a0[2]: Copied! <pre>series = load(\"TNGvariation_simulation\")\n</pre> series = load(\"TNGvariation_simulation\") In\u00a0[3]: Copied! <pre>snp = series.get_dataset(redshift=2.0)\nsnp.header[\"Redshift\"]\n</pre> snp = series.get_dataset(redshift=2.0) snp.header[\"Redshift\"] <pre>WARNING:scida.interfaces.mixins.units:Cannot determine units from neither unit file nor metadata for '/PartType0/MagneticFieldDivergenceAlternative'.\n</pre> Out[3]: <pre>2.0020281392528516</pre>"},{"location":"notebooks/series/#series","title":"Series\u00b6","text":"<p>So far we have only considered individual data sets. Often data sets are given in a series (e.g. multiple snapshots of a simulation, multiple exposures in a survey). Loading this as a series provides convenient access to all contained objects.</p> <p>Here we use the more specific <code>ArepoSimulation</code> class as we load such a simulation.</p>"},{"location":"notebooks/static/largedatasets/","title":"Handling Large Data Sets","text":"In\u00a0[\u00a0]: Copied! <pre>from scida import load\nds = load(\"/data/public/testdata-scida/TNG50-3_snapshot\")\n</pre> from scida import load ds = load(\"/data/public/testdata-scida/TNG50-3_snapshot\") <p>Before we start, let's enable a progress indicator from dask (note that this will only work for local schedulers, see next section):</p> In\u00a0[\u00a0]: Copied! <pre>from dask.diagnostics import ProgressBar\nProgressBar().register()\n</pre> from dask.diagnostics import ProgressBar ProgressBar().register() <p>And then we can request the actual computation:</p> In\u00a0[\u00a0]: Copied! <pre>%time ds.data[\"PartType0\"][\"Masses\"].sum().compute()\n</pre> %time ds.data[\"PartType0\"][\"Masses\"].sum().compute() <p>While the result is eventually computed, it is a bit slow, primarily because the actual reading of the data off disk is the limiting factor, and we can only use resources available on our local machine.</p> In\u00a0[\u00a0]: Copied! <pre>from dask.distributed import Client, LocalCluster\ncluster = LocalCluster(n_workers=8, threads_per_worker=1, \n                       dashboard_address=\":8787\")\nclient = Client(cluster)\n</pre> from dask.distributed import Client, LocalCluster cluster = LocalCluster(n_workers=8, threads_per_worker=1,                         dashboard_address=\":8787\") client = Client(cluster) <p>Here is our client. We can access the scheduler on specified dashboard port to investigate its state.</p> In\u00a0[\u00a0]: Copied! <pre>client\n</pre> client <p>We can now perform the same operations, but it is performed in a distributed manner, in parallel.</p> <p>One significant advantage is that (even when using only a single node) individual workers will load just the subsets of data they need to work on, meaing that I/O operations become parallel.</p> <p>Note: after creating a <code>Client()</code>, all calls to <code>.compute()</code> will automatically use this scheduler and its set of workers.</p> In\u00a0[\u00a0]: Copied! <pre>%time ds.data[\"PartType0\"][\"Masses\"].sum().compute()\n</pre> %time ds.data[\"PartType0\"][\"Masses\"].sum().compute() <p>The progress bar, we could use for the default scheduler (before initializing <code>LocalCluster</code>), is unavailable for the distributed scheduler. However, we can still view the progress of this task as it executes using its status dashboard (as a webpage in a new browser tab or within jupyter lab). You can find it by clicking on the \"Dashboard\" link above. If running this notebook server remotely, e.g. on a login node of a HPC cluster, you may have to change the '127.0.0.1' part of the address to be the same machine name/IP.</p>"},{"location":"notebooks/static/largedatasets/#handling-large-data-sets","title":"Handling Large Data Sets\u00b6","text":"<p>Until now, we have applied our framework to a very small simulation. However, what if we are working with a very large data set (like TNG50-1, which has $2160^3$ particles, $512$ times more than TNG50-4)?</p>"},{"location":"notebooks/static/largedatasets/#starting-simple-computing-in-chunks","title":"Starting simple: computing in chunks\u00b6","text":"<p>First, we can still run the same calculation as above, and it will \"just work\" (hopefully).</p> <p>This is because Dask has many versions of common algorithms and functions which work on \"blocks\" or \"chunks\" of the data, which split up the large array into smaller arrays. Work is needed on each chunk, after which the final answer is assembled.</p> <p>Importantly, in our case above, even if the <code>mass</code> array above does not fit into memory, the <code>mass.sum().compute()</code> will chunk the operation up in a way that the task can be calculated.</p>"},{"location":"notebooks/static/largedatasets/#more-advanced-computing-in-parallel","title":"More advanced: computing in parallel\u00b6","text":"<p>Rather than sequentially calculating large tasks, we can also run the computation in parallel.</p> <p>To do so different advanced dask schedulers are available. Here, we use the most straight forward distributed scheduler.</p> <p>Usually, we would start a scheduler and then connect new workers (e.g. running on multiple compute/backend nodes of a HPC cluster). After, tasks (either interactively or scripted) can leverage the power of these connected resources.</p> <p>For this example, we will use the same \"distributed\" scheduler/API, but keep things simple by using just the one (local) node we are currently running on.</p>"},{"location":"notebooks/static/visualization/","title":"Visualization","text":"In\u00a0[\u00a0]: Copied! <pre>import dask\nfrom dask.distributed import LocalCluster, Client\ncluster = LocalCluster(n_workers=2, threads_per_worker=10, memory_limit=\"8GB\", dashboard_address=\":8832\")\ndask.config.set({\"array.chunk-size\": \"512MiB\"})\nclient = Client(cluster)\n</pre> import dask from dask.distributed import LocalCluster, Client cluster = LocalCluster(n_workers=2, threads_per_worker=10, memory_limit=\"8GB\", dashboard_address=\":8832\") dask.config.set({\"array.chunk-size\": \"512MiB\"}) client = Client(cluster) In\u00a0[\u00a0]: Copied! <pre>from scida import load\nfrom scida.convenience import get_testdata\ngpath = get_testdata(\"TNG50-2_group\")\ndts = load(\"testdata://TNG50-2_snapshot\", catalog=gpath)\n</pre> from scida import load from scida.convenience import get_testdata gpath = get_testdata(\"TNG50-2_group\") dts = load(\"testdata://TNG50-2_snapshot\", catalog=gpath) In\u00a0[\u00a0]: Copied! <pre>ddf = dts.data[\"PartType0\"].get_dataframe([\"Coordinates0\", \"Coordinates1\", \"Masses\"])\n</pre> ddf = dts.data[\"PartType0\"].get_dataframe([\"Coordinates0\", \"Coordinates1\", \"Masses\"]) In\u00a0[\u00a0]: Copied! <pre>import holoviews as hv\nhv.extension(\"bokeh\")\n</pre> import holoviews as hv hv.extension(\"bokeh\") In\u00a0[\u00a0]: Copied! <pre>import holoviews.operation.datashader as hd\nimport datashader as ds\nimport dask.array as da\n\nshaded = hd.datashade(hv.Points(ddf, [\"Coordinates0\", \"Coordinates1\"]), cmap=\"viridis\", interpolation=\"linear\",\n                     aggregator=ds.sum(\"Masses\"), x_sampling=5, y_sampling=5)\nhd.dynspread(shaded, threshold=0.9, max_px=50).opts(bgcolor=\"black\", xaxis=None, yaxis=None, width=500, height=500)\n</pre> import holoviews.operation.datashader as hd import datashader as ds import dask.array as da  shaded = hd.datashade(hv.Points(ddf, [\"Coordinates0\", \"Coordinates1\"]), cmap=\"viridis\", interpolation=\"linear\",                      aggregator=ds.sum(\"Masses\"), x_sampling=5, y_sampling=5) hd.dynspread(shaded, threshold=0.9, max_px=50).opts(bgcolor=\"black\", xaxis=None, yaxis=None, width=500, height=500)"},{"location":"tutorial/","title":"Tutorial","text":"<p>           Simulations         </p> <p>Tutorial on a simulation dataset.</p> <p>           Observations         </p> <p>Tutorial on an observational dataset.</p>"},{"location":"tutorial/observations/","title":"Observations","text":""},{"location":"tutorial/observations/#tutorial-observational-data-set","title":"Tutorial (observational data set)","text":"<p>This package is designed to aid in the efficient analysis of large datasets, such as GAIA DR3.</p> <p>Tutorial dataset</p> <p>In the following, we will subset from the GAIA data release 3. The reduced dataset contains 100000 randomly selected entries only. The reduced dataset can be downloaded here. Check Supported Datasets for an incomplete list of supported datasets and requirements for support of new datasets. A tutorial for a cosmological simulation can be found here.</p> <p>It uses the dask library to perform computations, which has several key advantages:</p> <ol> <li>very large datasets which cannot normally fit into memory can be analyzed,</li> <li>calculations can be automatically distributed onto parallel 'workers', across one or more nodes, to speed them up,</li> <li>we can create abstract graphs (\"recipes\", such as for derived quantities) and only evaluate on actual demand.</li> </ol>"},{"location":"tutorial/observations/#loading-an-individual-dataset","title":"Loading an individual dataset","text":"<p>Here, we choose the GAIA data release 3 as an example. The dataset is obtained in HDF5 format as used at ITA Heidelberg. We intentionally select a small subset of the data to work with. Choosing a subset means that the data size is small and easy to work with. We demonstrate how to work with larger data sets at a later stage.</p> <p>First, we load the dataset using the convenience function <code>load()</code> that will determine the appropriate dataset class for us:</p> Loading a dataset<pre><code>&gt;&gt;&gt; from scida import load\n&gt;&gt;&gt; ds = load(\"gaia_dr3_subset100000.hdf5\", units=True) #(1)!\n&gt;&gt;&gt; ds.info() #(2)!\nclass: DatasetWithUnitMixin\nsource: /home/cbyrohl/data/testdata-scida/gaia_dr3_subset100000.hdf5\n=== Unit-aware Dataset ===\n==========================\n=== data ===\n+ root (fields: 27, entries: 100000)\n============\n</code></pre> <ol> <li>The <code>units=True</code> argument will attach code units to all fields (default). Alternative choices are False to go without units and cgs for cgs units.    The current default is False, which will change to True in the near future.</li> <li>Call to receive some information about the loaded dataset.</li> </ol> <p>The dataset is now loaded, and we can inspect its contents, specifically its container and fields loaded. We can access the data in the dataset by using the <code>data</code> attribute, which is a dictionary of containers and fields.</p> <p>We have a total of 27 fields available, which are:</p> Available fields<pre><code>&gt;&gt;&gt; ds.data.keys()\n['pmdec',\n 'distance_gspphot',\n...\n 'distance_gspphot_upper',\n 'pmra_error']\n</code></pre> <p>Let's take a look at some field in this container:</p> Inspecting a field<pre><code>&gt;&gt;&gt; ds.data[\"dec\"]\ndask.array&lt;mul, shape=(100000,), dtype=float64, chunksize=(100000,), chunktype=numpy.ndarray&gt; &lt;Unit('degree')&gt;\n</code></pre> <p>The field is a dask array, which is a lazy array that will only be evaluated when needed. How these lazy arrays and their units work and are to be used will be explored in the next section.</p>"},{"location":"tutorial/observations/#dask-arrays-and-units","title":"Dask arrays and units","text":""},{"location":"tutorial/observations/#dask-arrays","title":"Dask arrays","text":"<p>Dask arrays are virtual entities that are only evaluated when needed. If you are unfamiliar with dask arrays, consider taking a look at this 3-minute introduction.</p> <p>They are not numpy arrays, but they can be converted to them, and have most of their functionality. Within dask, an internal task graph is created that holds the recipes how to construct the array from the underlying data.</p> <p>In general, fields can be also be stored in flat or more nested structures, depending on the dataset.</p> <p>We can trigger the evaluation of the dask array by calling <code>compute()</code> on it:</p> Evaluating a dask array<pre><code>&gt;&gt;&gt; ds.data[\"dec\"].compute()\narray([ 0.86655069,  1.15477218,  2.14207063, ..., -1.5291509 ,\n       -1.30061261, -0.88984633]) &lt;Unit('degree')&gt;\n</code></pre> <p>However, directly evaluating dask arrays is strongly discouraged for large datasets, as it will load the entire dataset into memory. Instead, we will reduce the datasize by running desired analysis/reduction within dask before calling compute(), which we present in the next section.</p>"},{"location":"tutorial/observations/#units","title":"Units","text":"<p>If passing <code>units=True</code> (default) to <code>load()</code>, the dataset will be loaded with code units attached to all fields. These units are attached to each field / dask array. Units are provided via the pint package. See the pint documentation for more information. Also check out this page for more unit-related examples.</p> <p>In short, each field, that is represented by a modified dask array, has a magnitude (the dask array without any units attached) and a unit. These can be accessed via the <code>magnitude</code> and <code>units</code> attributes, respectively.</p> Accessing the magnitude and units of a field<pre><code>&gt;&gt;&gt; ds.data[\"dec\"].magnitude.compute(), ds.data[\"dec\"].units\n(dask.array&lt;mul, shape=(100000,), dtype=float64, chunksize=(100000,), chunktype=numpy.ndarray&gt;,\n &lt;Unit('degree')&gt;)\n</code></pre> <p>When defining derived fields from dask arrays, the correct units are automatically propagated to the new field, and dimensionality checks are performed. Importantly, the unit calculation is done immediately, thus allowing to directly see the resulting units and any dimensionality mismatches.</p>"},{"location":"tutorial/observations/#analyzing-the-data","title":"Analyzing the data","text":""},{"location":"tutorial/observations/#computing-a-simple-statistic-on-all-objects","title":"Computing a simple statistic on (all) objects","text":"<p>The fields in our data object behave similar to actual numpy arrays.</p> <p>As a first simple example, let's calculate the mean declination of the stars. Just as in numpy we can write</p> Calculating the mean declination<pre><code>&gt;&gt;&gt; dec = ds.data[\"dec\"]\n&gt;&gt;&gt; task = dec.mean()\n&gt;&gt;&gt; task\ndask.array&lt;mean_agg-aggregate, shape=(), dtype=float64, chunksize=(), chunktype=numpy.ndarray&gt; &lt;Unit('degree')&gt;\n</code></pre> <p>Note that all objects remain 'virtual': they are not calculated or loaded from disk, but are merely the required instructions, encoded into tasks.</p> <p>We can request a calculation of the actual operation(s) by applying the <code>.compute()</code> method to the task.</p> <pre><code>&gt;&gt;&gt; meandec = task.compute()\n&gt;&gt;&gt; meandec\n-18.433358575323904 &lt;Unit('degree')&gt;\n</code></pre> <p>As an example of calculating something more complicated than just <code>sum()</code>, let's do the usual \"poor man's projection\" via a 2D histogram.</p> <p>To do so, we use da.histogram2d() of dask, which is analogous to numpy.histogram2d(), except that it operates on a dask array. We discuss more advanced and interactive visualization methods here.</p> <pre><code>&gt;&gt;&gt; import dask.array as da\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; x = ds.data[\"l\"]\n&gt;&gt;&gt; y = ds.data[\"b\"]\n&gt;&gt;&gt; nbins = (360, 180)\n&gt;&gt;&gt; extent = [0.0, 360.0, -90.0, 90.0]\n&gt;&gt;&gt; xbins = np.linspace(*extent[:2], nbins[0] + 1)\n&gt;&gt;&gt; ybins = np.linspace(*extent[-2:], nbins[1] + 1)\n&gt;&gt;&gt; hist, xbins, ybins = da.histogram2d(x, y, bins=[xbins, ybins])\n&gt;&gt;&gt; im2d = hist.compute() #(1)!\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from matplotlib.colors import LogNorm\n&gt;&gt;&gt; plt.imshow(im2d.T, origin=\"lower\", norm=LogNorm(), extent=extent, interpolation=\"none\")\n&gt;&gt;&gt; plt.xlabel(\"l [deg]\")\n&gt;&gt;&gt; plt.ylabel(\"b [deg]\")\n&gt;&gt;&gt; plt.show()\n</code></pre> <ol> <li>The compute() on <code>im2d</code> results in a two-dimensional array which we can display.</li> </ol> <p></p> <p>Info</p> <p>Above image shows the histogram obtained for the full data set.</p>"},{"location":"tutorial/observations/#fits-files","title":"FITS files","text":"<p>Observations are often stored in FITS files. Support in scida is work-in-progress and requires the astropy package.</p> <p>Here we show use of the SDSS DR16.</p> <p>https://live-sdss4org-dr16.pantheonsite.io/spectro/spectro_access/</p> <p>SDSS DR16</p> <p>The SDSS DR16 redshift and classification file \"specObj-dr16.fits \" can be found here.</p> <p>It uses the dask library to perform computations, which has several key advantages:</p> <ol> <li>very large datasets which cannot normally fit into memory can be analyzed,</li> </ol> <pre><code>&gt;&gt;&gt; from scida import load\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; path = \"/virgotng/mpia/obs/SDSS/specObj-dr16.fits\"\n&gt;&gt;&gt; ds = load(path)\n&gt;&gt;&gt;\n&gt;&gt;&gt; cx = ds.data[\"CX\"].compute()\n&gt;&gt;&gt; cy = ds.data[\"CY\"].compute()\n&gt;&gt;&gt; cz = ds.data[\"CZ\"].compute()\n&gt;&gt;&gt; z = ds.data[\"Z\"].compute()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # order by redshift for scatter plotting\n&gt;&gt;&gt; idx = np.argsort(z)\n&gt;&gt;&gt;\n&gt;&gt;&gt; theta = np.arccos(cz.magnitude / np.sqrt(cx**2 + cy**2 + cz**2).magnitude)\n&gt;&gt;&gt; phi = np.arctan2(cy.magnitude,cx.magnitude)\n&gt;&gt;&gt;\n&gt;&gt;&gt; fig = plt.figure(figsize=(10,5))\n&gt;&gt;&gt; ax = fig.add_subplot(111, projection=\"aitoff\")\n&gt;&gt;&gt; ra = phi[idx]\n&gt;&gt;&gt; dec = -(theta-np.pi/2.0)\n&gt;&gt;&gt; sc = ax.scatter(ra, dec[idx], s=0.05, c=z[idx], rasterized=True)\n&gt;&gt;&gt; fig.colorbar(sc, label=\"redshift\")\n&gt;&gt;&gt; ax.set_xticklabels(['14h','16h','18h','20h','22h','0h','2h','4h','6h','8h','10h'])\n&gt;&gt;&gt; ax.set_xlabel(\"RA\")\n&gt;&gt;&gt; ax.set_ylabel(\"DEC\")\n&gt;&gt;&gt; ax.grid(True)\n&gt;&gt;&gt; plt.savefig(\"sdss_dr16.png\", dpi=150)\n&gt;&gt;&gt; plt.show()\n</code></pre> <p></p>"},{"location":"tutorial/simulations/","title":"Simulations","text":""},{"location":"tutorial/simulations/#getting-started","title":"Getting started","text":"<p>Tutorial dataset</p> <p>In the following, we will use a small test dataset from the TNG50 simulation. This is a cosmological galaxy formation simulation. This dataset is still a gigabyte in size and can be downloaded here. Note that analysis is not limited to simulations, but also observational data. Check Supported Datasets for an incomplete list of supported datasets and requirements for support of new datasets. A shorter tutorial for an observational dataset can be found here.</p> <p>This package is designed to aid in the efficient analysis of large simulations, such as cosmological (hydrodynamical) simulations of large-scale structure. Datasets are expected to be spatially unstructured, e.g. tessellation or particle based. Much of the functionality can also be used to process observational datasets.</p> <p>It uses the dask library to perform computations, which has several key advantages:</p> <ol> <li>very large datasets which cannot normally fit into memory can be analyzed,</li> <li>calculations can be automatically distributed onto parallel 'workers', across one or more nodes, to speed them up,</li> <li>we can create abstract graphs (\"recipes\", such as for derived quantities) and only evaluate on actual demand.</li> </ol>"},{"location":"tutorial/simulations/#loading-an-individual-dataset","title":"Loading an individual dataset","text":"<p>The first step is to choose an existing snapshot of a simulation. To start, we will intentionally select the output of TNG50-4 at redshift 0, which is the lowest resolution version of TNG50, a suite for galaxy formation simulations in cosmological volumes. Choosing TNG50-4 means that the data size in the snapshot is small and easy to work with. We demonstrate how to work with larger data sets at a later stage.</p> <p>First, we load the dataset using the convenience function <code>load()</code> that will determine the appropriate dataset class for us:</p> Loading a dataset<pre><code>&gt;&gt;&gt; from scida import load\n&gt;&gt;&gt; ds = load(\"TNG50-4_snapshot\", units=True) #(1)!\n&gt;&gt;&gt; ds.info() #(2)!\nclass: ArepoSnapshotWithUnitMixinAndCosmologyMixin\nsource: /vera/u/byrohlc/Downloads/snapdir_030\n=== Cosmological Simulation ===\nz = 2.32\ncosmology = FlatLambdaCDM(H0=67.74 km / (Mpc s), Om0=0.3089, Tcmb0=0.0 K, Neff=3.04, m_nu=None, Ob0=0.0486)\n===============================\n=== Unit-aware Dataset ===\n==========================\n=== data ===\n+ root (containers: 5)\n++ PartType0 (fields: 10, entries: 19124236)\n++ PartType1 (fields: 3, entries: 19683000)\n++ PartType3 (fields: 2, entries: 19683000)\n++ PartType4 (fields: 9, entries: 161401)\n++ PartType5 (fields: 20, entries: 2673)\n============\n</code></pre> <ol> <li>The <code>units=True</code> argument will attach code units to all fields. Alternative choices are False to go without units and cgs for cgs units.    The current default is False, which will change to True in the near future.</li> <li>Call to receive some information about the loaded dataset.</li> </ol> <p>The dataset is now loaded, and we can inspect its contents, specifically its container and fields loaded. We can access the data in the dataset by using the <code>data</code> attribute, which is a dictionary of containers and fields.</p> <p>From above output we see that the dataset contains seven containers, each of which contains a number of fields. For this snapshot, the containers are PartType0, PartType1, PartType3, PartType4, PartType5, Group, and Subhalo. The containers represent gas cells, dark matter particles, stars, black holes, groups and subhalos, respectively. Each container contains fields that are specific to the container, such as the mass of the gas cells, the velocity of the dark matter particles, etc. The fields are described in the TNG50 documentation.</p> <p>The available fields of the container \"PartType0\" are:</p> Available fields of a given container<pre><code>&gt;&gt;&gt; ds.data[\"PartType0\"].keys() #(1)!\n['Coordinates',\n 'Density',\n 'ElectronAbundance',\n 'GFM_Metallicity',\n 'InternalEnergy',\n 'Masses',\n 'ParticleIDs',\n 'StarFormationRate',\n 'Temperature',\n 'Velocities']\n</code></pre> <ol> <li>For many simulation types aliases exist, such that \"gas\" can be used instead of \"PartType0\", \"stars\" instead of \"PartType4\", etc.</li> </ol> <p>Let's take a look at some field in this container:</p> Inspecting a field<pre><code>&gt;&gt;&gt; gas = ds.data[\"PartType0\"]\n&gt;&gt;&gt; gas[\"StarFormationRate\"]\n'dask.array&lt;mul, shape=(19124236,), dtype=float32, chunksize=(19124236,), chunktype=numpy.ndarray&gt; Msun / year'\n</code></pre> <p>The field is a dask array, which is a lazy array that will only be evaluated when needed. How these lazy arrays and their units work and are to be used will be explored in the next section.</p>"},{"location":"tutorial/simulations/#dask-arrays-and-units","title":"Dask arrays and units","text":""},{"location":"tutorial/simulations/#dask-arrays","title":"Dask arrays","text":"<p>Dask arrays are virtual entities that are only evaluated when needed. If you are unfamiliar with dask arrays, consider taking a look at this 3-minute introduction.</p> <p>They are not numpy arrays, but they can be converted to them, and have most of their functionality. Within dask, an internal task graph is created that holds the recipes how to construct the array from the underlying data.</p> <p>In our case, in fact, the snapshot and its fields are split across multiple files on disk (as for most large datasets), and virtually combined into a single dask array. The user does not need to worry about this as these operations take place in the background.</p> <p>For the TNG50-4 datasets, the first level of <code>ds.data</code> maps the different particle types (such as gas and dark matter), and the second level holds the different physical field arrays (such as density and ionization).</p> <p>In general, fields can be also be stored in flat or more nested structures, depending on the dataset.</p> <p>We can trigger the evaluation of the dask array by calling <code>compute()</code> on it:</p> Evaluating a dask array<pre><code>&gt;&gt;&gt; gas[\"StarFormationRate\"].compute()\n'[0.017970681190490723 0.0 0.016353357583284378 ... 0.0 0.0 0.0] Msun / year'\n</code></pre> <p>However, directly evaluating dask arrays is strongly discouraged for large datasets, as it will load the entire dataset into memory. Instead, we will reduce the datasize by running desired analysis/reduction within dask before calling compute(), which we present in the next section.</p>"},{"location":"tutorial/simulations/#units","title":"Units","text":"<p>If passing <code>units=True</code> to <code>load()</code>, the dataset will be loaded with code units attached to all fields. These units are attached to each field / dask array. Units are provided via the pint package. See the pint documentation for more information. Also check out this page for more unit-related examples.</p> <p>In short, each field, that is represented by a modified dask array, has a magnitude (the dask array without any units attached) and a unit. These can be accessed via the <code>magnitude</code> and <code>units</code> attributes, respectively.</p> Accessing the magnitude and units of a field<pre><code>&gt;&gt;&gt; gas[\"Coordinates\"].magnitude\n'dask.array&lt;mul, shape=(19124236, 3), dtype=float32, chunksize=(11184810, 3), chunktype=numpy.ndarray&gt;'\n&gt;&gt;&gt; gas[\"Coordinates\"].units\n'code_length'\n</code></pre> <p>When defining derived fields from dask arrays, the correct units are automatically propagated to the new field, and dimensionality checks are performed. Importantly, the unit calculation is done immediately, thus allowing to directly see the resulting units and any dimensionality mismatches.</p>"},{"location":"tutorial/simulations/#analyzing-snapshot-data","title":"Analyzing snapshot data","text":""},{"location":"tutorial/simulations/#computing-a-simple-statistic-on-all-particles","title":"Computing a simple statistic on (all) particles","text":"<p>The fields in our snapshot object behave similar to actual numpy arrays.</p> <p>As a first simple example, let's calculate the total mass of gas cells in the entire simulation. Just as in numpy we can write</p> Calculating the total mass of gas cells<pre><code>&gt;&gt;&gt; masses = ds.data[\"PartType0\"][\"Masses\"]\n&gt;&gt;&gt; task = masses.sum()\n&gt;&gt;&gt; task\n'dask.array&lt;sum-aggregate, shape=(), dtype=float32, chunksize=(), chunktype=numpy.ndarray&gt; code_mass'\n</code></pre> <p>Note that all objects remain 'virtual': they are not calculated or loaded from disk, but are merely the required instructions, encoded into tasks.</p> <p>We can request a calculation of the actual operation(s) by applying the <code>.compute()</code> method to the task.</p> <pre><code>&gt;&gt;&gt; totmass = task.compute()\n&gt;&gt;&gt; totmass\n'57384.59375 code_mass'\n</code></pre> Converting units <p>We can easily convert the code units to something physically meaningful by using the <code>.to()</code> method of the task or its computed result:</p> Converting units<pre><code>&gt;&gt;&gt; totmass.to(\"Msun\")\n'8.3e15 Msun'\n</code></pre> <p>As an example of calculating something more complicated than just <code>sum()</code>, let's do the usual \"poor man's projection\" via a 2D histogram.</p> <p>To do so, we use da.histogram2d() of dask, which is analogous to numpy.histogram2d(), except that it operates on a dask array. We discuss more advanced and interactive visualization methods here.</p> <pre><code>&gt;&gt;&gt; import dask.array as da\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; coords = ds.data[\"PartType0\"][\"Coordinates\"]\n&gt;&gt;&gt; x = coords[:,0]\n&gt;&gt;&gt; y = coords[:,1]\n&gt;&gt;&gt; nbins = 512\n&gt;&gt;&gt; bins1d = np.linspace(0, ds.header[\"BoxSize\"], nbins+1)\n&gt;&gt;&gt; hist, xbins, ybins = da.histogram2d(x,y,bins=[bins1d,bins1d])\n&gt;&gt;&gt; im2d = hist.compute() #(1)!\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from matplotlib.colors import LogNorm\n&gt;&gt;&gt; fig = plt.figure(figsize=(6, 6))\n&gt;&gt;&gt; plt.imshow(im2d.T, norm=LogNorm(vmin=25, vmax=500), extent=[0, ds.header[\"BoxSize\"], 0, ds.header[\"BoxSize\"]], cmap=\"viridis\")\n&gt;&gt;&gt; plt.xlabel(\"x (ckpc/h)\")\n&gt;&gt;&gt; plt.ylabel(\"y (ckpc/h)\")\n&gt;&gt;&gt; plt.show()\n</code></pre> <ol> <li>The compute() on <code>im2d</code> results in a two-dimensional array which we can display.</li> </ol> <p></p>"},{"location":"tutorial/simulations/#catalogs","title":"Catalogs","text":"<p>Many cosmological simulations have a catalog of halos, subhalos, galaxies, etc.</p> <p>For AREPO/Gadget based simulations, we support use of this information. Find more find more information on how to use catalogs here.</p>"}]}